% Document setup
\documentclass[article, a4paper, 11pt, oneside]{memoir}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[UKenglish]{babel}

% Document info
\newcommand\doctitle{Notes on linear algebra}
\newcommand\docauthor{Danny Nyg√•rd Hansen}

% Formatting and layout
\usepackage[autostyle]{csquotes}
\renewcommand{\mktextelp}{(\textellipsis\unkern)}
\usepackage[final]{microtype}
\usepackage{xcolor}
\frenchspacing
\usepackage{latex-sty/articlepagestyle}
\usepackage{latex-sty/articlesectionstyle}

% Fonts
\usepackage{amssymb}
\usepackage[largesmallcaps,partialup]{kpfonts}
\DeclareSymbolFontAlphabet{\mathrm}{operators} % https://tex.stackexchange.com/questions/40874/kpfonts-siunitx-and-math-alphabets
\linespread{1.06}
% \let\mathfrak\undefined
% \usepackage{eufrak}
\DeclareMathAlphabet\mathfrak{U}{euf}{m}{n}
\SetMathAlphabet\mathfrak{bold}{U}{euf}{b}{n}
% https://tex.stackexchange.com/questions/13815/kpfonts-with-eufrak
\usepackage{inconsolata}

% Hyperlinks
\usepackage{hyperref}
\definecolor{linkcolor}{HTML}{4f4fa3}
\hypersetup{%
	pdftitle=\doctitle,
	pdfauthor=\docauthor,
	colorlinks,
	linkcolor=linkcolor,
	citecolor=linkcolor,
	urlcolor=linkcolor,
	bookmarksnumbered=true
}

% Equation numbering
\numberwithin{equation}{chapter}

% Footnotes
\footmarkstyle{\textsuperscript{#1}\hspace{0.25em}}

% Mathematics
\usepackage{latex-sty/basicmathcommands}
\usepackage{latex-sty/framedtheorems}
\usepackage{tikz-cd}
\tikzcdset{arrow style=math font} % https://tex.stackexchange.com/questions/300352/equalities-look-broken-with-tikz-cd-and-math-font
\usetikzlibrary{babel}

% Lists
\usepackage{enumitem}
\setenumerate[0]{label=\normalfont(\arabic*)}

% Bibliography
\usepackage[backend=biber, style=authoryear, maxcitenames=2, useprefix]{biblatex}
\addbibresource{references.bib}

% Title
\title{\doctitle}
\author{\docauthor}

\newcommand{\calM}{\mathcal{M}}
\newcommand{\calL}{\mathcal{L}}
\newcommand{\calV}{\mathcal{V}}
\newcommand{\calW}{\mathcal{W}}
\newcommand{\calU}{\mathcal{U}}
\newcommand{\calE}{\mathcal{E}}
\DeclareMathOperator{\sign}{sgn}
\DeclareMathOperator{\adj}{adj}
\DeclareMathOperator{\spec}{Spec}
\DeclareMathOperator{\trace}{tr}


\usepackage{listofitems}
\setsepchar{,}

\makeatletter

\newcommand{\mat@dims}[1]{%
    \readlist*\@dims{#1}%
    \ifnum \@dimslen=1
        \def\@dimsout{\@dims[1]}%
    \else
        \def\@dimsout{\@dims[1], \@dims[2]}%
    \fi
    \@dimsout
}

\newcommand{\matgroup}[3]{\mathrm{#1}_{#2}(#3)}
\newcommand{\matGL}[2]{\matgroup{GL}{#1}{#2}}
\newcommand{\trans}{^{\top}}
% \newcommand{\mat}[2]{\calM_{\mat@dims{#1}}(#2)}
\newcommand{\mat}[2]{\mathrm{Mat}_{\mat@dims{#1}}(#2)}
\newcommand{\matO}[1]{\mathrm{O}(#1)}
\newcommand{\matSO}[1]{\mathrm{SO}(#1)}
\newcommand{\field}{\mathbb{F}}



%%%%%%%%%%%%%%%% Gray codes
\DeclarePairedDelimiter{\floor}{\lfloor}{\rfloor}

\makeatother


\begin{document}

\maketitle

\chapter{Linear equations and matrices}

\section{Linear equations}

Throughout we let $\field$ denote an arbitrary field and $R$ a commutative ring. Let $m$ and $n$ be positive integers. A \emph{linear equation in $n$ unknowns} is an equation on the form
%
\begin{equation*}
    l \colon a_1 x_1 + \cdots + a_n x_n = b,
\end{equation*}
%
where $a_1, \ldots, a_n, b \in \field$. A \emph{solution} to $l$ is an element $v = (v_1, \ldots, v_n) \in \field^n$ such that
%
\begin{equation*}
    a_1 v_1 + \cdots + a_n v_n = b.
\end{equation*}
%
A \emph{system of linear equations in $n$ unknowns} is a tuple $L = (l_1, \ldots, l_m)$, where each $l_i$ is a linear equation in $n$ unknowns. An element $v \in \field^n$ is a \emph{solution} to $L$ if it is a solution to each linear equation $l_1, \ldots, l_m$.

Let $L$ and $L'$ be systems of linear equations in $n$ unknowns. We say that $L$ and $L'$ are \emph{solution equivalent} if they have the same solutions. Furthermore, we say that they are \emph{combination equivalent} if each equation in $L'$ is a linear combination of the equations in $L$, and vice versa. Clearly, if $L$ and $L'$ are combination equivalent they are also solution equivalent, but the converse does not hold.


\section{Matrices}

It is well-known that a system of linear equations is equivalent to a matrix equation on the form $Ax = b$, where $A \in \mat{m,n}{\field}$, $x \in \field^n$ and $b \in \field^m$. Recall the \emph{elementary row operations} on $A$:
%
\begin{enumerate}
    \item multiplication of one row of $A$ by a nonzero scalar,
    \item addition to one row of $A$ a scalar multiple of another (different) row, and
    \item interchange of two rows of $A$.
\end{enumerate}
%
If $e$ is an elementary row operation, we write $e(A)$ for the matrix obtained when applying $e$ to $A$. Clearly each elementary row operation $e$ has an \enquote{inverse}, i.e. an elementary row operation $e'$ such that $e'(e(A)) = e(e'(A)) = A$. Two matrices $A,B \in \mat{m,n}{\field}$ are called \emph{row-equivalent} if $A$ is obtained by applying a finite sequence of elementary row operations to $B$ (and vice versa, though this need not be assumed since each elementary row operation has an inverse).

Clearly, if $A, B \in \mat{m,n}{\field}$ are row-equivalent, then the systems of equations $Ax = 0$ and $Bx = 0$ are combination equivalent, hence have the same solutions.

\begin{definition}
    A matrix $H \in \mat{m,n}{\field}$ is called \emph{row-reduced} if
    %
    \begin{enumdef}
        \item the first nonzero entry of each nonzero row in $H$ is $1$, and
        \item each column of $H$ containing the leading nonzero entry of some row has all its other entries equal $0$.
    \end{enumdef}
    %
    If $H$ is row-reduced, it is called a \emph{row-reduced echelon matrix} if it also has the following properties:
    %
    \begin{enumdef}[resume]
        \item Every row of $H$ only containing zeroes occur below every row which has a nonzero entry, and
        \item if rows $1, \ldots, r$ are the nonzero rows of $H$, and if the leading nonzero entry of row $i$ occurs in column $k_i$, then $k_1 < \cdots < k_r$.
    \end{enumdef}
\end{definition}

An \emph{elementary matrix} is a matrix obtained by applying a single elementary row operation to the identity matrix $I$. It is easy to show that if $e$ is an elementary row operation and $E = e(I) \in \mat{m}{\field}$, then $e(A) = EA$ for $A \in \mat{m,n}{\field}$. If $B \in \mat{m,n}{\field}$, then $A$ and $B$ are row-equivalent if and only if $A = PB$, where $P \in \mat{m}{\field}$ is a product of elementary matrices.

\begin{proposition}
    Every matrix in $\mat{m,n}{\field}$ is row-equivalent to a unique row-reduced echelon matrix.
\end{proposition}

\begin{proof}
    The usual Gauss--Jordan elimination algorithm proves existence. If $H, K \in \mat{m,n}{R}$ are row-equivalent row-reduced echelon matrices, we claim that $H = K$. We prove this by induction in $n$. If $n = 1$ then this is obvious, so assume that $n > 1$. Let $H_1$ and $K_1$ be the matrices obtained by deleting the $n$th column in $H$ and $K$ respectively. Then $H_1$ and $K_1$ are also row-equivalent\footnote{It should be obvious that deleting columns preserves row-equivalence, but we give a more precise argument: If $P \in \mat{m}{\field}$ is a product of elementary matrices and $a_1, \ldots, a_n \in \field^m$ are the columns in $A$, then the columns in $PA$ are $Pa_1, \ldots, Pa_m$. Thus elementary row operations are applied to each column independently of the other columns.} and row-reduced echelon matrices, so by induction $H_1 = K_1$. Thus if $H$ and $K$ differ, they must differ in the $n$th column.

    Let $H_2$ be the matrix obtained by deleting columns in $H$, only keeping those columns containing pivots, as well as keeping the $n$th column. Define $K_2$ similarly. Thus we have deleted the same columns in $H$ and $K$, so $H_2$ and $K_2$ are also row-equivalent. Say that the number of columns in $H_2$ and $K_2$ is $r+1$, and write the matrices on the form
    %
    \begin{equation*}
        H_2
            = \begin{pmatrix}
                I_r & h \\
                0   & h'
            \end{pmatrix}
        \quad \text{and} \quad
        K_2
            = \begin{pmatrix}
                I_r & k \\
                0   & k'
            \end{pmatrix},
    \end{equation*}
    %
    where $h,k \in \field^r$ and $h',k' \in \field^{m-r}$ are column vectors. Since $H_2$ and $K_2$ are row-equivalent, the systems $H_2 x = 0$ and $K_2 x = 0$ are solution equivalent. If $h' = 0$, then $H_2 x = 0$ has the solution $(-h,1)$. But this is also a solution to $K_2 x = 0$, so $h = k$ and $k' = 0$. If $h' \neq 0$, then $H_2 x = 0$ only has the trivial solution. But then $K_2 x = 0$ also only has the trivial solution, and hence $k' \neq 0$. But that must be because both $H_2$ and $K_2$ has a pivot in the rightmost column, so also in this case $H_2 = K_2$.
\end{proof}


\section{Invertible matrices}

Notice that elementary matrices are invertible, since elementary row operations are invertible.

\begin{lemma}
    If $A \in \mat{n}{\field}$, then the following are equivalent:
    %
    \begin{enumlem}
        \item \label{enum:lemma-A-invertible} $A$ is invertible,
        \item \label{enum:lemma-A-equivalent-to-I} $A$ is row-equivalent to $I_n$,
        \item \label{enum:lemma-A-elementary-matrix-product} $A$ is a product of elementary matrices, and
        \item \label{enum:lemma-only-trivial-solution} the system $Ax = 0$ has only the trivial solution $x = 0$.
    \end{enumlem}
\end{lemma}
% I don't think we will need (iii), but I guess I'll leave it.

\begin{proof}
\begin{proofsec}
    \item[\subcref{enum:lemma-A-invertible} $\Leftrightarrow$ \subcref{enum:lemma-A-equivalent-to-I}]
    Let $H \in \mat{n}{\field}$ be a row-reduced echelon matrix that is row-equivalent to $A$. Then $H = PA$, where $P \in \mat{n}{\field}$ is a product of elementary matrices. Then $A = P\inv H$, so $A$ is invertible if and only if $H$ is. But the only invertible row-reduced echelon matrix is the identity matrix, so (i) and (ii) are equivalent.
    
    \item[\subcref{enum:lemma-A-equivalent-to-I} $\implies$ \subcref{enum:lemma-A-elementary-matrix-product}]
    As above, there exists a product $P$ of elementary matrices such that $I_n = PA$, so $A = P\inv$.

    \item[\subcref{enum:lemma-A-elementary-matrix-product} $\implies$ \subcref{enum:lemma-A-invertible}]
    This is obvious since elementary matrices are invertible.

    \item[\subcref{enum:lemma-A-equivalent-to-I} $\Leftrightarrow$ \subcref{enum:lemma-only-trivial-solution}]
    If $A$ and $I_n$ are row-equivalent, then the systems $Ax = 0$ and $I_n x = 0$ have the same solutions. Conversely, assume that $Ax = 0$ only has the trivial solution. If $H \in \mat{m,n}{\field}$ is a row-reduced echelon matrix that is row-equivalent to $A$, then $Hx = 0$ has no nontrivial solution. Thus if $r$ is the number of nonzero rows in $H$, then $r \geq n$. But then $r = n$, so $H$ must be the identity matrix.
\end{proofsec}
\end{proof}


\begin{proposition}
    Let $A \in \mat{n}{\field}$. Then the following are equivalent:
    %
    \begin{enumprop}
        \item $A$ is invertible,
        \item $A$ has a left inverse, and
        \item $A$ has a right inverse.
    \end{enumprop}
\end{proposition}

\begin{proof}
    If $A$ has a left inverse, then $Ax = 0$ has no nontrivial solution, so $A$ is invertible. If $A$ has a right inverse $B \in \mat{n}{\field}$, i.e. $AB = I$, then $B$ has a left inverse and is thus invertible. But then $A$ is the inverse of $B$ and hence is itself invertible.
\end{proof}


\chapter{Coordinates}

\newcommand{\coordmap}[1]{\phi_{#1}}
\newcommand{\coordvec}[2]{[#1]_{#2}}
\newcommand{\basischange}[2]{\phi_{#1,#2}}
\newcommand{\mr}[3]{{}_{#1}[#2]_{#3}}
\newcommand{\basischangemat}[2]{\mr{#1}{\square}{#2}}
\newcommand{\lin}{\calL}
\newcommand{\smr}[1]{\calM(#1)} % standard matrix representation

\newcommand{\colvec}[1]{\begin{pmatrix}#1\end{pmatrix}}

For $A \in \mat{m,n}{\field}$ we define the map $M_A \colon \field^n \to \field^m$ by $M_A v = Av$.

\begin{proposition}
    \label{prop:smr-properties}
    Let $(e_1, \ldots, e_n)$ be the standard basis for $\field^n$. The map
    %
    \begin{align*}
        \calM \colon \lin(\field^n, \field^m) &\to \mat{m,n}{\field}, \\
        T &\mapsto \bigl( Te_1 \mid \cdots \mid Te_n \bigr),
    \end{align*}
    %
    is a linear isomorphism with inverse $A \mapsto M_A$. The matrix $\smr{T}$ is called the \emph{standard matrix representation} of $T$. If $T \colon \field^n \to \field^m$ and $S \colon \field^m \to \field^l$ are linear maps, then
    %
    \begin{enumprop}
        \item \label{enum:smr-vector-multiplication} $Tv = \smr{T}v$ for all $v \in \field^n$.
        
        \item \label{enum:smr-of-identity-map} $\smr{\id_{\field^n}} = I$.

        \item \label{enum:smr-multiplicative} $\smr{S \circ T} = \smr{S} \smr{T}$.

        \item \label{enum:smr-invertibility} $T$ is invertible if and only if $\smr{T}$ is invertible, in which case $\smr{T\inv} = \smr{T}\inv$.
    \end{enumprop}
\end{proposition}

\begin{proof}
    The map $A \mapsto M_A$ is clearly linear, so to prove the first point it suffices to show that this is the inverse of $\calM$. Let $T \in \lin(\field^n,\field^m)$. Then
    %
    \begin{equation*}
        M_{\smr{T}} \colvec{\alpha_1 \\ \vdots \\ \alpha_n}
            = \smr{T} \colvec{\alpha_1 \\ \vdots \\ \alpha_n}
            = \bigl( Te_1 \mid \cdots \mid Te_n \bigr) \colvec{\alpha_1 \\ \vdots \\ \alpha_n}
            = \sum_{i=1}^n \alpha_i Te_i
            = T \colvec{\alpha_1 \\ \vdots \\ \alpha_n}
    \end{equation*}
    %
    for $\alpha_1, \ldots, \alpha_n \in \field$. Conversely, for $A \in \mat{m,n}{\field}$ we have
    %
    \begin{equation*}
        \smr{M_A}
            = \bigl( M_A e_1 \mid \cdots \mid M_A e_n \bigr)
            = \bigl( A e_1 \mid \cdots \mid A e_n \bigr)
            = A,
    \end{equation*}
    %
    since $Ae_i$ is the $i$th column of $A$. We prove the remaining claims:
    %
    \begin{proofsec}
        \item[Proof of \subcref{enum:smr-vector-multiplication}]
        Simply notice that $Tv = M_{\smr{T}}v = \smr{T}v$.

        \item[Proof of \subcref{enum:smr-of-identity-map}]
        This is obvious from the definition of $\calM$.

        \item[Proof of \subcref{enum:smr-multiplicative}]
        Let $v \in \field^n$ and notice that
        %
        \begin{equation*}
            \smr{S \circ T}v
                = (S \circ T) v
                = S(Tv)
                = S(\smr{T}v)
                = \smr{S}\smr{T}v
        \end{equation*}
        %
        by \subcref{enum:smr-vector-multiplication}. Since this holds for all $v$, the claim follows.

        \item[Proof of \subcref{enum:smr-invertibility}]
        This follows easily from \subcref{enum:smr-of-identity-map} and \subcref{enum:smr-multiplicative}.
    \end{proofsec}
\end{proof}

Let $V$ be a finite-dimensional $\field$-vector space. If $\calV = (v_1, \ldots, v_n)$ is a basis for $V$, then for every $v \in V$ there are unique $\alpha_1, \ldots, \alpha_n \in \field$ such that $v = \sum_{i=1}^n \alpha_i v_i$. Hence the map $\coordmap{\calV} \colon V \to \field^n$ given by $\coordmap{\calV}(v) = (\alpha_1, \ldots, \alpha_n)$ is well-defined. Furthermore, it is clearly linear, and since $\calV$ is a basis it is also bijective, hence a linear isomorphism. The map $\coordmap{\calV}$ is called the \emph{coordinate map} with respect to $\calV$, and the vector $\coordvec{v}{\calV} = \coordmap{\calV}(v)$ is called the \emph{coordinate vector} of $v$ with respect to $\calV$.

Now let $\calW$ be another basis for $V$. The composition $\basischange{\calW}{\calV} = \coordmap{\calW} \circ \coordmap{\calV}\inv$ is called the \emph{change of basis operator} from $\calV$ to $\calW$, and this makes the diagram
%
\begin{equation}
    \label{eq:change-of-basis-diagram}
    \begin{tikzcd}[row sep=small]
        & \field^n
            \ar[dd, "\basischange{\calW}{\calV}"] \\
        V
            \ar[ru, "\coordmap{\calV}"]
            \ar[rd, "\coordmap{\calW}", swap] \\
        & \field^n
    \end{tikzcd}
\end{equation}
%
commute. Its standard matrix is denoted $\basischangemat{\calW}{\calV}$. This has the expected properties:

\begin{proposition}
    Let $\calV$, $\calW$ and $\calU$ be bases for a finite-dimensional $\field$-vector space $V$. Then
    %
    \begin{enumprop}
        \item \label{enum:basis-change-coordvec} $\coordvec{v}{\calW} = \basischange{\calW}{\calV} (\coordvec{v}{\calV})$ for all $v \in V$. In particular, $\coordvec{v}{\calW} = \basischangemat{\calW}{\calV} \cdot \coordvec{v}{\calV}$.

        \item \label{enum:basis-change-identity-map} $\basischange{\calV}{\calV}$ is the identity map. In particular, $\basischangemat{\calV}{\calV}$ is the identity matrix.

        \item $\basischange{\calU}{\calW} \circ \basischange{\calW}{\calV} = \basischange{\calU}{\calV}$. In particular, $\basischangemat{\calU}{\calW} \cdot \basischangemat{\calW}{\calV} = \basischangemat{\calU}{\calV}$.

        \item $\basischange{\calW}{\calV}$ (resp. $\basischangemat{\calW}{\calV}$) is invertible with inverse $\basischange{\calV}{\calW}$ (resp. $\basischangemat{\calV}{\calW}$).
    \end{enumprop}
\end{proposition}

\begin{proof}
    All claims about change of basis matrices follow by \cref{prop:smr-properties} from the corresponding claims about change of basis operators.

    The claim \subcref{enum:basis-change-coordvec} follows by commutativity of the diagram \cref{eq:change-of-basis-diagram}, i.e.
    %
    \begin{equation*}
        \basischange{\calW}{\calV} (\coordvec{v}{\calV})
            = (\coordmap{\calW} \circ \coordmap{\calV}\inv) \circ \coordmap{\calV}(v)
            = \coordmap{\calW}(v)
            = \coordvec{v}{\calW}.
    \end{equation*}
    %
    Claim \subcref{enum:basis-change-identity-map} is an immediate consequence of the definition of $\basischange{\calV}{\calV}$. The remaining claims are proved similarly to \subcref{enum:basis-change-coordvec}.
\end{proof}

Next consider a linear map $T \colon V \to W$. If $\calV \in V^n$ and $\calW \in W^m$ are bases for $V$ and $W$ respectively, then the diagram
%
\begin{equation*}
    \begin{tikzcd}
        V
            \ar[d, "T", swap]
            \ar[r, "\coordmap{\calV}"]
        & \field^n
            \ar[d, "\coordmap{\calW} \circ T \circ \coordmap{\calV}\inv"] \\
        W
            \ar[r, "\coordmap{\calW}", swap]
        & \field^n
    \end{tikzcd}
\end{equation*}
%
commutes. The map $\coordmap{\calW} \circ T \circ \coordmap{\calV}\inv$ is the \emph{basis representation of $T$} with respect to the bases $\calV$ and $\calW$. We show below that this is a linear map $\field^n \to \field^m$, so it has a standard matrix, which we denote $\mr{\calW}{T}{\calV}$. This is called the \emph{matrix representation} of $T$ with respect to the bases $\calV$ and $\calW$.

\begin{proposition}
    \label{prop:mr-properties}
    Let $V$ and $W$ be finite-dimensional $\field$-vector spaces with bases $\calV \in V^n$ and $\calW \in W^m$, respectively. The map
    %
    \begin{align*}
        \mr{\calW}{\,\cdot\,}{\calV} \colon \lin(V,W) &\to \mat{m,n}{\field}, \\
        T &\mapsto \mr{\calW}{T}{\calV},
    \end{align*}
    %
    is a linear isomorphism. Let $T \colon V \to W$ and $S \colon W \to U$ be linear maps, and let $\calU \in U^l$ be a basis for $U$. Then
    %
    \begin{enumprop}
        \item \label{enum:mr-vector-multiplication} $\coordvec{Tv}{\calW} = \mr{\calW}{T}{\calV} \cdot \coordvec{v}{\calV}$ for all $v \in V$.

        \item \label{enum:mr-of-identity-map} If $\calV'$ is another basis for $V$, then $\mr{\calV'}{\id_V}{\calV} = \basischangemat{\calV'}{\calV}$.

        \item \label{enum:mr-multiplicative} $\mr{\calU}{S \circ T}{\calV} = \mr{\calU}{S}{\calW} \cdot \mr{\calW}{T}{\calV}$.

        \item \label{enum:mr-invertibility} $T$ is invertible if and only if $\mr{\calW}{T}{\calV}$ is invertible, in which case $\mr{\calV}{T\inv}{\calW} = \mr{\calW}{T}{\calV}\inv$.
    \end{enumprop}
\end{proposition}

\begin{proof}
    For the first claim, notice that the map $T \mapsto \coordmap{\calW} \circ T \circ \coordmap{\calV}\inv$ is a linear isomorphism, since pre- and postcomposition with linear isomorphisms are themselves linear isomorphisms. Composing this map with $\calM$ yields $\mr{\calW}{\,\cdot\,}{\calV}$, so this is a linear isomorphism by \cref{prop:smr-properties}.
    %
    \begin{proofsec}
        \item[Proof of \subcref{enum:mr-vector-multiplication}]
        Notice that
        %
        \begin{align*}
            \coordvec{Tv}{\calW}
                &= (\coordmap{\calW} \circ T)(v) \\
                &= (\coordmap{\calW} \circ T \circ \coordmap{\calV}\inv) \circ \coordmap{\calV}(v) \\
                &= (\coordmap{\calW} \circ T \circ \coordmap{\calV}\inv)(\coordvec{v}{\calV}) \\
                &= \mr{\calW}{T}{\calV} \cdot \coordvec{v}{\calV}.
        \end{align*}
        %
        where the last equality follows from \cref{enum:smr-vector-multiplication}.

        \item[Proof of \subcref{enum:mr-of-identity-map}]
        This is obvious from the definitions of $\mr{\calV'}{\id_V}{\calV}$ and $\basischangemat{\calV'}{\calV}$

        \item[Proof of \subcref{enum:mr-multiplicative}]
        Notice that
        %
        \begin{equation*}
            \coordmap{\calU} \circ (S \circ T) \circ \coordmap{\calV}\inv
                = (\coordmap{\calU} \circ S \circ \coordmap{\calW}\inv) \circ (\coordmap{\calW} \circ T \circ \coordmap{\calV}\inv)
        \end{equation*}
        %
        The claim then follows from \cref{enum:smr-multiplicative}.

        \item[Proof of \subcref{enum:mr-invertibility}]
        This is an immediate consequence of either \subcref{enum:mr-multiplicative} or of \cref{enum:smr-invertibility}.
    \end{proofsec}
\end{proof}


\begin{proposition}
    Let $\calV = (v_1, \ldots, v_n)$ be a basis for an $\field$-vector space $V$, and let $T \colon V \to V$ be a linear isomorphism. Let $\calW = (w_1, \ldots, w_n)$ where $w_i = Tv_i$. Then $\calW$ is a basis for $V$ and
    %
    \begin{equation*}
        \basischange{\calW}{\calV}
            = \coordmap{\calV} \circ T\inv \circ \coordmap{\calV}\inv,
        \quad \text{or} \quad
        \basischangemat{\calW}{\calV}
            = \mr{\calV}{T\inv}{\calV}.
    \end{equation*}
    %
    In particular, if $V = \field^n$ and $\calV$ is the standard basis $\calE$, then
    %
    \begin{equation*}
        \basischange{\calW}{\calE}
            = T\inv,
        \quad \text{or} \quad
        \basischangemat{\calW}{\calE}
            = \smr{T\inv}.
    \end{equation*}
\end{proposition}
%
We think of this result as follows: If we change basis by applying an invertible linear transformation $T$, we obtain the coordinate vectors corresponding to the transformed basis by applying $T\inv$ (in the old basis). This says that if we perform a \emph{passive transformation}, i.e. a change of basis while keeping vectors themselves fixed, the coordinates change by the inverse of said transformation.

\begin{proof}
    Let $v \in V$ and write $v = \sum_{i=1}^n \alpha_i v_i$. Then
    %
    \begin{equation*}
        Tv
            = \sum_{i=1}^n \alpha_i Tv_i
            = \sum_{i=1}^n \alpha_i w_i
            = \coordmap{\calW}\inv \colvec{\alpha_1 \\ \vdots \\ \alpha_n}
            = \coordmap{\calW}\inv \circ \coordmap{\calV}(v),
    \end{equation*}
    %
    implying that
    %
    \begin{equation*}
        \basischange{\calW}{\calV}
            = \coordmap{\calW} \circ \coordmap{\calV}\inv
            = (T \circ \coordmap{\calV}\inv)\inv \circ \coordmap{\calV}\inv
            = \coordmap{\calV} \circ T\inv \circ \coordmap{\calV}\inv
    \end{equation*}
    %
    as claimed.
\end{proof}


\chapter{Determinants}

\section{Existence of determinants}

If $M_1, \ldots, M_n, N$ are modules over a commutative ring $R$, a map
%
\begin{equation*}
    \phi \colon M_1 \prod \cdots \prod M_n \to N
\end{equation*}
%
is called \emph{$n$-linear} if the maps $m_i \mapsto \phi(m_1, \ldots, m_n)$ are linear for all $m_i \in M_i$. Since $\mat{m,n}{R} \cong (R^m)^n$, a map $\phi \colon \mat{m,n}{R} \to N$ that is linear in each row is also called $n$-linear.

In the case $M_1 = \cdots = M_n$, we call $\phi$ \emph{alternating} if $\phi(m_1, \ldots, m_n) = 0$ whenever $m_i = m_j$ for some $i \neq j$. Furthermore, $\phi$ is called \emph{skew-symmetric} if
%
\begin{multline*}
    \phi(m_1, \ldots, m_{i-1}, m_i, m_{i+1}, \ldots, m_{j-1}, m_j, m_{j+1}, \ldots, m_n) \\
        = -\phi(m_1, \ldots, m_{i-1}, m_j, m_{i+1}, \ldots, m_{j-1}, m_i, m_{j+1}, \ldots, m_n)
\end{multline*}
%
for all $i < j$.

\begin{lemma}
    Let $M$ and $N$ be $R$-modules, and let $\phi \colon M^n \to N$ be an $n$-linear map.
    %
    \begin{enumlem}
        \item \label{enum:alternating-implies-skew-symmetric} If $\phi$ is alternating, then $\phi$ is skew-symmetric.
        \item \label{enum:alternating-adjacent-rows} If $\phi(m_1, \ldots, m_n) = 0$ whenever $m_i = m_{i+1}$ for some $i = 1, \ldots, n-1$, then $\phi$ is alternating.
    \end{enumlem}
\end{lemma}

\begin{proof}
\begin{proofsec}
    \item[\subcref{enum:alternating-implies-skew-symmetric}]
    Consider $m_1, \ldots, m_n \in M$, and let $1 \leq i < j \leq n$. Define a map $\psi \colon M \prod M \to N$ by
    %
    \begin{equation*}
        \psi(a, b)
            = \phi(m_1, \ldots, m_{i-1}, a, m_{i+1}, \ldots, m_{j-1}, b, m_{j+1}, \ldots, m_n),
    \end{equation*}
    %
    and notice that it suffices to show that $\psi(m_i,m_j) = -\psi(m_j,m_i)$. But $\psi$ is $2$-linear and alternating, so for $a,b \in M$ we have
    %
    \begin{equation*}
        \psi(a+b, a+b)
            = \psi(a,a) + \psi(a,b) + \psi(b,a) + \psi(b,b)
            = \psi(a,b) + \psi(b,a).
    \end{equation*}
    %
    Thus $\psi(m_i,m_j) = -\psi(m_j,m_i)$, so $\phi$ is skew-symmetric as claimed.

    \item[\subcref{enum:alternating-adjacent-rows}] 
    The argument above shows that, in particular, if $A, B \in M^n$, and $B$ is obtained from $A$ by interchanging two adjacent elements, then $\phi(B) = -\phi(A)$. Assuming now that $B$ is obtained from $A$ by interchanging the $i$th and $j$th elements in $A$, with $i < j$, we claim that we may obtain $B$ by successively interchanging adjacent elements of $A$. Writing $A = (m_1, \ldots, m_n)$, we first perform $j - i$ such interchanges and arrive that the tuple
    %
    \begin{equation*}
        (m_1, \ldots, m_{i-1}, m_{i+1}, \ldots, m_{j-1}, m_j, m_i, m_{j+1}, \ldots, m_n),
    \end{equation*}
    %
    moving $m_i$ to the right $j - i$ places. Next we perform another $j-i-1$ interchanges, moving $m_j$ to the left until we reach
    %
    \begin{equation*}
        B = (m_1, \ldots, m_{i-1}, m_j, m_{i+1}, \ldots, m_{j-1}, m_i, m_{j+1}, \ldots, m_n).
    \end{equation*}
    %
    Since each interchange results in a sign change, we have
    %
    \begin{equation*}
        \phi(B) = (-1)^{2(j-i) - 1} \phi(A) = -\phi(A).
    \end{equation*}
    %
    If $m_i = m_j$ for $i < j$, then we claim that $\phi(A) = 0$. For let $B$ be obtained from $A$ by interchanging $m_{i+1}$ and $m_j$. Then $\phi(B) = 0$, so $\phi(A) = -\phi(B) = 0$ by the above argument, and hence $\phi$ is alternating as claimed.
    \end{proofsec}
\end{proof}

\begin{definition}
    If $n$ be a positive integer, a \emph{determinant function} is a map $\phi \colon \mat{n}{R} \to R$ that is $n$-linear, alternating, and which satisfies $\phi(I_n) = 1$.
\end{definition}
%
If $A \in \mat{n}{R}$ with $n > 1$ and $1 \leq i,j \leq n$, denote by $M(A)_{i,j}$ the matrix in $\mat{n-1}{R}$ obtained by removing the the $i$th row and the $j$th column of $A$. This is called the \emph{$(i,j)$-th minor} of $A$. If $\phi \colon \mat{n-1}{R} \to R$ is an $(n-1)$-linear function and $A \in \mat{n}{R}$, then we write $\phi_{i,j}(A) = \phi(M(A)_{i,j})$. Then $\phi_{i,j} \colon \mat{n}{R} \to R$ is clearly linear in all rows except row $i$, and is independent of row $i$.

\begin{theorem}
    \label{thm:determinant-recursive-definition}
    Let $n > 1$, and let $\phi \colon \mat{n-1}{R} \to R$ be alternating and $(n-1)$-linear. For $j = 1, \ldots, n$ define a map $\psi_j \colon \mat{n}{R} \to R$ by
    %
    \begin{equation*}
        \psi_j(A)
            = \sum_{i=1}^n (-1)^{i+j} a_{ij} \phi_{i,j}(A),
    \end{equation*}
    %
    for $A = (a_{ij}) \in \mat{n}{R}$. Then $\psi_j$ is alternating and $n$-linear. If $\phi$ is a determinant function, then so is $\psi_j$.
\end{theorem}

\begin{proof}
    Let $A = (a_{ij}) \in \mat{n}{R}$. Then $A \mapsto a_{ij}$ is independent of all rows except row $i$, and $\phi_{i,j}$ is linear in all rows except row $i$. Thus $A \mapsto a_{ij} \phi_{i,j}(A)$ is linear in all rows except row $i$. Conversely, $A \mapsto a_{ij}$ is linear in row $i$, and $\phi_{i,j}$ is independent of row $i$, so $A \mapsto a_{ij} \phi_{i,j}(A)$ is also linear in row $i$. Since $\psi_j$ is a linear combination of $n$-linear maps, is it itself $n$-linear.

    Now assume that $A$ has two equal adjacent rows, say $a_k, a_{k+1} \in R^n$. If $i \neq k$ and $i \neq k+1$, then $M(A)_{i,j}$ has two equal rows, so $\phi_{i,j}(A) = 0$. Thus
    %
    \begin{equation*}
        \psi_j(A)
            = (-1)^{k+j} a_{kj} \phi_{k,j}(A)
              + (-1)^{k+1+j} a_{(k+1)j} \phi_{k+1,j}(A).
    \end{equation*}
    %
    Since $a_k = a_{k+1}$ we also have $a_{kj} = a_{(k+1)j}$ and $M(A)_{k,j} = M(A)_{k+1,j}$. Thus $\psi_j(A) = 0$, so \cref{enum:alternating-adjacent-rows} implies that $\psi_j$ is alternating.

    Finally suppose that $\phi$ is a determinant function. Then $M(I_n)_{j,j} = I_{n-1}$ and we have
    %
    \begin{equation*}
        \psi_j(I_n)
            = (-1)^{j+j} \phi_{j,j}(I_n)
            = \phi(I_{n-1})
            = 1,
    \end{equation*}
    %
    so $\psi_j$ is also a determinant function.
\end{proof}


\begin{corollary}
    For every positive integer $n$, there exists a determinant function $\mat{n}{R} \to R$.
\end{corollary}

\begin{proof}
    The identity map on $\mat{1}{R} \cong R$ is a determinant function for $n = 1$, and \cref{thm:determinant-recursive-definition} allows us to recursively construct a determinant for each $n > 1$.
\end{proof}


\section{Uniqueness of determinants}

\begin{theorem}
    \label{thm:determinant-uniqueness}
    Let $n$ be a positive integer. There is precisely one determinant function on $\mat{n}{R}$, namely the function $\det \colon \mat{n}{R} \to R$ given by
    %
    \begin{equation*}
        \det A
            = \sum_{\sigma \in S_n} (\sign\sigma) a_{1 \sigma(1)} \cdots a_{n \sigma(n)}
    \end{equation*}
    %
    for $A = (a_{ij}) \in \mat{n}{R}$. If $\phi \colon \mat{n}{R} \to R$ is any alternating $n$-linear function, then
    %
    \begin{equation*}
        \phi(A)
            = (\det A) \phi(I_n).
    \end{equation*}
\end{theorem}
%
We use the notation $\det$ for the unique determinant on $\mat{n}{R}$ for all $n$.

\begin{proof}
    Let $e_1, \ldots, e_n$ denote the rows of $I_n$, and denote the rows of a matrix $A = (a_{ij}) \in \mat{n}{R}$ by $a_1, \ldots, a_n$. Then $a_i = \sum_{j=1}^n a_{ij} e_j$, so
    %
    \begin{equation*}
        \phi(A)
            = \sum_{k_1, \ldots, k_n} a_{1k_1} \cdots a_{nk_n} \phi(e_{k_1}, \ldots, e_{k_n}),
    \end{equation*}
    %
    where the sum is taken over all $k_i = 1, \ldots, n$. Since $\phi$ is alternating we have $\phi(e_{k_1}, \ldots, e_{k_n}) = 0$ if two of the indices $k_1, \ldots, k_n$ are equal. Thus it suffices to sum over those sequences $(k_1, \ldots, k_n)$ that are permutations of $(1, \ldots, n)$, and so
    %
    \begin{equation*}
        \phi(A)
            = \sum_{\sigma \in S_n} a_{1 \sigma(1)} \cdots a_{n \sigma(n)} \phi(e_{\sigma(1)}, \ldots, e_{\sigma(n)}).
    \end{equation*}
    %
    Next notice that, since $\phi$ is also skew-symmetric by \cref{enum:alternating-implies-skew-symmetric}, we have $\phi(e_{\sigma(1)}, \ldots, e_{\sigma(n)}) = (-1)^m \phi(e_1, \ldots, e_n)$, where $m$ is the number of transpositions of $(1, \ldots, n)$ it takes to obtain the permutation $(\sigma(1), \ldots, \sigma(n))$. But then $(-1)^m$ is just the sign of $\sigma$, so
    %
    \begin{equation*}
        \phi(A)
            = \sum_{\sigma \in S_n} (\sign\sigma) a_{1 \sigma(1)} \cdots a_{n \sigma(n)} \phi(I_n).
    \end{equation*}
    %
    Finally, if $\phi$ is a determinant function, then $\phi(I_n) = 1$, so we must have $\phi = \det$. The rest of the theorem follows directly from this.
\end{proof}


\section{Properties of determinants}

\begin{theorem}
    Let $A,B \in \mat{n}{R}$. Then
    %
    \begin{equation*}
        \det AB
            = (\det A) (\det B).
    \end{equation*}
    %
    In particular, $\det \colon \matGL{n}{R} \to R^*$ is a group homomorphism.
\end{theorem}

\begin{proof}
    The map $\phi \colon \mat{n}{R} \to R$ given by $\phi(A) = \det AB$ is clearly $n$-linear and alternating. Hence $\phi(A) = (\det A) \phi(I)$, and $\phi(I) = \det B$.

    Furthermore, if $A$ is invertible, then $1 = \det I = (\det A) (\det A\inv)$. Thus $\det A \in R^*$, so $\det$ is a group homomorphism as claimed.
\end{proof}


\begin{proposition}
    Let $A_{11}, \ldots, A_{nn}$ be square matrices with entries in $R$ and consider the block matrix
    %
    \begin{equation*}
        M
            = \begin{pmatrix}
                A_{11} & A_{12} & \cdots & A_{1n} \\
                0      & A_{22} & \cdots & A_{2n} \\
                \vdots & \ddots & \ddots & \vdots \\
                0      & \cdots & 0      & A_{nn}
            \end{pmatrix},
    \end{equation*}
    %
    where the remaining $A_{ij}$ are matrices of appropriate dimensions. Then $\det M = \bigprod_{i=1}^n \det A_{ii}$.
\end{proposition}

\begin{proof}
    By induction it suffices to consider the case where $M$ has the block form
    %
    \begin{equation*}
        M
            = \begin{pmatrix}
                A & C \\
                0 & B
            \end{pmatrix},
    \end{equation*}
    %
    where $A \in \mat{r}{R}$, $B \in \mat{s}{R}$ and $C \in \mat{r,s}{R}$ for appropriate integers $r,s$. Notice that if we define the matrices
    %
    \begin{equation*}
        M_1
            = \begin{pmatrix}
                I_r & 0 \\
                0   & B
            \end{pmatrix}
        \quad \text{and} \quad
        M_2
            = \begin{pmatrix}
                A & C   \\
                0 & I_s
            \end{pmatrix},
    \end{equation*}
    %
    then $M = M_1 M_2$. But using \cref{thm:determinant-recursive-definition} we easily see that $\det M_1 = \det B$ and $\det M_2 = \det A$, so it follows that
    %
    \begin{equation*}
        \det M
            = (\det M_1) (\det M_2)
            = (\det A) (\det B)
    \end{equation*}
    %
    as desired.
\end{proof}

% \begin{proof}
%     By induction it suffices to consider the case where $A$ has the block form
%     %
%     \begin{equation*}
%         A
%             = \begin{pmatrix}
%                 B & C \\
%                 0 & D
%             \end{pmatrix}.
%     \end{equation*}
%     %
%     Say that $B \in \mat{r}{R}$ and $D \in \mat{s}{R}$, and put $\phi(B,C,D) = \det A$. Then $D \mapsto \phi(B,C,D)$ is clearly $s$-linear and alternating, so \cref{thm:determinant-recursive-definition} implies that
%     %
%     \begin{equation*}
%         \phi(B,C,D)
%             = (\det D) \phi(B,C,I_s).
%     \end{equation*}
%     %
%     By subtracting multiples of the rows of $I_s$ from $C$ we obtain $\phi(B,C,I_s) = \phi(B,0,I_s)$. Next, $B \mapsto \phi(B,0,I_s)$ is also $r$-linear and alternating, so
%     %
%     \begin{equation*}
%         \phi(B,0,I_s)
%             = (\det B) \phi(I_r,0,I_s).
%     \end{equation*}
%     %
%     But $\phi(I_r,0,I_s) = 1$, so summarising we have
%     %
%     \begin{equation*}
%         \phi(B,C,D)
%             = (\det D) \phi(B,C,I_s)
%             = (\det D) \phi(B,0,I_s)
%             = (\det D) (\det B),
%     \end{equation*}
%     %
%     as desired.
% \end{proof}


\begin{proposition}
    Let $A \in \mat{n}{R}$. Then $\det A = \det A\trans$.
\end{proposition}

\begin{proof}
    Writing $A = (a_{ij})$, first notice that
    %
    \begin{equation*}
        \det A\trans
            = \sum_{\sigma \in S_n} (\sign \sigma\inv) a_{\sigma(1)1} \cdots a_{\sigma(n)n},
    \end{equation*}
    %
    since $\sign \sigma = \sign \sigma\inv$. Next notice that, if $j = \sigma(i)$, then $a_{\sigma(i)i} = a_{j \sigma\inv(j)}$. Since $R$ is commutative, it follows that
    %
    \begin{equation*}
        \det A\trans
            = \sum_{\sigma \in S_n} (\sign \sigma\inv) a_{1\sigma\inv(1)} \cdots a_{n\sigma\inv(n)},
    \end{equation*}
    %
    and since $\sigma \mapsto \sigma\inv$ is a bijection on $S_n$, it follows that $\det A\trans = \det A$ as desired.
\end{proof}

Let $A \in \mat{n}{R}$. For $1 \leq i,j \leq n$, the \emph{$(i,j)$-th cofactor} of $A$ is the number $A_{i,j} = (-1)^{i+j} \det M(A)_{i,j}$, where we recall that $M(A)_{i,j}$ is the $(i,j)$-th minor of $A$. The \emph{adjoint matrix} of $A$ is the matrix $\adj A \in \mat{n}{R}$ whose $(i,j)$-th entry is the cofactor $A_{j,i}$. Note that
%
\begin{equation*}
    (A\trans)_{i,j}
        = (-1)^{i+j} \det M(A\trans)_{i,j}
        = (-1)^{j+i} \det M(A)_{j,i}
        = A_{j,i},
\end{equation*}
%
so $\adj A\trans = (\adj A)\trans$. We have the following:

\begin{proposition}
    \label{thm:adjoint-matrix-product}
    Let $A \in \mat{n}{R}$. Then
    %
    \begin{equation*}
        (\adj A) A
            = (\det A) I
            = A (\adj A).
    \end{equation*}
\end{proposition}

\begin{proof}
    Writing $A = (a_{ij})$ and fixing some $j \in \{1, \ldots, n\}$, \cref{thm:determinant-recursive-definition} implies that
    %
    \begin{equation*}
        \det A
            = \sum_{i=1}^n (-1)^{i+j} a_{ij} \det M(A)_{i,j}
            = \sum_{i=1}^n a_{ij} A_{i,j},
    \end{equation*}
    %
    which is just the $(j,j)$-th entry in the product $(\adj A)A$.

    Next we claim that if $k \neq j$, then $\sum_{i=1}^n a_{ik} A_{i,j} = 0$. Let $B = (b_{ij}) \in \mat{n}{R}$ be the matrix obtained from $A$ by replacing the $j$th column of $A$ by its $k$th column. Then $B$ has two equal columns, so $\det B = 0$. Also, $b_{ij} = a_{ik}$ and $M(B)_{i,j} = M(A)_{i,j}$, so it follows that
    %
    \begin{align*}
        0
            &= \det B
             = \sum_{i=1}^n (-1)^{i+j} b_{ij} \det M(B)_{i,j} \\
            &= \sum_{i=1}^n (-1)^{i+j} a_{ik} \det M(A)_{i,j}
             = \sum_{i=1}^n a_{ik} A_{i,j}.
    \end{align*}
    %
    That is, the $(j,k)$-th entry of the product $(\adj A)A$ is zero, so the off-diagonal entries of $(\adj A)A$ are zero. In total we thus have $(\adj A)A = (\det A) I$.

    Finally we prove the equality $A(\adj A) = (\det A) I$, Applying the first equality to $A\trans$ yields
    %
    \begin{equation*}
        (\adj A\trans) A\trans
            = (\det A\trans)I
            = (\det A)I,
    \end{equation*}
    %
    and transposing we get
    %
    \begin{equation*}
        A (\adj A)
            = A (\adj A\trans)\trans
            = (\det A) I
    \end{equation*}
    %
    as desired.
\end{proof}


\begin{corollary}
    Let $A \in \mat{n}{R}$. Then $A$ is a unit in $\mat{n}{R}$ if and only if $\det A$ is a unit in $R$.
\end{corollary}

\begin{proof}
    This follows directly from \cref{thm:adjoint-matrix-product}.
\end{proof}


\section{Determinants and eigenvalues}

Let $V$ be a vector space of dimension $n < \infty$. If $T \in \calL(V)$, then recall that an \emph{eigenvalue} of $T$ is an element $\lambda \in F$ such that there is a nonzero vector $v \in V$ with $Tv = \lambda v$. The set of eigenvalues of $T$ is called the \emph{spectrum} of $T$ and is denoted $\spec T$. Clearly $\lambda \in \spec T$ if and only if $\lambda I - T$ is not injective, i.e. if $\det(\lambda I - T) = 0$. This motivates the definition of the \emph{characteristic polynomial} $p_T(t) \in F[t]$ of $T$, given by $p_T(t) = \det(tI - T)$. The eigenvalues of $T$ are then precisely the roots of $p_T(t)$.

\begin{proposition}
    Let $T \in \calL(V)$.
    %
    \begin{enumprop}
        \item $p_T(t)$ is a monic polynomial of degree $n$.
        \item The constant term of $p_T(t)$ equals $(-1)^n \det T$.
        \item The coefficient of $t^{n-1}$ in $p_T(t)$ equals $-\trace T$.
    \end{enumprop}
    %
    Assume further that $p_T(t)$ splits over $F$. Then:
    %
    \begin{enumprop}[resume]
        \item $T$ has an eigenvalue.
        \item $\det T$ is the product of the eigenvalues of $T$.
        \item $\trace T$ is the sum of the eigenvalues of $T$.
    \end{enumprop}
\end{proposition}
%
The condition that $p_T(t)$ splits over $F$ means that $p_T(t)$ decomposes into a product of linear factors on the form $t - a \in F[t]$ (up to multiplication by a constant). This is in particular the case if $F$ is algebraically closed.

\begin{proof}
\begin{proofsec}
    \item[(i)]
    Let $A = (a_{ij}) \in \mat{n}{F}$ be a matrix representation of $T$. The $(i,j)$-th entry of $tI - A$ is then $t\delta_{ij} - a_{ij}$, so
    %
    \begin{equation}
        \label{eq:characteristic-polynomial-Leibniz}
        \det(tI - T)
            = \sum_{\sigma \in S_n} (\sign\sigma) (t\delta_{1\sigma(1)} - a_{1\sigma(1)}) \cdots (t\delta_{n\sigma(n)} - a_{n \sigma(n)})
    \end{equation}
    %
    by \cref{thm:determinant-uniqueness}. Thus $p_T(t)$ is a polynomial in $t$. Furthermore, the only entries in $tI - A$ containing $t$ are the diagonal entries, and the largest number of such entries occurring in a single term of \cref{eq:characteristic-polynomial-Leibniz} is $n$, so $\deg p_T(t) \leq n$. But notice that there is only one term in which $t$ appears $n$ times, namely the term corresponding to the identity permutation in $S_n$, giving the product of the diagonal entries in $tI-A$. This term equals
    %
    \begin{equation}
        \label{eq:characteristic-polynomial-diagonal-product}
        (t-a_{11})(t-a_{22}) \cdots (t-a_{nn}),
    \end{equation}
    %
    and multiplying out we see that the only resulting term containing $t^n$ is $t^n$ itself. Hence $p_T(t)$ is monic and of degree $n$. Thus we may write $p_T(t) = \sum_{i=0}^n c_i t^i$ for appropriate $c_0, \ldots, c_n \in F$.

    \item[(ii)]
    Simply notice that
    %
    \begin{equation*}
        (-1)^n \det T
            = \det(-T)
            = p_T(0)
            = c_0
    \end{equation*}
    %
    by $n$-linearity of $\det$ and the definition of $p_T(t)$.

    \item[(iii)]
    The only way for one of the terms in \cref{eq:characteristic-polynomial-Leibniz} to contain the factor $t^{n-1}$ is for at least $n-1$ of the $b_{ij}$ to be a diagonal element. But in choosing $n-1$ elements along the diagonal we are forced to also choose the final diagonal element, since otherwise $\sigma$ would not be a permutation. Hence the factor $t^n$ can only appear in the product \cref{eq:characteristic-polynomial-diagonal-product}. It is then clear that
    %
    \begin{equation*}
        c_{n-1}
            = - (a_{11} + \cdots + a_{nn})
            = - \trace T
    \end{equation*}
    %
    as claimed.

    \item[(iv)]
    Now assume that $p_T(t)$ splits over $F$. Then some linear factor $t-\lambda \in F[t]$ divides $p_T(t)$, which implies that $\lambda \in F$ is an eigenvalue of $T$.
    
    \item[(v)]
    Since $p_T(t)$ is monic we have
    %
    \begin{equation*}
        p_T(t)
            = (t - \lambda_1) (t - \lambda_2) \cdots (t - \lambda_n)
    \end{equation*}
    %
    for appropriate $\lambda_1, \ldots, \lambda_n \in F$. These are then the (not necessarily distinct) eigenvalues of $T$. Thus $p_T(0) = (-1)^n \lambda_1 \cdots \lambda_n$, and the claim follows from (ii).

    \item[(vi)]
    We similarly find that $c_{n-1} = -(\lambda_1 + \cdots + \lambda_n)$, so the final claim follows from (iii).
\end{proofsec}
\end{proof}


\section{Proofs without determinants}

\subsection{Existence of eigenvalues}

% \begin{theorem}[The spectral mapping theorem]
%     Let $F$ be an algebraically closed field, let $A \in \mat{n}{F}$ and let $p(t) \in F[t]$. Then
%     %
%     \begin{equation*}
%         \spec p(A) = p(\spec A).
%     \end{equation*}
% \end{theorem}

% \begin{proof}
%     First let $\lambda \in \spec A$, and choose a corresponding eigenvector $v \in F^n$. If we write $p(t) = \sum_{i=0}^n a_i t^i$, then
%     %
%     \begin{equation*}
%         p(A)v
%             = \biggl( \sum_{i=0}^n a_i A^i \biggr) v
%             = \sum_{i=0}^n a_i A^i v
%             = \sum_{i=0}^n a_i \lambda^i v
%             = \biggl( \sum_{i=0}^n a_i \lambda^i \biggr) v
%             = p(\lambda) v.
%     \end{equation*}
%     %
%     Thus $p(\lambda) \in \spec p(A)$, and so $p(\spec A) \subseteq \spec p(A)$.

%     For the opposite inclusion, let $\lambda \in \spec p(A)$, i.e. $(p(A) - \lambda I) v = 0$ for some nonzero $v \in F^n$.
% \end{proof}

\newcommand{\ev}{\mathrm{ev}}

Assume that $F$ is algebraically closed, and consider $T \in \calL(V)$. For $d \in \naturals$, let $F[t]_d$ denote the vector space of polynomials in $F[t]$ with degree strictly less than $d$, such that $\dim F[t]_d = d$. Consider the map $\ev_T \colon F[t]_{n^2+1} \to \calL(V)$ given by $\ev_T(p) = p(T)$. This cannot be injective, so there is some nonzero $p(t) \in F[t]_{n^2+1}$ such that $p(T) = 0$. Note that $p(t)$ cannot be constant.

Since $F$ is algebraically closed, there exist $c, \lambda_1, \ldots, \lambda_m \in F$ such that $p(t) = c \bigprod_{i=1}^m (t - \lambda_i)$. But then
%
\begin{equation*}
    0
        = p(T)
        = c \bigprod_{i=1}^m (T - \lambda_i I),
\end{equation*}
%
so at least one $T - \lambda_i I$ is not injective. Hence $\lambda_i$ is an eigenvalue of $T$.


\subsection{Trace is sum of eigenvalues}

\newcommand{\Span}{\operatorname{span}}


\begin{corollary}
    Let $F$ be algebraically closed, and let $T \in \calL(V)$. Then the sum of the eigenvalues of $T$ is $\trace T$.
\end{corollary}

\begin{proof}
    Let $A \in \mat{n}{F}$ be an upper triangular matrix for $T$. The diagonal elements of $A$ are the eigenvalues, and the trace of $T$ is just the sum of these elements.
\end{proof}


\chapter{Triangularisation and diagonalisation}

\section{Triangularisation}

\begin{proposition}
    Let $V$ be an $F$-vector space with $n = \dim V < \infty$, and let $\calV = (v_1, \ldots, v_n)$ be an ordered basis for $V$. The matrix of an operator $T \in \calL(V)$ is upper triangular with respect to $\calV$ if and only if $\Span(v_1, \ldots, v_i)$ is invariant under $T$ for all $i \in \{1, \ldots, n\}$.
\end{proposition}

\begin{proof}
    This is obvious.
\end{proof}


\begin{lemma}
    Let $V$ be an $F$-vector space, and let $T \in \calL(V)$ be an isomorphism. If $U$ is a finite-dimensional subspace of $V$ that is invariant under $T$, then $U$ is also invariant under $T\inv$.
\end{lemma}

\begin{proof}
    Since $U$ is finite-dimensional and $T|_U \colon U \to U$ is injective, applying the rank--nullity theorem implies that $T|_U$ is also surjective. Hence if $u \in U$, then there exists a $v \in U$ such that $Tv = u$. It follows that
    %
    \begin{equation*}
        T\inv u
            = T\inv Tv
            = v
            \in U,
    \end{equation*}
    %
    so $U$ is invariant under $T\inv$.
\end{proof}


\begin{proposition}
    Let $V$ be a finite-dimensional $F$-vector space, and let $\calV$ be an ordered basis for $V$. If $T \in \calL(V)$ is an isomorphism that is upper triangular with respect to $\calV$, then $T\inv$ is also upper triangular with respect to $\calV$.

    In particular, the subset of $\matGL{n}{F}$ consisting of upper triangular matrices is a subgroup.
\end{proposition}

\begin{proof}
    This is an obvious consequence of the above two results.
\end{proof}


\begin{lemma}
    Let $A \in \mat{n}{F}$ be upper triangular. Then $A$ is invertible if and only if all its diagonal elements are nonzero.
\end{lemma}

\begin{proof}
    Denote the diagonal elements of $A$ by $\lambda_1, \ldots, \lambda_n$, and let $e_1, \ldots, e_n$ denote the standard basis of $F^n$. First assume that the diagonal elements are nonzero. Then notice that $e_1 \in R(A)$, and that
    %
    \begin{equation*}
        A e_i
            = a_1 e_1 + \cdots + a_{i-1} e_{i-1} + \lambda_i e_i
    \end{equation*}
    %
    for appropriate $a_1, \ldots, a_{i-1} \in F$. By induction we then have $e_i \in R(A)$. Since $(e_1, \ldots, e_n)$ is a basis, this implies that $R(A) = F^n$.

    Conversely, assume that some diagonal element $\lambda_i$ is zero. If $i = 1$, then $Ae_1 = 0$ so $A$ is singular. If $i > 0$, then
    %
    \begin{equation*}
        A \Span(e_1, \ldots, e_i)
            \subseteq \Span(e_1, \ldots, e_{i-1}),
    \end{equation*}
    %
    so again $A$ is singular.
\end{proof}


\begin{lemma}
    Let $A \in \mat{n}{F}$ be upper triangular. Then the eigenvalues of $A$ are its diagonal elements.
\end{lemma}

\begin{proof}
    Let $\lambda \in F$, and denote the diagonal elements of $A$ by $\lambda_1, \ldots, \lambda_n$. By [lemma], the matrix $\lambda I - A$ is singular if and only if $\lambda - \lambda_i = 0$ for some $i$, and hence $\lambda_1, \ldots, \lambda_n$ are the eigenvalues of $A$.
\end{proof}


\begin{proposition}
    Let $F$ be algebraically closed, and let $V$ be a finite-dimensional $F$-vector space. If $T \in \calL(V)$, then $V$ has an ordered basis with respect to which the matrix of $T$ is upper triangular.
\end{proposition}

\begin{proof}
    This is obvious if $\dim V = 1$, so assume that $n = \dim V > 1$, and assume that the claim is true for $F$-vector spaces of dimension $n-1$. Let $v_1 \in V$ be an eigenvector for $T$, and let $U = \Span(v_1)$. Since $U$ is invariant under $T$, we may define a linear operator\footnote{The operator $\tilde T$ may arise as follows: Let $\pi \colon V \to V/U$ be the quotient map. Then $U \subseteq \ker (\pi \circ T)$ since $U$ is invariant under $T$, so $\pi \circ T$ descends to a linear map $\tilde T \colon V/U \to V/U$.} $\tilde T \in \calL(V/U)$ by $\tilde T(v + U) = Tv + U$. Since $\dim V/U = n-1$, by induction there is a basis $v_2 + U, \ldots, v_n + U$ of $V/U$ with respect to which the matrix of $\tilde T$ is upper triangular. It is easy to show that the collection $v_1, \ldots, v_n$ is linearly independent, hence a basis for $V$.

    Now notice that
    %
    \begin{equation*}
        Tv_i + U
            = \tilde T(v_i + U)
            \in \Span(v_2 + U, \ldots, v_i + U)
    \end{equation*}
    %
    for $i \in \{2, \ldots, n\}$. That is, there exist $a_2, \ldots, a_i \in F$ such that
    %
    \begin{equation*}
        Tv_i + U
            = (a_2 v_2 + \cdots + a_i v_i) + U.
    \end{equation*}
    %
    But then $Tv_i \in \Span(v_1, \ldots, v_i)$ for all $i \in \{2, \ldots, n\}$, and since $U$ is invariant under $T$ this also holds for $i = 1$. Hence $T$ is upper triangular with respect to the basis $v_1, \ldots, v_n$ of $V$.
\end{proof}


\begin{theorem}[Schur's Theorem]
    Let $F$ be algebraically closed, and let $V$ be a finite-dimensional inner product space over $F$. If $T \in \calL(V)$, then $V$ has an ordered orthonormal basis with respect to which the matrix of $T$ is upper triangular.
\end{theorem}

\begin{proof}
    By [proposition] $V$ has an ordered basis $\calV = (v_1, \ldots, v_n)$ with respect to which the matrix of $T$ is upper triangular. Now apply the Gram--Schmidt procedure to $\calV$ and obtain an orthonormal basis $\calE = (e_1, \ldots, e_n)$ for $V$ such that
    %
    \begin{equation*}
        \Span(e_1, \ldots, e_i)
            = \Span(v_1, \ldots, v_i)
    \end{equation*}
    %
    for all $i \in \{1, \ldots, n\}$. Then [proposition with invariant subspaces] shows that the matrix of $T$ with respect to $\calE$ is also upper triangular, proving the claim.
\end{proof}


\section{Orthonormal diagonalisation}

Let $T \colon V \to V$ is an operator on an $F$-vector space $V$, and let $U$ be a subspace of $V$ that is invariant under $T$. Say that $W$ is a complement of $V$, i.e. that $V = U \oplus W$, then $W$ is not necessarily invariant under $T$. However, we have the following:

\begin{lemma}
    \label{thm:adjoint-invariant-subspace}
    Let $T \in \calL(V)$ be an operator on a finite-dimensional inner product space $V$. If a subspace $U$ of $V$ is invariant under $T$, then $U^\perp$ is invariant under $T^*$.
\end{lemma}

\begin{proof}
    Let $v \in U^\perp$. For $u \in U$ we have $Tu \in U$, so
    %
    \begin{equation*}
        \inner{T^*v}{u}.
            = \inner{v}{Tu}
            = 0.
    \end{equation*}
    %
    Since this holds for all $u \in U$, it follows that $T^*v \in U^\perp$ as desired.
\end{proof}

\begin{theorem}[The spectral theorem]
    Let $F$ be either the real or the complex numbers, let $V$ be a finite-dimensional inner product space over $F$, and consider $T \in \calL(V)$. Then $T$ is orthogonally diagonalisable if and only if
    %
    \begin{enumthm}
        \item $F = \reals$ and $T$ is self-adjoint, or
        \item $F = \complex$ and $T$ is normal.
    \end{enumthm}
\end{theorem}

\begin{proof}
    Assume that either $F = \reals$ and $T$ is self-adjoint, or that $F = \complex$ and $T$ is normal. We prove by induction in $n = \dim V$ that $T$ is orthogonally diagonalisable. If $n = 1$ then this follows since $T$ has an eigenvalue, so assume that the claim is proved for operators on spaces of dimension strictly less than $n$.

    Let $\lambda \in \spec T$, and consider the corresponding eigenspace $E_T(\lambda)$. If $d = \dim E_T(\lambda) = n$, then any orthonormal basis of $E_T(\lambda)$ will suffice. Assume therefore that $0 < d < n$.

    Clearly $E_T(\lambda)$ is invariant under $T$, and we claim that it is also invariant under $T^*$. If $T$ is self-adjoint this is obvious, and if $T$ is normal then for all $w \in E_T(\lambda)$,
    %
    \begin{equation*}
        TT^*w
            = T^*Tw
            = T^*(\lambda w)
            = \lambda T^* w,
    \end{equation*}
    %
    so we also have $T^*(w) \in E_T(\lambda)$. It follows from \cref{thm:adjoint-invariant-subspace} that $E_T(\lambda)^\perp$ is also invariant under both $T$ and $T^*$. We furthermore have $\dim E_T(\lambda)^\perp = n-d$ and $0 < n-d < n$. Let $T_\parallel \in \calL(E_T(\lambda))$ and $T_\perp \in \calL(E_T(\lambda)^\perp)$ denote the restrictions of $T$ to $E_T(\lambda)$ and $E_T(\lambda)^\perp$ respectively. Both these operators are also self-adjoint or normal, depending on the hypothesis, so the induction hypothesis furnishes orthonormal bases $\calU$ and $\calW$ for $E_T(\lambda)$ and $E_T(\lambda)^\perp$ consisting of eigenvectors of $T$. But then $\calV = \calU \union \calW$ is an orthonormal basis for $V$ as desired.

    Conversely, assume that $\calV$ is an orthonormal basis of $V$ consisting of eigenvectors for $T$, and let $A \in \mat{n}{F}$ be the matrix of $T$ with respect to $\calV$. Then $A$ is diagonal with the eigenvalues of $T$ on its diagonal. If $F = \reals$ then the eigenvalues of $T$ are real, so $A$ is a real symmetric matrix, and hence $T$ is self-adjoint. If instead $F = \complex$, then since $A$ is diagonal we have $A^*A = AA^*$, which implies that $T$ is normal.
\end{proof}


\chapter{Complex numbers}

It is well-known that a complex number $z = a + \iu b$ has a representation as a matrix
%
\begin{equation*}
    A =
    \begin{pmatrix}
        a & -b \\
        b &  a
    \end{pmatrix},
\end{equation*}
%
and that the subring of $\mat{2}{\reals}$ consisting of such matrices is isomorphic to $\complex$. Letting $r = \abs{z} = \sqrt{\det A}$ we obtain a matrix $Q = A/r \in \matSO{2}$. Let us call the pair $(r,Q)$ the \emph{geometric representation} of $z$.

Let $\complex^*$ denote the group of nonzero complex numbers under multiplication. We define an action of $\complex^*$ on $\reals^2$ as follows: If $v \in \reals^2$ then, in the notation above, we let $zv = rQv$; that is, $z$ acts on $v$ by applying the rotation matrix $Q$ and scaling by $r$.

Alternatively, given $v = (x,y) \in \reals^2$ form the complex number $w = x + \iu y$ with corresponding matrix
%
\begin{equation*}
    B =
    \begin{pmatrix}
        x & -y \\
        y &  x
    \end{pmatrix}.
\end{equation*}
%
Then $zw$ has the corresponding matrix $rQB$, the first column of which is $zv = rQv$. Thus the action of $\complex^*$ on $\reals^2$ is also obtained by considering a vector in $\reals^2$ as a complex number and performing complex multiplication.

\begin{lemma}
    The action of $\complex^*$ on $\reals^2$ preserves angles.
\end{lemma}

\begin{proof}
    Let $z \in \complex^*$ have have the geometric representation $(r,Q)$, and let $v, u \in \reals^2$. Then notice that
    %
    \begin{equation*}
        \inner{zv}{zu}
            = r^2 \inner{Qv}{Qu}
            = r^2 \inner{v}{u},
    \end{equation*}
    %
    since $Q$ is orthogonal. In particular we have $\norm{zv} = r \norm{v}$. If $\theta \in [0,\pi]$ is the angle between $zv$ and $zu$, then the Cauchy--Schwarz inequality implies that
    %
    \begin{equation*}
        \cos\theta
            = \frac{\inner{zv}{zu}}{\norm{zv}\,\norm{zu}}
            = \frac{r^2 \inner{v}{u}}{r^2 \norm{v}\,\norm{u}}
            = \frac{\inner{v}{u}}{\norm{v}\,\norm{u}},
    \end{equation*}
    %
    which is just the cosine of the angle between $v$ and $u$. This proves the lemma.
\end{proof}

Now let $U \subseteq \complex$ be a nonempty open set, and let $f \colon U \to \complex$ be a holomorphic function that does not attain the value zero.\footnote{If $f$ is not identically zero, then $f\preim(\complex^*)$ is a nonempty open subset of $\complex$, so this is a very natural assumption.} Considering $U$ and $\complex$ as real two-dimensional manifolds, let $T_p f \colon T_p U \to T_{f(p)} \complex$ be the tangent map of $f$ at $p \in U$. The Jacobian matrix of $f$ at $p$ is then simply the matrix corresponding to the complex number $f'(p)$, so if $v \in T_p U$, then the vector $T_p f(v) \in T_{f(p)} \complex \cong \reals^2$ is just the action of $f'(p)$ on $v$. The lemma then implies that, for $v,u \in T_p U$,
%
\begin{equation*}
    \inner{T_p f(v)}{T_p f(u)}
        = \inner{f'(p)v}{f'(p)u}
        = \abs{f'(p)}^2 \inner{v}{u}.
\end{equation*}
%
Since $f$ is holomorphic it is smooth as a function on $\reals^2$, the map $p \mapsto \abs{f'(p)}^2$ is also smooth and nonzero everywhere, and so $f$ is conformal.


\chapter{Gray codes}

\newcommand{\bin}[2][]{\mathrm{bin}_{#1}(#2)}
\newcommand{\gray}[2][]{\mathrm{gr}_{#1}(#2)}
\newcommand{\xor}{\oplus}
\newcommand{\shiftr}{^{\gg}}
\newcommand{\conc}{\circ}
\newcommand{\emptystring}{\lambda}

[This doesn't belong here, I just needed a LaTeX editor to write the proof.]

If $a$ and $b$ are binary strings of the same length, we denote the bitwise exclusive disjunction of $a$ and $b$ by $a \xor b$. We denote the concatenation of $a$ with $b$ either by $a \conc b$ or $ab$. Also, if $b$ is a binary string, denote by $b\shiftr$ the right logical shift of $b$, i.e. the string obtained by removing the rightmost bit of $b$ and appending a $0$ on the left of the result.

Let $n \in \naturals$. For a number $k \in \naturals$ with $k < 2^n$ we denote the $n$-bit binary representation of $k$ by $\bin[n]{k}$. Furthermore, we denote the $n$-bit Gray code for $k$ by $\gray[n]{k}$. By definition, $\gray[0]{0} = \emptystring$ and
%
\begin{equation*}
    \gray[n+1]{k}
        = \begin{cases}
            0 \conc \gray[n]{k},           & k < 2^n,    \\
            1 \conc \gray[n]{2^{n+1}-1-k}, & k \geq 2^n.
        \end{cases}
\end{equation*}
%
for all $n \in \naturals$ and $(n+1)$-bit numbers $k$. We claim the following:

\begin{proposition}
    Let $n \in \naturals$, and let $k \in \naturals$ be an $n$-bit number. Writing $\bin[n]{k} = b_{n-1} \cdots b_0$ we have $\gray[n]{k} = a_{n-1} \cdots a_0$, where $a_{n-1} = b_{n-1}$ and
    %
    \begin{equation}
        \label{eq:gray-consecutive-bits}
        a_i
            = b_{i+1} \xor b_i
    \end{equation}
    %
    for $i \in \{0, \ldots, n-2\}$. That is,
    %
    \begin{equation*}
        \gray[n]{k}
            = b_{n-1} (b_{n-1} \xor b_{n-2}) \cdots (b_1 \xor b_0).
    \end{equation*}
    %
    Conversely we have
    %
    \begin{equation*}
        b_i
            = a_i \xor \cdots \xor a_{n-1}.
    \end{equation*}
\end{proposition}
%
The formula \cref{eq:gray-consecutive-bits} also holds in the case $i = n-1$ if we let $b_n = 0$, i.e. we prepend zeros if necessary.

\begin{proof}
    If $n = 0$, then the claim is obvious since there are no $0$-bit numbers. Now let $k$ be an $(n+1)$-bit number, so that $k < 2^{n+1}$, and write $\bin[n+1]{k} = b_n \cdots b_0$. If $k < 2^n$, then $b_n = 0$ and $\gray[n+1]{k} = 0 \conc \gray[n]{k}$. By induction we have
    %
    \begin{align*}
        \gray[n]{k}
            &= b_{n-1} (b_{n-1} \xor b_{n-2}) \cdots (b_1 \xor b_0) \\
            &= (b_n \xor b_{n-1}) (b_{n-1} \xor b_{n-2}) \cdots (b_1 \xor b_0),
    \end{align*}
    %
    so it follows that
    %
    \begin{equation*}
        \gray[n+1]{k}
            = b_n \conc \gray[n]{k}
            = b_n (b_n \xor b_{n-1}) (b_{n-1} \xor b_{n-2}) \cdots (b_1 \xor b_0)
    \end{equation*}
    %
    as claimed. If instead $k \geq 2^n$, then $b_n = 1$. Writing $k = 2^n + r$ with $0 \leq r < 2^n$ we have $\bin[n]{r} = b_{n-1} \cdots b_0$. Now notice that $\bin[n]{2^n - 1 - r} = \conj{b}_{n-1} \cdots \conj{b}_0$ since
    %
    \begin{equation*}
        (\conj{b}_{n-1} \cdots \conj{b}_0)_2 + r + 1
            = (\conj{b}_{n-1} \cdots \conj{b}_0)_2 + (b_{n-1} \cdots b_0)_2 + 1
            = 2^n.
    \end{equation*}
    %
    By induction we have
    %
    \begin{align*}
        \gray[n]{2^n-1-r}
            &= \conj{b}_{n-1} (\conj{b}_{n-1} \xor \conj{b}_{n-2}) \cdots (\conj{b}_1 \xor \conj{b}_0) \\
            &= (b_n \xor b_{n-1}) (b_{n-1} \xor b_{n-2}) \cdots (b_1 \xor b_0)
    \end{align*}
    %
    since $b_n = 1$, so it follows that
    %
    \begin{align*}
        \gray[n+1]{k}
            &= b_n \conc \gray[n]{2^n - 1 - r} \\
            &= b_n (b_n \xor b_{n-1}) (b_{n-1} \xor b_{n-2}) \cdots (b_1 \xor b_0) 
    \end{align*}
    %
    as desired.

    For the final claim, simply notice that
    %
    \begin{align*}
        a_i \xor \cdots \xor a_{n-1}
            &= (b_i \xor b_{i+1}) \xor (b_{i+1} \xor b_{i+2}) \xor \cdots \xor (b_{n-2} \xor b_{n-1}) \xor b_{n-1} \\
            &= b_i \xor (b_{i+1} \xor b_{i+1}) \xor (b_{i+2} \xor \cdots \xor b_{n-2}) \xor (b_{n-1} \xor b_{n-1}) \\
            &= b_i.
    \end{align*}
    %
    Alternatively we may notice that \cref{eq:gray-consecutive-bits} defines a linear system of equations with coefficients in $\ints/2\ints$ and invert this.
\end{proof}



\begin{corollary}
    For $n \in \naturals$ and any $n$-bit number $k$, we have
    %
    \begin{equation*}
        \gray[n]{k}
            = \bin[n]{k} \xor \bin[n]{k}\shiftr.
    \end{equation*}
\end{corollary}

\begin{proof}
    Writing $\bin[n]{k} = b_{n-1} \cdots b_0$, the proposition implies that
    %
    \begin{align*}
        \gray[n]{k}
            &= b_{n-1} (b_{n-1} \xor b_{n-2}) \cdots (b_1 \xor b_0) \\
            &= (0 \xor b_{n-1}) (b_{n-1} \xor b_{n-2}) \cdots (b_1 \xor b_0).
    \end{align*}
    %
    But $\bin[n]{k}\shiftr = 0 b_{n-1} \cdots b_1$, so the claim follows.
\end{proof}


\nocite{*}

\printbibliography

\end{document}