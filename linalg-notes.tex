%==============================================================
%  PREAMBLE
%==============================================================

% Document setup
\documentclass[a4paper, 11pt]{memoir}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[danish, UKenglish]{babel}

% Standard preamble
% \usepackage{latex-sty/standard-preamble}

% Document info
\makeatletter
\def\@title{Linear Algebra}
\def\@author{Danny Nyg√•rd Hansen}
\def\@alttitle{Finite- and infinite-dimensional vector spaces}
\title{\@title}
\author{\@author}
\makeatother

% Mathematics
\usepackage{latex-sty/mathcommands-basic}
\usepackage{latex-sty/mathcommands-topology}
\usepackage{latex-sty/theorems-changedot}
\usepackage{latex-sty/theorems-references}
\usepackage{tikz-cd}
\tikzcdset{arrow style=math font} % https://tex.stackexchange.com/questions/300352/equalities-look-broken-with-tikz-cd-and-math-font
\usetikzlibrary{babel}

% https://tex.stackexchange.com/a/124311/63353
\makeatletter
\g@addto@macro\bfseries{\boldmath}
\makeatother


%==============================================================
%  DOCUMENT SPECIFIC COMMANDS
%==============================================================

\DeclareMathOperator{\spec}{Spec}
\DeclareMathOperator{\chr}{char}

\newcommand{\ev}{\mathrm{ev}}
\newcommand{\gen}[1]{\langle#1\rangle}
\DeclarePairedDelimiterX{\genset}[2]{\langle}{\rangle}{#1\;\delimsize\vert\;#2}

% Table of contents
\makeatletter

\renewcommand*{\cftchapterleader}{}
\renewcommand*{\cftsectionleader}{}
\renewcommand{\cftchapterpagefont}{}
\renewcommand*{\cftchapterformatpnum}[1]{\itshape~~{\footnotesize\textbullet}~~#1}
\renewcommand*{\cftsectionformatpnum}[1]{\itshape~~{\footnotesize\textbullet}~~#1}
\renewcommand{\cftchapterafterpnum}{\cftparfillskip}
\renewcommand{\cftsectionafterpnum}{\cftparfillskip}

% I'm trying to redefine chapter/section numberings to get a small caps minuscule letter for appendices.
% Is it smart to redefine this stuff???
% And is that really what I want? It looks fine in headers!
\renewcommand*{\numberline}[1]{%
  \numberlinehook{#1}%
  \hb@xt@\@tempdima{\@cftn@me\@cftbsnum {#1}\@cftasnum\hfil}\@cftasnumb}
\renewcommand{\chapternumberline}[1]{%
  \chapternumberlinehook{#1}%
  \hb@xt@\@tempdima{\@chapapp@head\@cftbsnum {#1}\@cftasnum\hfil}%
  \@cftasnumb}

\renewcommand{\cftchapterpagefont}{\normalfont\large} % Chapter page numbers
\renewcommand{\cftchapterfont}{\normalfont\large} % Chapter title and number
\setlength{\cftbeforesectionskip}{3pt}
\setrmarg{3.55em plus 1fil}

\makeatother


\newenvironment{displaytheorem}{%
	\begin{displayquote}\itshape%
}{%
	\end{displayquote}%
}

\makeatletter
\newtheoremstyle{changedotcustomnumber}%
    {}%
    {\item[\hskip\labelsep \theorem@headerfont ##3~~\theorembullet~~##1\theorem@separator]}
\newtheoremstyle{changedotbreakcustomnumber}%
    {}%
    {\item[\rlap{\vbox{\hbox{\hskip\labelsep \theorem@headerfont
            ##3~~\theorembullet~~##1\theorem@separator}\hbox{\strut}}}]}

\newtheoremstyle{plaincustomnumber}%
    {\item[\hskip\labelsep \theorem@headerfont ##1\ ##2\theorem@separator]}%
    {\item[\hskip\labelsep \theorem@headerfont ##1~##3\theorem@separator]}
\makeatother

\usepackage{bbding}

\theoremseparator{.}
\theoremsymbol{}
\theoremstyle{plaincustomnumber}
\newtheorem{problem}{Problem}
\theoremstyle{changedotbreakcustomnumber}
\newtheorem{problembreak}{\raisebox{-2pt}{\PencilLeft}}



%==============================================================
%  TO BE DELETED
%==============================================================

\DeclareMathOperator{\sign}{sgn}
\DeclareMathOperator{\adj}{adj}
\DeclareMathOperator{\trace}{tr}
\DeclareMathOperator{\Span}{span}

\newcommand{\calM}{\mathcal{M}}
\newcommand{\calN}{\mathcal{N}}
\newcommand{\calT}{\mathcal{T}}
\newcommand{\calL}{\mathcal{L}}
\newcommand{\calV}{\mathcal{V}}
\newcommand{\calW}{\mathcal{W}}
\newcommand{\calU}{\mathcal{U}}
\newcommand{\calE}{\mathcal{E}}
\newcommand{\calF}{\mathcal{F}}
\newcommand{\calO}{\mathcal{O}}
\newcommand{\calH}{\mathcal{H}}
\newcommand{\calK}{\mathcal{K}}
\newcommand{\calI}{\mathcal{I}}
\newcommand{\calJ}{\mathcal{J}}
\newcommand{\calS}{\mathcal{S}}
\newcommand{\calA}{\mathcal{A}}
\newcommand{\calC}{\mathcal{C}}
\newcommand{\calB}{\mathcal{B}}
\newcommand{\calD}{\mathcal{D}}

\usepackage{listofitems}
\setsepchar{,}

\makeatletter
\newcommand{\mat@dims}[1]{%
    \readlist*\@dims{#1}%
    \ifnum \@dimslen=1
        \def\@dimsout{\@dims[1]}%
    \else
        \def\@dimsout{\@dims[1], \@dims[2]}%
    \fi
    \@dimsout
}


\newcommand{\matgroup}[3]{\mathrm{#1}_{#2}(#3)}
\newcommand{\matGL}[2]{\matgroup{GL}{#1}{#2}}
\newcommand{\trans}{^{\top}}
% \newcommand{\mat}[2]{\calM_{\mat@dims{#1}}(#2)}
\newcommand{\mat}[2]{\mathrm{M}_{\mat@dims{#1}}(#2)}
\newcommand{\matO}[1]{\mathrm{O}(#1)}
\newcommand{\matSO}[1]{\mathrm{SO}(#1)}
\newcommand{\field}{\mathbb{F}}
\newcommand{\fieldK}{\mathbb{K}}
\newcommand{\cof}{\operatorname{cof}}
\newcommand{\im}{\operatorname{im}}

\newcommand{\vecU}[1]{\mathrm{U}(#1)}
\newcommand{\vecO}[1]{\mathrm{O}(#1)}
\newcommand{\vecSp}[1]{\mathrm{Sp}(#1)}
\makeatother

% https://tex.stackexchange.com/questions/55863/how-to-detect-if-in-an-exercise-environment
\usepackage{etoolbox}
\newbool{indefinition}
\AtBeginEnvironment{definition}{\booltrue{indefinition}}

% \newcommand{\keyword}[1]{{\itshape\bfseries #1}}
\newcommand{\keyword}[1]{\ifbool{indefinition}{{\itshape #1}}{{\itshape\bfseries #1}}}



%==============================================================
%  END PREAMBLE
%==============================================================

\begin{document}

\frontmatter

\maketitle

\tableofcontents
\newpage

\chapter{Preface}

These notes cover aspects of linear algebra that I have not found satisfactory expositions of elsewhere. We generally restrict ourselves to the finite-dimensional case, unless results can be generalised without significant effort. For instance, in the context of inner product spaces there is of course no loss in generality by restricting to the real or the complex numbers, and the elementary theory of Hilbert space adjoints is not simplified substantially by the assumption of finite dimension, so we make no such assumption. On the other hand, we only prove the spectral theorem for normal operators on finite-dimensional spaces.

Throughout we let $\field$ denote an arbitrary field, $\fieldK$ a field that is either the real or the complex numbers, and $R$ a commutative ring. Unless otherwise specified, vector spaces will be vector spaces over $\field$, and modules will be left modules over $R$. Furthermore, sesquilinear forms are linear in their \emph{second} entry. This is rarely relevant, but it seems more natural both in the theory of duality of spaces equipped with sesquilinear forms (see \cref{par:Riesz-map}) and in the representation of sesquilinear forms with matrices (see \cref{par:sesquilinear-form-matrix-representation}).

\mainmatter

\chapter{Linear equations and matrices}

\section{Linear equations}

Let $m$ and $n$ be positive integers. A \keyword{linear equation in $n$ unknowns} is an equation on the form
%
\begin{equation*}
    l \colon a_1 x_1 + \cdots + a_n x_n = b,
\end{equation*}
%
where $a_1, \ldots, a_n, b \in \field$. A \keyword{solution} to $l$ is an element $v = (v_1, \ldots, v_n) \in \field^n$ such that
%
\begin{equation*}
    a_1 v_1 + \cdots + a_n v_n = b.
\end{equation*}
%
A \keyword{system of linear equations in $n$ unknowns} is a tuple $L = (l_1, \ldots, l_m)$, where each $l_i$ is a linear equation in $n$ unknowns. An element $v \in \field^n$ is a \keyword{solution} to $L$ if it is a solution to each linear equation $l_1, \ldots, l_m$.

Let $L$ and $L'$ be systems of linear equations in $n$ unknowns. We say that $L$ and $L'$ are \keyword{solution equivalent} if they have the same solutions. Furthermore, we say that they are \keyword{combination equivalent} if each equation in $L'$ is a linear combination of the equations in $L$, and vice versa. Clearly, if $L$ and $L'$ are combination equivalent they are also solution equivalent, but the converse does not hold.


\section{Matrices}



\newpar

For $m,n \in \naturals$ we denote by $\mat{m,n}{R}$ the set of $m \times n$ matrices over $R$. In the case where $R = \field$, it is well-known that a system of linear equations is equivalent to a matrix equation on the form $Ax = b$, where $A \in \mat{m,n}{\field}$, $x \in \field^n$ and $b \in \field^m$. Recall the \keyword{elementary row operations} on $A$:
%
\begin{enumerate}
    \item multiplication of one row of $A$ by a nonzero scalar,
    \item addition to one row of $A$ a scalar multiple of another (different) row, and
    \item interchange of two rows of $A$.
\end{enumerate}
%
If $e$ is an elementary row operation, we write $e(A)$ for the matrix obtained when applying $e$ to $A$. Clearly each elementary row operation $e$ has an \enquote{inverse}, i.e. an elementary row operation $e'$ such that $e'(e(A)) = e(e'(A)) = A$. Two matrices $A,B \in \mat{m,n}{\field}$ are called \keyword{row-equivalent} if $A$ is obtained by applying a finite sequence of elementary row operations to $B$ (and vice versa, though this need not be assumed since each elementary row operation has an inverse).

Clearly, if $A, B \in \mat{m,n}{\field}$ are row-equivalent, then the systems of equations $Ax = 0$ and $Bx = 0$ are combination equivalent, hence have the same solutions.

An \keyword{elementary matrix} is a matrix obtained by applying a single elementary row operation to the identity matrix $I$. It is easy to show that if $e$ is an elementary row operation and $E = e(I) \in \mat{m}{\field}$, then $e(A) = EA$ for $A \in \mat{m,n}{\field}$. If also $B \in \mat{m,n}{\field}$, then $A$ and $B$ are row-equivalent if and only if $A = PB$, where $P \in \mat{m}{\field}$ is a product of elementary matrices.


\newpar

We now show that every matrix is row-equivalent to a matrix with a particularly simple form:

\begin{definition}
    A matrix $H \in \mat{m,n}{\field}$ is called \keyword{row-reduced} if
    %
    \begin{enumdef}
        \item the first nonzero entry of each nonzero row in $H$ is $1$, and
        \item each column of $H$ containing the leading nonzero entry of some row has all its other entries equal $0$.
    \end{enumdef}
    %
    If $H$ is row-reduced, it is called a \keyword{row-reduced echelon matrix} if it also has the following properties:
    %
    \begin{enumdef}[resume]
        \item Every row of $H$ only containing zeroes occur below every row which has a nonzero entry, and
        \item if rows $1, \ldots, r$ are the nonzero rows of $H$, and if the leading nonzero entry of row $i$ occurs in column $k_i$, then $k_1 < \cdots < k_r$.
    \end{enumdef}
\end{definition}


\begin{proposition}
    Every matrix in $\mat{m,n}{\field}$ is row-equivalent to a unique row-reduced echelon matrix.
\end{proposition}

\begin{proof}
    The usual Gauss--Jordan elimination algorithm proves existence. If $H, K \in \mat{m,n}{R}$ are row-equivalent row-reduced echelon matrices, we claim that $H = K$. We prove this by induction in $n$. If $n = 1$ then this is obvious, so assume that $n > 1$. Let $H_1$ and $K_1$ be the matrices obtained by deleting the $n$th column in $H$ and $K$ respectively. Then $H_1$ and $K_1$ are also row-equivalent\footnote{It should be obvious that deleting columns preserves row-equivalence, but we give a more precise argument: If $P \in \mat{m}{\field}$ is a product of elementary matrices and $a_1, \ldots, a_n \in \field^m$ are the columns in $A$, then the columns in $PA$ are $Pa_1, \ldots, Pa_m$. Thus elementary row operations are applied to each column independently of the other columns.} and row-reduced echelon matrices, so by induction $H_1 = K_1$. Thus if $H$ and $K$ differ, they must differ in the $n$th column.

    Let $H_2$ be the matrix obtained by deleting columns in $H$, only keeping those columns containing pivots, as well as keeping the $n$th column. Define $K_2$ similarly. Thus we have deleted the same columns in $H$ and $K$, so $H_2$ and $K_2$ are also row-equivalent. Say that the number of columns in $H_2$ and $K_2$ is $r+1$, and write the matrices on the form
    %
    \begin{equation*}
        H_2
            = \begin{pmatrix}
                I_r & h \\
                0   & h'
            \end{pmatrix}
        \quad \text{and} \quad
        K_2
            = \begin{pmatrix}
                I_r & k \\
                0   & k'
            \end{pmatrix},
    \end{equation*}
    %
    where $h,k \in \field^r$ and $h',k' \in \field^{m-r}$ are column vectors. Since $H_2$ and $K_2$ are row-equivalent, the systems $H_2 x = 0$ and $K_2 x = 0$ are solution equivalent. If $h' = 0$, then $H_2 x = 0$ has the solution $(-h,1)$. But this is also a solution to $K_2 x = 0$, so $h = k$ and $k' = 0$. If $h' \neq 0$, then $H_2 x = 0$ only has the trivial solution. But then $K_2 x = 0$ also only has the trivial solution, and hence $k' \neq 0$. But that must be because both $H_2$ and $K_2$ has a pivot in the rightmost column, so also in this case $H_2 = K_2$.
\end{proof}


\newpar

Next we study when square matrices are invertible. The main result says for a square matrix to be invertible, it suffices that it has either a left- or a right-inverse. Since (as we will see in \cref{par:matrix-rep}) square matrices are precisely the linear endomorphisms on finite-dimensional vector spaces, this shows that for such an endomorphism to be invertible, it suffices that it has either a left- or a right-inverse. This result is also a direct consequence of the rank--nullity theorem, see e.g. \textcite[Corollary~2.9]{romanlinalg}.

Notice that elementary matrices are invertible since elementary row operations are invertible.

\begin{lemma}
    If $A \in \mat{n}{\field}$, then the following are equivalent:
    %
    \begin{enumlem}
        \item \label{enum:lemma-A-invertible} $A$ is invertible,
        \item \label{enum:lemma-A-equivalent-to-I} $A$ is row-equivalent to $I_n$,
        \item \label{enum:lemma-A-elementary-matrix-product} $A$ is a product of elementary matrices, and
        \item \label{enum:lemma-only-trivial-solution} the system $Ax = 0$ has only the trivial solution $x = 0$.
    \end{enumlem}
\end{lemma}

\begin{proof}
\begin{proofsec*}
    \item[\Namesubcref{enum:lemma-A-invertible} $\Leftrightarrow$ \namesubcref{enum:lemma-A-equivalent-to-I}]
    Let $H \in \mat{n}{\field}$ be a row-reduced echelon matrix that is row-equivalent to $A$. Then $H = PA$, where $P \in \mat{n}{\field}$ is a product of elementary matrices. Then $A = P\inv H$, so $A$ is invertible if and only if $H$ is. But the only invertible row-reduced echelon matrix is the identity matrix, so \subcref{enum:lemma-A-invertible} and \subcref{enum:lemma-A-equivalent-to-I} are equivalent.
    
    \item[\Namesubcref{enum:lemma-A-equivalent-to-I} $\implies$ \namesubcref{enum:lemma-A-elementary-matrix-product}]
    As above, there exists a product $P$ of elementary matrices such that $I_n = PA$, so $A = P\inv$.

    \item[\Namesubcref{enum:lemma-A-elementary-matrix-product} $\implies$ \namesubcref{enum:lemma-A-invertible}]
    This is obvious since elementary matrices are invertible.

    \item[\Namesubcref{enum:lemma-A-equivalent-to-I} $\Leftrightarrow$ \namesubcref{enum:lemma-only-trivial-solution}]
    If $A$ and $I_n$ are row-equivalent, then the systems $Ax = 0$ and $I_n x = 0$ have the same solutions. Conversely, assume that $Ax = 0$ only has the trivial solution. If $H \in \mat{m,n}{\field}$ is a row-reduced echelon matrix that is row-equivalent to $A$, then $Hx = 0$ has no nontrivial solution. Thus if $r$ is the number of nonzero rows in $H$, then $r \geq n$. But then $r = n$, so $H$ must be the identity matrix.
\end{proofsec*}
\end{proof}


\begin{proposition}
    Let $A \in \mat{n}{\field}$. Then the following are equivalent:
    %
    \begin{enumprop}
        \item $A$ is invertible,
        \item $A$ has a left inverse, and
        \item $A$ has a right inverse.
    \end{enumprop}
\end{proposition}

\begin{proof}
    If $A$ has a left inverse, then $Ax = 0$ has no nontrivial solution, so $A$ is invertible. If $A$ has a right inverse $B \in \mat{n}{\field}$, i.e. $AB = I$, then $B$ has a left inverse and is thus invertible. But then $A$ is the inverse of $B$ and hence is itself invertible.
\end{proof}




\chapter{Bases and coordinates}

\section{Bases}

\newpar

If $\calV$ is a subset of $V$, the \keyword{span} of $\calV$, denoted $\Span\calV$ or $\gen{\calV}$, is the smallest subspace of $V$ containing $\calV$. Equivalently, it is the set of all linear combinations
%
\begin{equation*}
    \alpha_1 v_1 + \cdots + \alpha_n v_n,
\end{equation*}
%
where $\alpha_i \in \field$ and $v_i \in \calV$. We say that $\calV$ is \keyword{linearly independent} if any linear relation
%
\begin{equation*}
    \alpha_1 v_1 + \cdots + \alpha_n v_n = 0
\end{equation*}
%
among elements $v_i$ in $\calV$ can only be satisfied if $\alpha_1 = \cdots = \alpha_n = 0$. If $\calV$ is both linearly independent and a spanning set, then we call it a \keyword{basis} for $V$. We have the folloing characterisation of bases:

\begin{propositionnoproof}
    A subset $\calV \subseteq V$ is a basis for $V$ if and only if
    %
    \begin{equation*}
        V
            = \bigoplus_{v \in \calV} \gen{v}.
    \end{equation*}
\end{propositionnoproof}

We next prove that bases always exist, but first we need a different characterisation of bases. An element $v \in V$ is an \keyword{essentially unique} linear combination of the elements in $\calV$ if there is an up to ordering unique way to express $v$ as a linear combination of elements in $\calV$. It is easy to see that $\calV$ is linearly independent if and only if every nonzero $v \in \gen{\calV}$ is an essentially unique linear combination of the elements in $\calV$.

\begin{proposition}
    Let $\calV$ be a subset of $V$. The following are equivalent:
    %
    \begin{enumprop}
        \item \label{enum:linearly-independent-and-span} $\calV$ is linearly independent and spans $V$.
        \item \label{enum:essentially-unique-linear-combination} Every nonzero $v \in V$ is an essentially unique linear combination of vectors in $\calV$.
        \item \label{enum:minimal-spanning-set} $\calV$ is a minimal spanning set.
        \item \label{enum:maximal-linearly-independent-set} $\calV$ is a maximal linearly independent set.
    \end{enumprop}
\end{proposition}

\begin{proof}
\begin{proofsec*}
    \item[\subcref{enum:linearly-independent-and-span} $\iff$ \subcref{enum:essentially-unique-linear-combination}]
    This follows easily as mentioned above.

    \item[\subcref{enum:linearly-independent-and-span} $\iff$ \subcref{enum:minimal-spanning-set}]
    If \subcref{enum:linearly-independent-and-span} holds and a proper subset $\calV'$ of $\calV$ spanned $V$, then any element of $\calV \setminus \calV'$ is a linear combination of elements in $\calV'$, so $\calV$ is not linearly independent. Conversely, if $\calV$ is a minimal spanning set but is not linearly independent, then some $v \in \calV$ is a linear combination of the other elements in $\calV$, so $\calV \setminus \{v\}$ is also a spanning set.

    \item[\subcref{enum:linearly-independent-and-span} $\iff$ \subcref{enum:maximal-linearly-independent-set}]
    Again assuming \subcref{enum:linearly-independent-and-span}, if $\calV$ were not maximal there would be some $v \in V \setminus \calV$ such that $\calV \union \{v\}$ were linearly independent. But then $v$ would not be a linear combination of elements in $\calV$. Conversely, if $\calV$ is a maximal linearly independent set that did not span $V$, then there would be some $v \in V \setminus \calV$ that is not a linear combination of elements in $\calV$. But then $\calV \union \{v\}$ is also linearly independent.
\end{proofsec*}
\end{proof}


\begin{theorem}
    \label{prop:basis-existence}
    Let $V$ be a vector space. If $\calI \subseteq V$ is linearly independent, $\calS \subseteq V$ is a spanning set, and $\calI \subseteq \calS$, then there is a basis $\calV$ for $V$ with $\calI \subseteq \calV \subseteq \calS$.
\end{theorem}

\begin{proof}
    Let $\calA$ be the collection of linearly independent subsets $\calJ$ of $V$ with $\calI \subseteq \calJ \subseteq \calS$. If $\calC$ is a chain in $\calA$, then
    %
    \begin{equation*}
        \calU
            = \bigunion_{\calJ \in \calC} \calJ
    \end{equation*}
    %
    is linearly independent and satisfies $\calI \subseteq \calU \subseteq \calS$, so it lies in $\calA$. Hence every chain in $\calA$ has an upper bound, so it has a maximal element $\calV$. This is linearly independent since it lies in $\calA$, and it is also a spanning set by maximality, hence it is a basis.
\end{proof}


\begin{corollary}
    Every vector space has a basis.
\end{corollary}

\begin{proof}
    Let $\calI = \emptyset$ and $\calS = V$ in \cref{prop:basis-existence}.
\end{proof}


\newpar

We next turn to the concept of the \keyword{dimension} of a vector space. Our presentation will focus on finite-dimensional vector spaces.

\begin{proposition}
    If the vectors $v_1, \ldots, v_n$ in $V$ are linearly independent, and the vectors $w_1, \ldots, w_m$ span $V$, then $n \leq m$.
\end{proposition}

\begin{proof}
    List the vectors as follows:
    %
    \begin{equation*}
        w_1, \ldots, w_m; v_1, \ldots, v_n.
    \end{equation*}
    %
    We transform this list such that the collection of vectors on the left-hand side of the semicolon always span $V$, and such that the vectors on the right-hand side are always linearly independent. Note that $v_1$ is a linear combination of the $w_j$, implying that we may add $v_1$ to the left-hand side and remove one of the $w_j$ (which, by reindexing, we may assume is $w_1$) and still have a spanning set. We simultaneously remove $v_1$ from the right-hand side. That is, we obtain
    %
    \begin{equation*}
        v_1, w_2, \ldots, w_m; v_2, \ldots, v_n.
    \end{equation*}
    %
    If $m < n$, then applying this process recursively will eventually exhaust the $w_j$, at which point we would have
    %
    \begin{equation*}
        v_1, \ldots, v_m; v_{m+1}, \ldots, v_n.
    \end{equation*}
    %
    But this is not possible, since $v_n$ does not lie in the span of $v_1, \ldots, v_m$. Hence $n \leq m$.
\end{proof}


\begin{corollarynoproof}
    If $V$ has a finite spanning set, then all bases for $V$ have the same cardinality.
\end{corollarynoproof}
%
This in fact holds for arbitrary vector spaces, though the proof is significantly more involved (cf. \cite[Theorem~1.12]{romanlinalg}).

Since bases always exist and all bases have the same cardinality, the following definition makes sense:

\begin{definition}[Dimension]
    The \keyword{dimension} of a vector space $V$, written $\dim V$, is the cardinality of any basis for $V$.
\end{definition}


\newpar

We now turn to a different characterisation of the dimension of finite-dimensional vector spaces. Below we write $\dim V = \infty$ if the dimension of the vector space $V$ is infinite. A \keyword{series} of subspaces $U_i$ of $V$ is a finite or infinite decreasing sequence
%
\begin{equation*}
    V
        = U_0
        \supsetneq U_1
        \supsetneq U_2
        \supsetneq \cdots.
\end{equation*}
%
If the sequence is finite, then the \keyword{length} of the series is the number of strict inclusions. If the sequence is infinite, then we say that the length of the series is $\infty$. The maximal length of a series of subspaces of $V$ is denoted $l(V)$.

In the proposition below, we write $\dim V = \infty$ if the dimension of $V$ is infinite.

\begin{proposition}
    Let $V$ be a vector space. Then $\dim V = l(V)$.
\end{proposition}

\begin{proof}
    First assume that $V$ is finite-dimensional, and let $\calV = (v_1, \ldots, v_n)$ be a basis for $V$. Then there is a series
    %
    \begin{equation*}
        V
            = \gen{v_1, \ldots, v_n}
            \supsetneq \gen{v_1, \ldots, v_{n-1}}
            \supsetneq \cdots
            \supsetneq \gen{v_1}
            \supsetneq 0
    \end{equation*}
    %
    of subspaces of $V$, so $\dim V \leq l(V)$. Conversely, let
    %
    \begin{equation*}
        V
            = U_0
            \supsetneq U_1
            \supsetneq U_2
            \supsetneq \cdots
    \end{equation*}
    %
    be a series of subspaces of $V$. If the series ends with $0$, remove it. Hence all subspaces in the series are nontrivial. Then choose for each $i$ an element $v_i \in U_i \setminus U_{i+1}$, and collect them in a set $\calI$. It is clear that $\calI$ is linearly independent, hence finite. Thus the series is also finite with length $\card{\calI}-1$. Adding back $0$ to the series we obtain a series that is at least as long as the original sequence, and that is of length $\card{\calI} \leq \dim V$. Since the sequence was arbitrary, $l(V) \leq \dim V$.

    Next assume that $V$ is infinite-dimensional. Then $V$ contains a sequence $(v_i)_{i\in\naturals}$ that is linearly independent, so the series
    %
    \begin{equation*}
        V
            \supseteq \genset{v_i}{i \in \naturals}
            \supsetneq \genset{v_i}{i \geq 2}
            \supsetneq \genset{v_i}{i \geq 3}
            \supsetneq \cdots
    \end{equation*}
    %
    is infinite, and $l(V) = \infty$. Conversely, assume that $V$ has an infinite series. As above we construct a linearly independent set $\calI$ whose size equals the length of the sequence. Thus $V$ contains an infinite linearly independent set, so $\dim V = \infty$.
\end{proof}




\section{Coordinate maps and matrices}

\newcommand{\coordmap}[1]{\phi_{#1}}
\newcommand{\coordvec}[2]{[#1]_{#2}}
\newcommand{\basischange}[2]{\phi_{#1,#2}}
\newcommand{\mr}[3]{{}_{#1}[#2]_{#3}}
\newcommand{\basischangemat}[2]{\mr{#1}{\square}{#2}}
\newcommand{\lin}{\calL}
\newcommand{\smr}[1]{\calM(#1)} % standard matrix representation

\newcommand{\colvec}[1]{\begin{pmatrix}#1\end{pmatrix}}




\newpar

Every matrix $A \in \mat{m,n}{\field}$ gives rise to a map $M_A \colon \field^n \to \field^m$ given by $M_A v = Av$. The next result shows that every linear map $\field^n \to \field^m$ arises in this way:

\begin{proposition}
    \label{prop:smr-properties}
    Let $(e_1, \ldots, e_n)$ be the standard basis for $\field^n$. The map
    %
    \begin{align*}
        \calM \colon \lin(\field^n, \field^m) &\to \mat{m,n}{\field}, \\
        T &\mapsto \bigl( Te_1 \mid \cdots \mid Te_n \bigr),
    \end{align*}
    %
    is a linear isomorphism with inverse $A \mapsto M_A$. The matrix $\smr{T}$ is called the \keyword{standard matrix representation} of $T$. If $T \colon \field^n \to \field^m$ and $S \colon \field^m \to \field^l$ are linear maps, then
    %
    \begin{enumprop}
        \item \label{enum:smr-vector-multiplication} $Tv = \smr{T}v$ for all $v \in \field^n$.
        
        \item \label{enum:smr-of-identity-map} $\smr{\id_{\field^n}} = I$.

        \item \label{enum:smr-multiplicative} $\smr{S \circ T} = \smr{S} \smr{T}$.

        \item \label{enum:smr-invertibility} $T$ is invertible if and only if $\smr{T}$ is invertible, in which case $\smr{T\inv} = \smr{T}\inv$.
    \end{enumprop}
\end{proposition}

\begin{proof}
    The map $A \mapsto M_A$ is clearly linear, so to prove the first point it suffices to show that this is the inverse of $\calM$. Let $T \in \lin(\field^n,\field^m)$. Then
    %
    \begin{equation*}
        M_{\smr{T}} \colvec{\alpha_1 \\ \vdots \\ \alpha_n}
            = \smr{T} \colvec{\alpha_1 \\ \vdots \\ \alpha_n}
            = \bigl( Te_1 \mid \cdots \mid Te_n \bigr) \colvec{\alpha_1 \\ \vdots \\ \alpha_n}
            = \sum_{i=1}^n \alpha_i Te_i
            = T \colvec{\alpha_1 \\ \vdots \\ \alpha_n}
    \end{equation*}
    %
    for $\alpha_1, \ldots, \alpha_n \in \field$. Conversely, for $A \in \mat{m,n}{\field}$ we have
    %
    \begin{equation*}
        \smr{M_A}
            = \bigl( M_A e_1 \mid \cdots \mid M_A e_n \bigr)
            = \bigl( A e_1 \mid \cdots \mid A e_n \bigr)
            = A,
    \end{equation*}
    %
    since $Ae_i$ is the $i$th column of $A$. We prove the remaining claims:
    %
    \begin{proofsec}
        \item[\Namesubcref{enum:smr-vector-multiplication}]
        Simply notice that $Tv = M_{\smr{T}}v = \smr{T}v$.

        \item[\Namesubcref{enum:smr-of-identity-map}]
        This is obvious from the definition of $\calM$.

        \item[\Namesubcref{enum:smr-multiplicative}]
        Let $v \in \field^n$ and notice that
        %
        \begin{equation*}
            \smr{S \circ T}v
                = (S \circ T) v
                = S(Tv)
                = S(\smr{T}v)
                = \smr{S}\smr{T}v
        \end{equation*}
        %
        by \subcref{enum:smr-vector-multiplication}. Since this holds for all $v$, the claim follows.

        \item[\Namesubcref{enum:smr-invertibility}]
        This follows easily from \subcref{enum:smr-of-identity-map} and \subcref{enum:smr-multiplicative}.
    \end{proofsec}
\end{proof}


\newpar

Having characterised all linear maps between powers of the field $\field$, we now wish to characterise the linear maps between abstract finite-dimensional vector spaces. The first order of business is to establish a correspondence between such a vector space and an appropriate power of $\field$.

Let $V$ be a finite-dimensional $\field$-vector space. If $\calV = (v_1, \ldots, v_n)$ is an ordered basis for $V$, then for every $v \in V$ there are unique $\alpha_1, \ldots, \alpha_n \in \field$ such that $v = \sum_{i=1}^n \alpha_i v_i$. Hence the map $\coordmap{\calV} \colon V \to \field^n$ given by $\coordmap{\calV}(v) = (\alpha_1, \ldots, \alpha_n)$ is well-defined. Furthermore, it is clearly linear, and since $\calV$ is a basis it is also bijective, hence a linear isomorphism. The map $\coordmap{\calV}$ is called the \keyword{coordinate map} with respect to $\calV$, and the vector $\coordvec{v}{\calV} = \coordmap{\calV}(v)$ is called the \keyword{coordinate vector} of $v$ with respect to $\calV$.

Now let $\calW$ be another ordered basis for $V$. The composition $\basischange{\calW}{\calV} = \coordmap{\calW} \circ \coordmap{\calV}\inv$ is called the \keyword{change of basis operator} from $\calV$ to $\calW$, and this makes the diagram
%
\begin{equation}
    \label{eq:change-of-basis-diagram}
    \begin{tikzcd}[row sep=small]
        & \field^n
            \ar[dd, "\basischange{\calW}{\calV}"] \\
        V
            \ar[ru, "\coordmap{\calV}"]
            \ar[rd, "\coordmap{\calW}", swap] \\
        & \field^n
    \end{tikzcd}
\end{equation}
%
commute. Its standard matrix is denoted $\basischangemat{\calW}{\calV}$. This has the expected properties:

\begin{proposition}
    Let $\calV$, $\calW$ and $\calU$ be ordered bases for a finite-dimensional $\field$-vector space $V$. Then
    %
    \begin{enumprop}
        \item \label{enum:basis-change-coordvec} $\coordvec{v}{\calW} = \basischange{\calW}{\calV} (\coordvec{v}{\calV})$ for all $v \in V$. In particular, $\coordvec{v}{\calW} = \basischangemat{\calW}{\calV} \cdot \coordvec{v}{\calV}$.

        \item \label{enum:basis-change-identity-map} $\basischange{\calV}{\calV}$ is the identity map. In particular, $\basischangemat{\calV}{\calV}$ is the identity matrix.

        \item $\basischange{\calU}{\calW} \circ \basischange{\calW}{\calV} = \basischange{\calU}{\calV}$. In particular, $\basischangemat{\calU}{\calW} \cdot \basischangemat{\calW}{\calV} = \basischangemat{\calU}{\calV}$.

        \item $\basischange{\calW}{\calV}$ (resp. $\basischangemat{\calW}{\calV}$) is invertible with inverse $\basischange{\calV}{\calW}$ (resp. $\basischangemat{\calV}{\calW}$).
    \end{enumprop}
\end{proposition}

\begin{proof}
    All claims about change of basis matrices follow by \cref{prop:smr-properties} from the corresponding claims about change of basis operators.

    The claim \subcref{enum:basis-change-coordvec} follows by commutativity of the diagram \cref{eq:change-of-basis-diagram}, i.e.
    %
    \begin{equation*}
        \basischange{\calW}{\calV} (\coordvec{v}{\calV})
            = (\coordmap{\calW} \circ \coordmap{\calV}\inv) \circ \coordmap{\calV}(v)
            = \coordmap{\calW}(v)
            = \coordvec{v}{\calW}.
    \end{equation*}
    %
    Claim \subcref{enum:basis-change-identity-map} is an immediate consequence of the definition of $\basischange{\calV}{\calV}$. The remaining claims are proved similarly to \subcref{enum:basis-change-coordvec}.
\end{proof}


\newpar\label{par:matrix-rep}

Next consider a linear map $T \colon V \to W$. If $\calV \in V^n$ and $\calW \in W^m$ are bases for $V$ and $W$ respectively, then the diagram
%
\begin{equation*}
    \begin{tikzcd}
        V
            \ar[d, "T", swap]
            \ar[r, "\coordmap{\calV}"]
        & \field^n
            \ar[d, "\coordmap{\calW} \circ T \circ \coordmap{\calV}\inv"] \\
        W
            \ar[r, "\coordmap{\calW}", swap]
        & \field^m
    \end{tikzcd}
\end{equation*}
%
commutes. The map $\coordmap{\calW} \circ T \circ \coordmap{\calV}\inv$ is the \keyword{basis representation} of $T$ with respect to the bases $\calV$ and $\calW$. This is a linear map $\field^n \to \field^m$, so it has a standard matrix which we denote $\mr{\calW}{T}{\calV}$. This is called the \keyword{matrix representation} of $T$ with respect to the bases $\calV$ and $\calW$.

\begin{proposition}
    \label{prop:mr-properties}
    Let $V$ and $W$ be finite-dimensional $\field$-vector spaces with ordered bases $\calV = (v_1, \ldots, v_n) \in V^n$ and $\calW \in W^m$, respectively. The map
    %
    \begin{align*}
        \mr{\calW}{\,\cdot\,}{\calV} \colon \lin(V,W) &\to \mat{m,n}{\field}, \\
        T &\mapsto \mr{\calW}{T}{\calV},
    \end{align*}
    %
    is a linear isomorphism. Let $T \colon V \to W$ and $S \colon W \to U$ be linear maps, and let $\calU \in U^l$ be an ordered basis for $U$. Then
    %
    \begin{enumprop}
        \item \label{enum:mr-explicit-formula} $\mr{\calW}{T}{\calV} = \bigl( \coordvec{Tv_1}{\calW} \mid \cdots \mid \coordvec{Tv_n}{\calW} \bigr)$.

        \item \label{enum:mr-vector-multiplication} $\coordvec{Tv}{\calW} = \mr{\calW}{T}{\calV} \cdot \coordvec{v}{\calV}$ for all $v \in V$.

        \item \label{enum:mr-of-identity-map} If $\calV'$ is another basis for $V$, then $\mr{\calV'}{\id_V}{\calV} = \basischangemat{\calV'}{\calV}$.

        \item \label{enum:mr-multiplicative} $\mr{\calU}{S \circ T}{\calV} = \mr{\calU}{S}{\calW} \cdot \mr{\calW}{T}{\calV}$.

        \item \label{enum:mr-invertibility} $T$ is invertible if and only if $\mr{\calW}{T}{\calV}$ is invertible, in which case $\mr{\calV}{T\inv}{\calW} = \mr{\calW}{T}{\calV}\inv$.
    \end{enumprop}
\end{proposition}

\begin{proof}
    For the first claim, notice that the map $T \mapsto \coordmap{\calW} \circ T \circ \coordmap{\calV}\inv$ is a linear isomorphism, since pre- and postcomposition with linear isomorphisms are themselves linear isomorphisms. Composing this map with $\calM$ yields $\mr{\calW}{\,\cdot\,}{\calV}$, so this is a linear isomorphism by \cref{prop:smr-properties}.
    %
    \begin{proofsec}
        \item[\Namesubcref{enum:mr-explicit-formula}]
        If $(e_1, \ldots, e_n)$ is the standard basis for $\field^n$, then the definition of the standard matrix representation yields that the $i$th column of $\mr{\calW}{T}{\calV}$ is given by
        %
        \begin{equation*}
            \mr{\calW}{T}{\calV} \cdot e_i
                = \smr{\coordmap{\calW} \circ T \circ \coordmap{\calV}\inv} \cdot e_i
                = (\coordmap{\calW} \circ T \circ \coordmap{\calV}\inv) e_i
                = \coordmap{\calW}(Tv_i)
                = \coordvec{Tv_i}{\calW},
        \end{equation*}
        %
        as claimed.

        \item[\Namesubcref{enum:mr-vector-multiplication}]
        Notice that
        %
        \begin{align*}
            \coordvec{Tv}{\calW}
                &= (\coordmap{\calW} \circ T)(v) \\
                &= (\coordmap{\calW} \circ T \circ \coordmap{\calV}\inv) \circ \coordmap{\calV}(v) \\
                &= (\coordmap{\calW} \circ T \circ \coordmap{\calV}\inv)(\coordvec{v}{\calV}) \\
                &= \mr{\calW}{T}{\calV} \cdot \coordvec{v}{\calV}.
        \end{align*}
        %
        where the last equality follows from \cref{enum:smr-vector-multiplication}.

        \item[\Namesubcref{enum:mr-of-identity-map}]
        This is obvious from the definitions of $\mr{\calV'}{\id_V}{\calV}$ and $\basischangemat{\calV'}{\calV}$.

        \item[\Namesubcref{enum:mr-multiplicative}]
        Notice that
        %
        \begin{equation*}
            \coordmap{\calU} \circ (S \circ T) \circ \coordmap{\calV}\inv
                = (\coordmap{\calU} \circ S \circ \coordmap{\calW}\inv) \circ (\coordmap{\calW} \circ T \circ \coordmap{\calV}\inv)
        \end{equation*}
        %
        The claim then follows from \cref{enum:smr-multiplicative}.

        \item[\Namesubcref{enum:mr-invertibility}]
        This is an immediate consequence of either \subcref{enum:mr-multiplicative} or of \cref{enum:smr-invertibility}.
    \end{proofsec}
\end{proof}


\begin{proposition}
    Let $\calV = (v_1, \ldots, v_n)$ be an ordered basis for an $\field$-vector space $V$, and let $T \colon V \to V$ be a linear isomorphism. Let $\calW = (w_1, \ldots, w_n)$ where $w_i = Tv_i$. Then $\calW$ is an ordered basis for $V$ and
    %
    \begin{equation*}
        \basischange{\calW}{\calV}
            = \coordmap{\calV} \circ T\inv \circ \coordmap{\calV}\inv,
        \quad \text{or} \quad
        \basischangemat{\calW}{\calV}
            = \mr{\calV}{T\inv}{\calV}.
    \end{equation*}
    %
    In particular, if $V = \field^n$ and $\calV$ is the standard basis $\calE$, then
    %
    \begin{equation*}
        \basischange{\calW}{\calE}
            = T\inv,
        \quad \text{or} \quad
        \basischangemat{\calW}{\calE}
            = \smr{T}\inv.
    \end{equation*}
\end{proposition}
%
We think of this result as follows: If we change basis by applying an invertible linear transformation $T$, we obtain the coordinate vectors corresponding to the transformed basis by applying $T\inv$ (in the old basis). This says that if we perform a \emph{passive transformation}, i.e. a change of basis while keeping vectors themselves fixed, the coordinates change by the inverse of said transformation.

\begin{proof}
    Let $v \in V$ and write $v = \sum_{i=1}^n \alpha_i v_i$. Then
    %
    \begin{equation*}
        Tv
            = \sum_{i=1}^n \alpha_i Tv_i
            = \sum_{i=1}^n \alpha_i w_i
            = \coordmap{\calW}\inv \colvec{\alpha_1 \\ \vdots \\ \alpha_n}
            = \coordmap{\calW}\inv \circ \coordmap{\calV}(v),
    \end{equation*}
    %
    implying that
    %
    \begin{equation*}
        \basischange{\calW}{\calV}
            = \coordmap{\calW} \circ \coordmap{\calV}\inv
            = (T \circ \coordmap{\calV}\inv)\inv \circ \coordmap{\calV}\inv
            = \coordmap{\calV} \circ T\inv \circ \coordmap{\calV}\inv
    \end{equation*}
    %
    as claimed.
\end{proof}


\chapter{Structural properties of vector spaces}\label{testchapter}

\section{Projections I}

\newpar

Let $V$ be a vector space. A linear operator $P \colon V \to V$ is called a \keyword{projection} if it is idempotent, i.e. if $P^2 = P$.

\begin{proposition}
    \label{prop:projection-characterisation}
    A linear map $P \colon V \to V$ is a projection if and only if there exist subspaces $U$ and $W$ of $V$ such that $V = U \oplus W$, $P|_U = \iota_U$ and $P|_W = 0$. In this case $U = \im P$ and $W = \ker P$.
\end{proposition}
%
We say that $P$ is the projection onto $U$ along $W$.

\begin{proof}
    Assume that $P$ is a projection, and let $v \in \im P$. Then $v = Pu$ for some $u \in V$, and
    %
    \begin{equation*}
        Pv
            = P^2 u
            = Pu
            = v.
    \end{equation*}
    %
    If also $v \in \ker P$, then $v = 0$. Furthermore, for any $v \in V$ we have $v = Pv + (v - Pv) \in \im P \oplus \ker P$, so $\im P$ and $\ker P$ are indeed complements in $V$.

    The converse is obvious, and so is the characterisation of $U$ and $W$.
\end{proof}


\newpar

We will return to projections in \cref{sec:projections-2}.


\section{Quotient spaces and complements}

\newpar

If $U$ is a subspace of an $\field$-vector space $V$, then its underlying additive group is a subgroup of the underlying additive group of $V$. Since $V$ considered as such is abelian, we may consider the quotient group $V/U$ whose elements are cosets $v + U$ for $v \in V$. It is then trivial to check that the operation $\alpha(v + U) \defn \alpha v + U$ for $\alpha \in \field$ makes $V/U$ into a vector space. We denote by $\pi_U$ or simply by $\pi$ the quotient map $\pi \colon V \to V/U$ given by $\pi(v) = v + U$.

\begin{theorem}
    Let $U$ be a subspace of $V$. If $T \colon V \to W$ satisfies $U \subseteq \ker T$, then there is a unique linear map $\tilde{T} \colon V/U \to W$ such that the diagram
    %
    \begin{equation*}
        \begin{tikzcd}[row sep=small]
            & W \\
            V
                \ar[ur, "T"]
                \ar[dr, "\pi", swap] \\
            & V/U
                \ar[uu, "\tilde{T}", swap, dashed]
        \end{tikzcd}
    \end{equation*}
    %
    commutes.
\end{theorem}

\begin{proof}
    The corresponding result for groups yields a unique group homomorphism $\tilde{T}$. This is easily seen to also be a linear map. TODO do I also need for topological vector spaces? If not, put in preface that e.g. $\cong$ is only linear isomorphism, not anything topological.
\end{proof}
%
This has the following immediate consequence:

\begin{corollarynoproof}[Canonical decomposition]
    \label{cor:canonical-decomposition}
    Every linear map $T \colon V \to W$ may be decomposed as follows:
    %
    \begin{equation*}
        \begin{tikzcd}
            V
                \ar[r, "\pi", swap, twoheadrightarrow]
                \ar[rrr, bend left, "T"]
            & V/\ker T
                \ar[r, "\sim", "\tilde{T}"']
            & \im T
                \ar[r, "\iota_{\im T}", swap, hookrightarrow]
            & U
        \end{tikzcd}
    \end{equation*}
    %
    In particular we have the \keyword{first isomorphism theorem}: $V/\ker T \cong \im T$.
\end{corollarynoproof}


\newpar

If $U$ is a subspace of $V$, then a subspace $W$ of $V$ with the property that $V = U \oplus W$ is called a \keyword{complement} of $U$. Complements are certainly not unique, but we have the following:

\begin{lemma}
    \label{lem:nested-complements}
    Assume that $V$ has two direct sum compositions
    %
    \begin{equation*}
        U \oplus W_1
            = V
            = U \oplus W_2,
    \end{equation*}
    %
    where $W_1 \subseteq W_2$. Then $W_1 = W_2$.
\end{lemma}

\begin{proof}
    Assume that $v \in W_2$. Then there exist unique $u \in U$ and $w \in W_1$ such that $v = u + w$. But then $w$ also lies in $W_2$, and uniqueness implies that $u = 0$ and $w = v$. But then $v \in W_1$ as desired.
\end{proof}
%
Next we note the following characterisation of complements:

\begin{proposition}
    \label{prop:complement-iso-to-quotient}
    Let $U$ be a subspace of $V$, and let $W$ be a complement of $U$. The projection $P$ onto $W$ along $U$ induces an isomorphism $V/U \cong W$.
\end{proposition}

\begin{proof}
    Note that $\ker P = U$ and $\im P = W$ by \cref{prop:projection-characterisation}, so \cref{cor:canonical-decomposition} implies that $W \cong V/U$ as claimed. [TODO if I want to do TVS, when is this a homeomorphism?]
\end{proof}

\newcommand{\codim}{\operatorname{codim}}


\newpar

So far in this section we have not made use of the fact that all vector spaces have bases. This fact enters the present discussion through the following result:

\begin{proposition}
    Every subspace $U$ of a vector space $V$ has a complement.
\end{proposition}

\begin{proof}
    Choose a basis $\calU$ for $U$ and extend it to a basis $\calV$ for $V$ using \cref{prop:basis-existence}. Then we clearly have $V = U \oplus \gen{\calV \setminus \calU}$.
\end{proof}

If $U$ is a subspace of $V$, then the dimension of the quotient space $V/U$ is called the \keyword{codimension} of $U$ in $V$ and is denoted $\codim_V U$ or simply $\codim U$. The results above then implies the following:

\begin{corollarynoproof}
    If $U$ is a subspace of $V$, then
    %
    \begin{equation*}
        \dim V
            = \dim U + \codim U.
    \end{equation*}
\end{corollarynoproof}


\begin{corollarynoproof}[The rank--nullity theorem]
    \label{cor:rank-nullity}
    Let $T \in \calL(V,W)$. Then $\codim \ker T = \dim \im T$, and in particular
    %
    \begin{equation*}
        \dim V
            = \dim \ker T + \dim \im T.
    \end{equation*}
\end{corollarynoproof}


\section{Linear maps}

\newpar\label{par:equivalent-similar-congruent-maps}

We begin by surveying the different kinds of ways two linear maps can be \textquote{the same}. The most general way two maps can be the same is the following:
%
\begin{definition}[Equivalence of maps]
    The linear maps $T \colon V \to W$ and $S \colon X \to Y$ are \keyword{equivalent} if there exist linear isomorphisms $P \colon X \to V$ and $Q \colon Y \to W$ such that
    %
    \begin{equation*}
        S = Q\inv TP.
    \end{equation*}
    %
    The matrices $A,B \in \mat{m,n}{\field}$ are \keyword{equivalent} if there exist invertible matrices $P \in \mat{n}{\field}$ and $Q \in \mat{m}{\field}$ such that
    %
    \begin{equation*}
        B
            = Q\inv AP.
    \end{equation*}
\end{definition}
%
If isomorphic vector spaces are \textquote{the same}, then it makes sense that this notion of sameness should be inherited by linear maps between vector spaces. In \cref{par:matrix-rep} we saw that a map between finite-dimensional spaces is equivalent to its basis representation.

Next we have the following notion:
%
\begin{definition}[Similarity of maps]
    The linear maps $T \colon V \to V$ and $S \colon W \to W$ are \keyword{similar} if there exists a linear isomorphism $P \colon W \to V$ such that
    %
    \begin{equation*}
        S = P\inv TP.
    \end{equation*}
    %
    The matrices $A,B \in \mat{n}{\field}$ are \keyword{similar} if there exists an invertible matrix $P \in \mat{n}{\field}$ such that
    %
    \begin{equation*}
        B
            = P\inv AP.
    \end{equation*}
\end{definition}
%
Notice that this only makes sense for endomorphisms, but the two maps in question can of course be defined on different spaces. As before, endomorphisms of finite-dimensional spaces are similar to their basis representation. We will also see in \cref{prop:diagonalisability-equivalent-properties} that a map is so-called \keyword{diagonalisable} if and only if it is similar to a multiplication operator.

The third and final sameness notion is easy to state for matrices, but only makes sense for general linear transformations between spaces equipped with sesquilinear forms. We give the general definition here, but it will only make sense after reading \cref{ch:sesquilinear-forms}.
%
\begin{definition}[Congruency of maps]
    If $V$ and $W$ satisfy the assumptions in \cref{par:Hilbert-space-adjoints}, then the linear maps $T \colon V \to V$ and $S \colon W \to W$ are \keyword{congruent} if there exists a linear isomorphism $P \colon W \to V$ such that
    %
    \begin{equation*}
        S = P^* TP.
    \end{equation*}
    %
    The matrices $A,B \in \mat{n}{\field}$ are \keyword{congruent} if there exists an invertible matrix $P \in \mat{n}{\field}$ such that
    %
    \begin{equation*}
        B
            = P\trans AP.
    \end{equation*}
\end{definition}
%
This notion will turn up in the matrix representation of sesquilinear forms, cf. \cref{par:sesquilinear-matrix-transformation}.

Note that all of these notions can be qualified by adverbs such as \textquote{orthogonally} or \textquote{isometrically} if the mediating maps (or matrices) $P$ and $Q$ above have the corresponding properties, here of being orthogonal and isometric. [TODO but what's the difference between orthogonal and isometric??]


\newpar

If a linear map $T \colon V \to W$ is bijective, then its inverse is easily seen to be linear. But if $T$ is only injective (or surjective), does it have a linear left-inverse (or right-inverse)? The answer is affirmative:

\begin{lemma}
    If $T \colon V \to W$ is injective (surjective), then it has a linear left-inverse (right-inverse).
\end{lemma}

\begin{proof}
    First assume that $T$ is injective and restrict its codomain to obtain an isomorphism $\tilde{T} \colon V \to \im T$. If $U$ is a complement of $\im T$, writing $W = \im T \oplus U$ and letting $S = \tilde{T}\inv \oplus 0$ we get a linear left-inverse of $T$.

    Next assume that $T$ is surjective. Writing $V = \ker T \oplus U$, $T|_U \colon U \to W$ is an isomorphism. If $\iota_U \colon U \to V$ is the inclusion map, $S = \iota_U \circ T|_U\inv$ is a right-inverse of $T$.
\end{proof}
%
Similarly, we can ask whether monomorphisms (epimorphisms) are necessarily injective (surjective):

\begin{lemma}
    If $T \colon V \to W$ is a monomorphism (epimorphism), then it is injective (surjective).
\end{lemma}

\begin{proof}
    First assume that $T$ is not injective, and assume that $v \neq v'$ satisfy $Tv = Tv'$. Let $U$ be a nontrivial vector space, let $u \in U$ be nonzero, and consider linear maps $S,R \colon U \to V$ with $Su = v$ and $Ru = v'$, and that agree on a complement of $\gen{u}$. Then $TS = TR$ but $S \neq R$, so $T$ is not a monomorphism.

    Similarly, if $T$ is not surjective then let $w \in W \setminus \im T$ and define maps $S,R \colon W \to U$ that agree on a complement of $\gen{w}$, and that satisfy $Sw \neq Rw$. Then $ST = RT$, but $S \neq R$.
\end{proof}
%
These lemmas together imply the following:

\begin{theoremnoproof}
    A linear map is injective (surjective) if and only if it is a monomorphism (epimorphism) if and only if it has a left-inverse (right-inverse).
\end{theoremnoproof}


\newpar

Finally we note that between \emph{finite-dimensional} spaces, \cref{cor:rank-nullity} has the following fundamental corollary:

\begin{corollarynoproof}
    If $V$ and $W$ are finite-dimensional, then $T \colon V \to W$ is injective if and only if it is surjective.
\end{corollarynoproof}


\section{Duality}

\newpar\label{par:duality}

If $V$ is an $\field$-vector space, then a \keyword{linear functional} is a linear map $V \to \field$. Since $\field$ itself is an $\field$-vector space, the set $\calL(V,\field)$ is also vector space. We denote this by $V^*$ and call it the \keyword{algebraic dual space} of $V$.

We note that if $v \in V$ is nonzero, then there exists a $\phi \in V^*$ with $\phi(v) \neq 0$: For extend $v$ to a basis for $V$, let $\phi(v) = 1$ and let $\phi = 0$ on any complement of $\gen{v}$.

The algebraic dual space is of little interest when the vector space in question is an infinite-dimensional topological $\fieldK$-vector space. If $V$ is such a space, we instead often let $V^*$ denote the \keyword{topological dual space}, the subspace of the algebraic dual space consisting of the \emph{continuous} functionals. In the sequel, $V^*$ will denote the algebraic dual space unless otherwise stated.


% We are already in a position to prove the following:

% \begin{proposition}
%     \label{prop:dual-of-product}
%     Let $V$ and $W$ be vector spaces. Then the map $\alpha \colon V^* \oplus W^* \to (V \oplus W)^*$ given by
%     %
%     \begin{equation*}
%         \alpha(\phi,\psi)(v,w)
%             = \phi(v) + \psi(w)
%     \end{equation*}
%     %
%     is an isomorphism.
% \end{proposition}

% \begin{proof}
%     We show that $\alpha$ is both injective and surjective. For injectivity, let $\phi,\phi' \in V^*$ and $\psi,\psi' \in W^*$, and assume that $\phi \neq \phi'$. This means that there is some $v \in V$ with $\phi(v) \neq \phi'(v)$. Hence
%     %
%     \begin{equation*}
%         \alpha(\phi,\psi)(v,0)
%             = \phi(v)
%             \neq \phi'(v)
%             = \alpha(\phi',\psi')(v,0),
%     \end{equation*}
%     %
%     so $\alpha(\phi,\psi) \neq \alpha(\phi',\psi')$, and similarly if $\psi \neq \psi'$.

%     To prove surjectivity, let $\chi \in (V \oplus W)^*$ and define $\phi = \chi(\,\cdot\,,0)$ and $\psi = \chi(0,\,\cdot\,)$. Then $\alpha(\phi,\psi) = \chi$ as required.
% \end{proof}


\newpar

We study how a basis for $V$ gives rise to a basis for $V^*$. Let $\calV = \set{v_i}{i \in I}$, where $I$ is some index set, be a basis for a vector space $V$. For $i \in I$ we then define $v_i^* \in V^*$ by $v_i^*(v_j) = \delta_{ij}$, and let $\calV^* = \set{v_i^*}{i \in I}$.

\begin{proposition}
    \label{prop:dual-basis}
    If $\calV$ is a basis for $V$, then the set $\calV^*$ is linearly independent, and hence $\dim V \leq \dim V^*$. If $V$ is finite-dimensional and $\calV = (v_1, \ldots, v_n)$, then $\calV^*$ is a basis for $V^*$ called the \keyword{dual basis} of $\calV$, and
    %
    \begin{equation*}
        \phi
            = \sum_{i=1}^n \phi(v_i) v_i^*
    \end{equation*}
    %
    for all $\phi \in V^*$. In particular, $V \cong V^*$.
\end{proposition}

\begin{proof}
    Applying the functional
    %
    \begin{equation*}
        \alpha_{i_1} v_{i_1}^* + \cdots + \alpha_{i_n} v_{i_n}^* = 0
    \end{equation*}
    %
    to the vector $v_{i_k}$ we find that $\alpha_{i_k} = 0$. If $V$ is finite-dimensional with $\dim V = n$ and $\phi \in V^*$, then
    %
    \begin{equation*}
        \phi(v_j)
            = \sum_{i=1}^n \phi(v_i) \delta_{ij}
            = \sum_{i=1}^n \phi(v_i) v_i^*(v_j),
    \end{equation*}
    %
    so $\phi = \sum_{i=1}^n \phi(v_i) v_i^* \in \gen{\calV^*}$.
\end{proof}
%
The above in particular says that if $\phi = \phi_1 v_1^* + \cdots + \phi_n v_n^*$, then $\phi_i = \phi(v_i)$.

So in the finite-dimensional case, $V$ and $V^*$ are isomorphic. In the infinite-dimensional case, one can show (cf. \cite[Theorem~3.12]{romanlinalg}, which says that then $\dim V < \dim V^*$) that the algebraic dual space of $V$ always has a strictly greater dimension than $V$, so these cannot be isomorphic. If instead $V$ is a topological vector space, then we instead consider the continuous dual space $V^*$, and since this is generally smaller than the algebraic one, $V$ again has a chance of being isomorphic to $V^*$ (though note that the dual basis elements are not guaranteed to be continuous). We will return to this point below.


\newpar

If $V$ is a vector space, we may consider its dual $V^*$. And if $V$ is finite-dimensional, then so is $V^*$, and so we may consider \emph{its} dual, $V^{**}$. On the other hand, if $V$ is a topological vector space, then its topological dual $V^*$ naturally carries the weak$^*$-topology, in which case we may also consider \emph{its} (topological) dual. In either case we call $V^{**}$ the (algebraic or topological) \keyword{double dual space} of $V$. This will again denote the algebraic double dual space unless we state otherwise.

We construct a map from $V$ into $V^{**}$ as follows: For $v \in V$ define $\ev_v \colon V^* \to \field$ by evaluation at $v$, i.e. $\ev_v(\phi) = \phi(v)$. This induces a map $\ev \colon V \to V^{**}$ given by $v \mapsto \ev_v$. If $v \neq w$, then we may hope to find a $\phi \in V^*$ such that $\phi(v) \neq \phi(w)$, which would implies that $\ev_v(\phi) \neq \ev_w(\phi)$, and so $\ev$ would be injective. If $V$ is finite-dimensional, then this is clearly possible. However, if $V$ is an infinite-dimensional topological vector space and $V^*$ instead denotes the topological dual, then we can still performs the constructions above, but then there might not even be any nonzero continuous linear functionals on $V$. This is for instance the case for the Lebesgue space $\calL^p([0,1])$ for $p \in (0,1)$ (cf. \cite[¬ß1.47]{rudinfunctional}). On the other hand, the Hahn--Banach theorem implies that we can in fact find such a functional $\phi$ in case $V$ is locally convex.

Whether or not $\ev$ is injective, it may not be surjective, even if $V$ is a Banach space. If $V^*$ denotes the algebraic dual and $\ev$ is an isomorphism, then $V$ is called \keyword{reflexive}. If $V^*$ instead denotes the topological dual, then we also call $V$ reflexive if $\ev$ is an isomorphism, but we also require it to be a homeomorphism. Indeed, as mentioned above the algebraic dual of an infinite-dimensional vector space $V$ is of strictly greater dimension than $V$ itself, $V$ cannot be isomorphic to its algebraic double dual. On the other hand, finite-dimensional vector spaces are always isomorphic to their double dual, so reflexivity is fairly trivial for vector spaces that are not topological. Hence the notion is usually only interesting for topological vector spaces. Whenever we below consider $V^*$ the algebraic (topological) dual, the property of being reflexive will be in relation to the corresponding algebraic (topological) double dual space.

Finally, if $\calV = (v_1, \ldots, v_n)$ is a basis for $V$, then the basis $\calV^*$ itself has a dual basis $\calV^{**}$. Applying an element $v_i^{**}$ to the functional $\phi$ then yields
%
\begin{equation*}
    v_i^{**}(\phi)
        = \phi_i
        = \phi(v_i).
\end{equation*}
%
That is, $v_i^{**} = \ev_{v_i}$.


\newpar

In the remainder of this section we do not consider topological vector spaces. We now introduce a new concept that is useful in characterising dual spaces:

\begin{definition}
    Let $M \subseteq V$. The \keyword{annihilator} of $M$ is the set
    %
    \begin{equation*}
        M^0
            = \set{\phi \in V^*}{\phi|_M = 0}.
    \end{equation*}
\end{definition}
%
It is easy to see that $M^0$ is a subspace of $V^*$ even when $M$ is not. Furthermore, notice that $\emptyset^0 = V^*$. It is also obvious that if $M \subseteq N$ then $N^0 \subseteq M^0$.

Assume that we have a direct sum decomposition $V = U \oplus W$. If $\phi \in U^*$ then we may extend $\phi$ to a functional $\overline{\phi}$ on $V$ by letting $\overline{\phi}(w) = 0$ for all $w \in W$. We say that $\overline{\phi}$ is the \keyword{extension by $0$} of $\phi$. The map $\eta \colon U^* \to V^*$ given by $\eta(\phi) = \overline{\phi}$ has a left-inverse, namely the pullback\footnote{In \cref{sec:operator-adjoints} we will meet this pullback again under the name \emph{operator adjoint}.}
%
\begin{align*}
    \iota_U^\dagger \colon V^* &\to U^*, \\
    \psi &\mapsto \psi \circ \iota_U,
\end{align*}
%
where $\iota_U \colon U \to V$ is the inclusion map: For notice that $(\iota_U^\dagger \circ \eta)(\phi) = \overline{\phi} \circ \iota_U = \phi$. In particular, $\eta$ is injective and $\iota_U^\dagger$ is surjective. Now notice that $\im \eta = W^0$, and that $\ker \iota_U^\dagger = U^0$. Hence we have proved:

\begin{propositionnoproof}
    \label{prop:subspace-dual-complement-annihilator}
    If $V = U \oplus W$, then the map $U^* \to W^0$ given by $\phi \mapsto \overline{\phi}$ is an isomorphism. If $U$ is finite-dimensional, i.e. if $W$ has finite codimension, then in particular $\dim W^0 = \codim W$.
\end{propositionnoproof}

\begin{propositionnoproof}
    If $U$ is a subspace of $V$, then $V^*/U^0 \cong U^*$.
\end{propositionnoproof}

\begin{corollary}
    If $U$ is a subspace of $V$, then $(V/U)^* \cong U^0$.
\end{corollary}

\begin{proof}
    If $W$ is a complement of $U$, then $V/U \cong W$ by \cref{prop:complement-iso-to-quotient}, so $(V/U)^* \cong W^*$. But then \cref{prop:subspace-dual-complement-annihilator} implies the claim.
\end{proof}


Finally, we can also use annihilators to characterise the dual space of a direct sum:

\begin{proposition}
    $(U \oplus W)^* = U^0 \oplus W^0$.
\end{proposition}
%
Since $U^0 \cong W^*$ and $W^0 \cong U^*$, this gives an alternative proof of \cref{prop:dual-of-product}. TODO find a place for dual-of-product

\begin{proof}
    The sum of $U^0$ and $W^0$ is clearly direct, and the inclusion \textquote{$\supseteq$} is obvious. Now let $\phi \in V^*$, and let $P_U$ and $P_W$ be the projections onto $U$ and $W$ along $W$ and $U$, respectively. then $P_W + P_U = \id_V$, so
    %
    \begin{equation*}
        \phi
            = \phi \circ (P_W + P_U)
            = \phi \circ P_W + \phi \circ P_U
            \in U^0 \oplus W^0,
    \end{equation*}
    %
    proving the other inclusion.
\end{proof}


\section{Operator adjoints}\label{sec:operator-adjoints}

\renewcommand{\hom}[1][]{\mathrm{Hom}_{#1}}

\newpar

Let $\calC$ be a locally small category, and let $f \colon A \to B$ be an arrow in $\calC$. For every object $C$, precomposition with $f$ then induces an arrow
%
\begin{align*}
    \hom[\calC](f,C) \colon \hom[\calC](B,C) &\to \hom[\calC](A,C), \\
        g &\mapsto g \circ f.
\end{align*}
%
This gives rise to a contravariant functor $\hom[\calC](-,C) \colon \calC \to \mathbf{Set}$. Specialising to the case where $\calC$ is the category $\field\text{-}\mathbf{Vect}$ and where $C$ is the field $\field$ (considered as a vector space), we obtain the functor $(-)^*$ sending a vector space $V$ to its (algebraic) dual $V^*$, and a linear map $T$ to its pullback. Since we will use the notation $T^*$ for the Hilbert space adjoint, we instead write $T^\dagger$ for the pullback of $T$, following \textcite{follandrealanalysis}. We also call this the \emph{operator adjoint} of $T$:

\begin{definition}[Operator adjoints]
    Let $V$ and $W$ be $\field$-vector spaces, and let $T \colon V \to W$ be a linear map. The \emph{(operator) adjoint} of $T$ is the pullback
    %
    \begin{align*}
        T^\dagger \colon W^* &\to V^*, \\
        \phi &\mapsto \phi \circ T.
    \end{align*}
\end{definition}
%
This already satisfies $\id_V^\dagger = \id_{V^*}$ and $(ST)^\dagger = T^\dagger S^\dagger$ by functoriality, so that in particular $(T\inv)^\dagger = (T^\dagger)\inv$ when $T$ is invertible. Furthermore, it is easy to show that the map $T \mapsto T^\dagger$ is linear. As before, if $W$ is either finite-dimensional or a locally convex space, if $Tv \neq Sv$ there is a $\phi \in W^*$ with $\phi(Tv) \neq \phi(Sv)$, so in these cases $T \mapsto T^\dagger$ is injective.

\begin{proposition}
    \label{prop:operator-adjoint-kernel-image}
    Let $T \in \calL(V,W)$.
    %
    \begin{enumprop}
        \item \label{enum:operator-adjoint-kernel} $\ker T^\dagger = (\im T)^0$.
        \item \label{enum:operator-adjoint-image} $\im T^\dagger = (\ker T)^0$.
    \end{enumprop}
\end{proposition}

\begin{proof}
    For part \subcref{enum:operator-adjoint-kernel}, notice that
    %
    \begin{align*}
        \ker T^\dagger
            &= \set{\psi \in W^*}{T^\dagger \psi = 0} \\
            &= \set{\psi \in W^*}{\psi \circ T = 0} \\
            &= \set{\psi \in W^*}{\psi(\im T) = \{0\}} \\
            &= (\im T)^0.
    \end{align*}
    %
    For part \subcref{enum:operator-adjoint-image}, if $\phi \in \im T^\dagger$ then there is a $\psi \in W^*$ with $\phi = T^\dagger \psi = \psi \circ T$. Hence $\ker T \subseteq \ker \phi$, so $\phi \in (\ker T)^0$. For the opposite inclusion, let $\phi \in (\ker T)^0$. Let $U$ be a complement of $\ker T$ and let $S \colon W \to U$ be a linear left-inverse of $T|_U$. Letting $\psi = \phi \circ S$ we thus get
    %
    \begin{equation*}
        T^\dagger \psi
            = \psi \circ T
            = \phi \circ S \circ T.
    \end{equation*}
    %
    This agrees with $\phi$ on $U$ by definition of $T$, and it agrees with $\phi$ on $\ker T$ since $\phi$ lies in $(\ker T)^0$. Thus $\phi \in \im T^\dagger$.
\end{proof}


Since $T^\dagger$ is itself a linear map, we may of course consider \emph{its} adjoint $T^{\dagger\dagger} \colon V^{**} \to W^{**}$. If $V$ is not reflexive, then as far as I know there is little to say about $T^{\dagger\dagger}$, but if it is then we have the following:

\begin{proposition}
    If $V$ is reflexive and $T \colon V \to W$ is linear, then
    %
    \begin{equation*}
        T^{\dagger\dagger}
            = \ev \circ T \circ \ev\inv,
    \end{equation*}
    %
    where the leftmost $\ev$ is evaluation on $W$, and the rightmost $\ev$ is evaluation on $V$.
\end{proposition}

\begin{proof}
    If $v \in V$, then notice that $T^{\dagger\dagger} \ev_v = \ev_v \circ T^\dagger$. But if $\phi \in W^*$, then notice that
    %
    \begin{equation*}
        (\ev_v \circ T^\dagger)(\phi)
            = (T^\dagger\phi)v
            = (\phi \circ T)v
            = \ev_{Tv}(\phi).
    \end{equation*}
    %
    Hence $T^{\dagger\dagger} \ev_v = \ev_{Tv}$ for all $v \in V$, so $T^{\dagger\dagger} \circ \ev = \ev \circ T$. The claim follows since $V$ is reflexive.
\end{proof}



\newpar

We now consider the case where $V$ and $W$ are finite-dimensional in more detail.

\newcommand{\rank}{\operatorname{rank}}

\begin{corollary}
    \label{cor:adjoint-rank}
    If $T \in \calL(V,W)$ with $V$ and $W$ finite-dimensional, then $\rank T^\dagger = \rank T$.
\end{corollary}

\begin{proof}
    Note that
    %
    \begin{equation*}
        \dim \im T^\dagger
            \overset{(1)}{=} \dim (\ker T)^0
            \overset{(2)}{=} \codim \ker T
            \overset{(3)}{=} \dim \im T,
    \end{equation*}
    %
    where $(1)$ follows by \cref{enum:operator-adjoint-kernel}, $(2)$ by \cref{prop:subspace-dual-complement-annihilator}, and $(3)$ by \cref{cor:rank-nullity}.
\end{proof}


\begin{proposition}
    \label{prop:adjoint-mr}
    If $T \in \calL(V,W)$ is a linear map between finite-dimensional vector spaces, and $\calV$ and $\calW$ are ordered bases for $V$ and $W$ respectively, then
    %
    \begin{equation*}
        \mr{\calV^*}{T^\dagger}{\calW^*}
            = \bigl( \mr{\calW}{T}{\calV} \bigr)\trans.
    \end{equation*}
\end{proposition}

\begin{proof}
    Write $\calV = (v_1, \ldots, v_n)$ and $\calW = (w_1, \ldots, w_m)$. Then
    %
    \begin{equation*}
        \bigl( \mr{\calW}{T}{\calV} \bigr)_{ij}
            = \bigl( \coordvec{Tv_j}{\calW} \bigr)_i
            = w_i^*(Tv_j),
    \end{equation*}
    %
    and
    %
    \begin{equation*}
        \bigl( \mr{\calV^*}{T^\dagger}{\calW^*} \bigr)_{ij}
            = \bigl( \coordvec{T^\dagger w_j^*}{\calV^*} \bigr)_i
            = v_i^{**}(T^\dagger w_j^*)
            = T^\dagger w_j^*(v_i)
            = w_j^*(Tv_i).
    \end{equation*}
    %
    These expressions are the same, but with $i$ and $j$ switched.
\end{proof}

From this we obtain the following result on the row and column rank of a matrix. This is often proved by showing that one can apply elementary row and column operations to the matrix in question, preserving the row and column rank, and obtain a diagonal matrix whose entries are either zero or one. The common row and column rank of the matrix is then simply the number of ones. Going through the abstract theory above we avoid these considerations.

\begin{corollary}
    The row rank and the column rank of a matrix $A \in \mat{m,n}{\field}$ are equal.
\end{corollary}

\begin{proof}
    The matrix representation of the multiplication operator $M_A$ with respect to the standard bases on $\field^n$ and $\field^m$ is just $A$ itself, and \cref{prop:adjoint-mr} then implies that the matrix representation of $(M_A)^\dagger$ with respect to the dual bases is $A\trans$. But the rank of an operator equals the rank of any matrix representation of that operator, so \cref{cor:adjoint-rank} implies that $A$ and $A\trans$ have the same (column) rank. Finally, the column rank of $A\trans$ is the row rank of $A$, proving the claim.
\end{proof}


\newpar

If $V$ and $W$ are instead \emph{topological} vector spaces, of arbitrary dimension, and $V^*$ and $W^*$ denote their respective \emph{continuous} dual spaces, then we may also consider the adjoint $T^\dagger$ of a \emph{continuous} linear map $T \colon V \to W$. It then turns out that $T^\dagger$ is also continuous when $V^*$ and $W^*$ are equipped with the appropriate topologies.

\begin{proposition}
    Let $T \colon V \to W$ be a continuous linear map, and let $T^\dagger \colon W^* \to V^*$ be its adjoint.
    %
    \begin{enumprop}
        \item $T^\dagger$ is continuous with respect to the weak$^*$-topologies on $W^*$ and $V^*$.

        \item \label{enum:operator-adjoint-continuous-normed} If $V$ and $W$ are normed vector spaces, then $T^\dagger$ is continuous with respect to the operator norms on $W^*$ and $V^*$, and $\norm{T^\dagger} = \norm{T}$.
    \end{enumprop}
\end{proposition}

\begin{proof}
    Let $(\phi_i)_{i \in I}$ be a net in $W^*$ that converges to some $\phi \in W^*$. That is, $\phi_i(w) \to \phi(w)$ for all $w \in W$, so in particular $\phi_i(Tv) \to \phi(Tv)$ for all $v \in V$. But then $T^\dagger \phi_i = \phi_i \circ T$ converges to $T^\dagger \phi = \phi \circ T$, so $T^\dagger$ is continuous as claimed.

    If $V$ and $W$ are normed, then
    %
    \begin{equation*}
        \norm{T^\dagger \phi}
            = \norm{\phi \circ T}
            \leq \norm{\phi} \, \norm{T}
    \end{equation*}
    %
    for all $\phi \in W^*$, implying that $T^\dagger$ is bounded with $\norm{T^\dagger} \leq \norm{T}$. If $T \neq 0$, then let $v \in V$ with $\norm{v} = 1$ such that $Tv \neq 0$. The Hahn--Banach theorem then furnishes a $\phi \in W^*$ with $\norm{\phi} = 1$ and $\phi(Tv) = \norm{Tv}$ (cf. \cite[Theorem~5.8(b)]{follandrealanalysis}). It follows that
    %
    \begin{equation*}
        \norm{T^\dagger}
            \geq \norm{T^\dagger \phi}
            \geq \abs{(T^\dagger \phi)v}
            = \abs{\phi(Tv)}
            = \norm{Tv}.
    \end{equation*}
    %
    This inequality then holds for all $v \in V$ with $\norm{v} = 1$, implying that $\norm{T^\dagger} \geq \norm{T}$.
\end{proof}

% TODO: $T \mapsto T^\dagger$ not necessarily injective now?

\section{Resolutions of the identity}

Let $V$ be a vector space. Two projections $P,Q \in \calL(V)$ are said to be \keyword{orthogonal} if $PQ = QP = 0$, in which case we write $P \perp Q$. A \keyword{resolution of the identity} on $V$ is a decomposition
%
\begin{equation*}
    \id_V
        = P_1 + \cdots + P_k.
\end{equation*}
%
of the identity map on $V$, where $P_1, \ldots, P_k$ are pairwise orthogonal projections on $V$. If $V$ is an inner product space and the $P_i$ are themselves orthogonal projections, then we also say that the resolution of the identity is \keyword{orthogonal}. [TODO move orthogonal to somewhere in chapter on sesquilinear]

\begin{proposition}
    \label{prop:resolution-of-the-identity-characterisation}
    If $\id_V = P_1 + \cdots + P_k$ is a resolution of the identity, then
    %
    \begin{equation*}
        V
            = \im P_1 \oplus \cdots \oplus \im P_k,
        \quad \text{and} \quad
        \ker P_i
            = \bigoplus_{j \neq i} \im P_j
    \end{equation*}
    %
    for all $i = 1, \ldots, k$. Conversely, if
    %
    \begin{equation*}
        V
            = U_1 \oplus \cdots \oplus U_k
    \end{equation*}
    %
    and $P_i$ is the projection onto $U_i$ along $\bigoplus_{j \neq i} U_j$, then $\id_V = P_1 + \cdots + P_k$ is a resolution of the identity.
\end{proposition}

\begin{proof}
    Clearly $V$ is a (not necessarily direct) sum of the above images. To see that the sum is direct, if for $v_1, \ldots, v_k \in V$ we have
    %
    \begin{equation*}
        P_1 v_1 + \cdots P_k v_k = 0,
    \end{equation*}
    %
    then applying $P_i$ we get $P_i v_i = 0$. Furthermore, we clearly have $\bigoplus_{j \neq i} \im P_j \subseteq \ker P_i$ by orthogonality. For the opposite inclusion, notice that
    %
    \begin{equation*}
        \im P_i \oplus \ker P_i
            = V
            = \im P_i \oplus \biggl( \bigoplus_{j \neq i} \im P_j \biggr).
    \end{equation*}
    %
    And since $\bigoplus_{j \neq i} \im P_j \subseteq \im P_i$ by orthogonality, the opposite inclusion follows from \cref{lem:nested-complements}.

    For the converse, if $i \neq j$ then $\im P_i = U_i \subseteq \ker P_j$, so $P_i \perp P_j$. Furthermore, if $v = u_1 + \cdots + u_k$ with $u_i \in U_i$, then $P_i v = u_i$, so
    %
    \begin{equation*}
        v
            = u_1 + \cdots + u_k
            = P_1 v + \cdots + P_k v
            = (P_1 + \cdots + P_k) v,
    \end{equation*}
    %
    as desired.
\end{proof}





\chapter{Determinants}

\section{Existence and uniqueness}\label{sec:determinant-existence-uniqueness}

\newpar

We begin by establishing some terminology and some basic properties of maps between modules. If $M_1, \ldots, M_n, N$ are modules over a commutative ring $R$, a map
%
\begin{equation*}
    \phi \colon M_1 \prod \cdots \prod M_n \to N
\end{equation*}
%
is called \keyword{$n$-linear} if, for all $i$, the maps $m_i \mapsto \phi(m_1, \ldots, m_n)$ are linear for all choices of $m_j \in M_j$ where $j \neq i$. Since there is a natural isomorphism $\mat{m,n}{R} \cong (R^n)^m$, a map $\phi \colon \mat{m,n}{R} \to N$ that is linear in each row is also called $n$-linear.

In the case $M_1 = \cdots = M_n$, we call $\phi$ \keyword{alternating} if $\phi(m_1, \ldots, m_n) = 0$ whenever $m_i = m_j$ for some $i \neq j$. Furthermore, $\phi$ is called \keyword{skew-symmetric} if
%
\begin{multline*}
    \phi(m_1, \ldots, m_{i-1}, m_i, m_{i+1}, \ldots, m_{j-1}, m_j, m_{j+1}, \ldots, m_n) \\
        = -\phi(m_1, \ldots, m_{i-1}, m_j, m_{i+1}, \ldots, m_{j-1}, m_i, m_{j+1}, \ldots, m_n)
\end{multline*}
%
for all $i < j$.

With this terminology at hand, we can now define determinants:

\begin{definition}[Determinant functions]
    If $n$ be a positive integer, a \keyword{determinant function} is a map $\phi \colon \mat{n}{R} \to R$ that is $n$-linear, alternating, and which satisfies $\phi(I_n) = 1$.
\end{definition}


Before proceeding with proving the existence of determinants, we need the following lemma:

\begin{lemma}
    Let $M$ and $N$ be $R$-modules, and let $\phi \colon M^n \to N$ be an $n$-linear map.
    %
    \begin{enumlem}
        \item \label{enum:alternating-implies-skew-symmetric} If $\phi$ is alternating, then $\phi$ is skew-symmetric. If $\chr R \neq 2$ then the converse also holds.
        \item \label{enum:alternating-adjacent-rows} If $\phi(m_1, \ldots, m_n) = 0$ whenever $m_i = m_{i+1}$ for some $i = 1, \ldots, n-1$, then $\phi$ is alternating.
    \end{enumlem}
\end{lemma}
%
We shall not use the converse direction of \cref{enum:alternating-implies-skew-symmetric} but we include it for completeness.

\begin{proof}
\begin{proofsec*}
    \item[\Namesubcref{enum:alternating-implies-skew-symmetric}]
    Consider $m_1, \ldots, m_n \in M$, and let $1 \leq i < j \leq n$. Define a map $\psi \colon M \prod M \to N$ by
    %
    \begin{equation*}
        \psi(a, b)
            = \phi(m_1, \ldots, m_{i-1}, a, m_{i+1}, \ldots, m_{j-1}, b, m_{j+1}, \ldots, m_n),
    \end{equation*}
    %
    and notice that it suffices to show that $\psi(m_i,m_j) = -\psi(m_j,m_i)$. But $\psi$ is $2$-linear and alternating, so for $a,b \in M$ we have
    %
    \begin{equation*}
        \psi(a+b, a+b)
            = \psi(a,a) + \psi(a,b) + \psi(b,a) + \psi(b,b)
            = \psi(a,b) + \psi(b,a).
    \end{equation*}
    %
    Thus $\psi(m_i,m_j) = -\psi(m_j,m_i)$, so $\phi$ is skew-symmetric as claimed.

    Conversely, if $\chr R \neq 2$ and $\psi$ is skew-symmetric, then since $\psi(a,b) = -\psi(b,a)$, letting $a = b$ we have $2\psi(a,a) = 0$, so $\psi(a,a) = 0$.

    \item[\Namesubcref{enum:alternating-adjacent-rows}] 
    The argument above shows that, in particular, if $A, B \in M^n$, and $B$ is obtained from $A$ by interchanging two adjacent elements, then $\phi(B) = -\phi(A)$. Assuming now that $B$ is obtained from $A$ by interchanging the $i$th and $j$th elements in $A$, with $i < j$, we claim that we may obtain $B$ by successively interchanging adjacent elements of $A$. Writing $A = (m_1, \ldots, m_n)$, we first perform $j - i$ such interchanges and arrive that the tuple
    %
    \begin{equation*}
        (m_1, \ldots, m_{i-1}, m_{i+1}, \ldots, m_{j-1}, m_j, m_i, m_{j+1}, \ldots, m_n),
    \end{equation*}
    %
    moving $m_i$ to the right $j - i$ places. Next we perform another $j-i-1$ interchanges, moving $m_j$ to the left until we reach
    %
    \begin{equation*}
        B = (m_1, \ldots, m_{i-1}, m_j, m_{i+1}, \ldots, m_{j-1}, m_i, m_{j+1}, \ldots, m_n).
    \end{equation*}
    %
    Since each interchange results in a sign change, we have
    %
    \begin{equation*}
        \phi(B) = (-1)^{2(j-i) - 1} \phi(A) = -\phi(A).
    \end{equation*}
    %
    If $m_i = m_j$ for $i < j$, then we claim that $\phi(A) = 0$. For let $B$ be obtained from $A$ by interchanging $m_{i+1}$ and $m_j$. Then $\phi(B) = 0$, so $\phi(A) = -\phi(B) = 0$ by the above argument, and hence $\phi$ is alternating as claimed.
    \end{proofsec*}
\end{proof}


\newpar

We now proceed with constructing determinants. If $A \in \mat{n}{R}$ with $n > 1$ and $1 \leq i,j \leq n$, denote by $M(A)_{i,j}$ the matrix in $\mat{n-1}{R}$ obtained by removing the the $i$th row and the $j$th column of $A$. This is called the \keyword{$(i,j)$-th minor} of $A$. If $\phi \colon \mat{n-1}{R} \to R$ is an $(n-1)$-linear function and $A \in \mat{n}{R}$, then we write $\phi_{i,j}(A) = \phi(M(A)_{i,j})$. Then $\phi_{i,j} \colon \mat{n}{R} \to R$ is clearly linear in all rows except row $i$, and is independent of row $i$.

We construct determinants recursively, using the Laplace expansion:

\begin{theorem}[Construction of determinants]
    \label{thm:determinant-recursive-definition}
    Let $n > 1$, and let $\phi \colon \mat{n-1}{R} \to R$ be alternating and $(n-1)$-linear. For $j = 1, \ldots, n$ define a map $\psi_j \colon \mat{n}{R} \to R$ by
    %
    \begin{equation*}
        \psi_j(A)
            = \sum_{i=1}^n (-1)^{i+j} a_{ij} \phi_{i,j}(A),
    \end{equation*}
    %
    for $A = (a_{ij}) \in \mat{n}{R}$. Then $\psi_j$ is alternating and $n$-linear. If $\phi$ is a determinant function, then so is $\psi_j$.
\end{theorem}

\begin{proof}
    Let $A = (a_{ij}) \in \mat{n}{R}$. Then $A \mapsto a_{ij}$ is independent of all rows except row $i$, and $\phi_{i,j}$ is linear in all rows except row $i$. Thus $A \mapsto a_{ij} \phi_{i,j}(A)$ is linear in all rows except row $i$. Conversely, $A \mapsto a_{ij}$ is linear in row $i$, and $\phi_{i,j}$ is independent of row $i$, so $A \mapsto a_{ij} \phi_{i,j}(A)$ is also linear in row $i$. Since $\psi_j$ is a linear combination of $n$-linear maps, is it itself $n$-linear.

    Now assume that $A$ has two equal adjacent rows, say $a_k, a_{k+1} \in R^n$. If $i \neq k$ and $i \neq k+1$, then $M(A)_{i,j}$ has two equal rows, so $\phi_{i,j}(A) = 0$. Thus
    %
    \begin{equation*}
        \psi_j(A)
            = (-1)^{k+j} a_{kj} \phi_{k,j}(A)
              + (-1)^{k+1+j} a_{(k+1)j} \phi_{k+1,j}(A).
    \end{equation*}
    %
    Since $a_k = a_{k+1}$ we also have $a_{kj} = a_{(k+1)j}$ and $M(A)_{k,j} = M(A)_{k+1,j}$. Thus $\psi_j(A) = 0$, so \cref{enum:alternating-adjacent-rows} implies that $\psi_j$ is alternating.

    Finally suppose that $\phi$ is a determinant function. Then $M(I_n)_{j,j} = I_{n-1}$ and we have
    %
    \begin{equation*}
        \psi_j(I_n)
            = (-1)^{j+j} \phi_{j,j}(I_n)
            = \phi(I_{n-1})
            = 1,
    \end{equation*}
    %
    so $\psi_j$ is also a determinant function.
\end{proof}


\begin{corollary}[Existence of determinants]
    For every positive integer $n$, there exists a determinant function $\mat{n}{R} \to R$.
\end{corollary}

\begin{proof}
    The identity map on $\mat{1}{R} \cong R$ is a determinant function for $n = 1$, and \cref{thm:determinant-recursive-definition} allows us to recursively construct a determinant for each $n > 1$.
\end{proof}


\newpar

We finally show that determinants are unique by showing that any determinant function must be given by the Leibniz formula:

\begin{theorem}[Uniqueness of determinants]
    \label{thm:determinant-uniqueness}
    Let $n$ be a positive integer. There is precisely one determinant function on $\mat{n}{R}$, namely the function $\det \colon \mat{n}{R} \to R$ given by
    %
    \begin{equation*}
        \det A
            = \sum_{\sigma \in S_n} (\sign\sigma) a_{1 \sigma(1)} \cdots a_{n \sigma(n)}
    \end{equation*}
    %
    for $A = (a_{ij}) \in \mat{n}{R}$. If $\phi \colon \mat{n}{R} \to R$ is any alternating $n$-linear function, then
    %
    \begin{equation*}
        \phi(A)
            = (\det A) \phi(I_n).
    \end{equation*}
\end{theorem}
%
We use the notation $\det$ for the unique determinant on $\mat{n}{R}$ for all $n$.

\begin{proof}
    Let $e_1, \ldots, e_n$ denote the rows of $I_n$, and denote the rows of a matrix $A = (a_{ij}) \in \mat{n}{R}$ by $a_1, \ldots, a_n$. Then $a_i = \sum_{j=1}^n a_{ij} e_j$, so
    %
    \begin{equation*}
        \phi(A)
            = \sum_{k_1, \ldots, k_n} a_{1k_1} \cdots a_{nk_n} \phi(e_{k_1}, \ldots, e_{k_n}),
    \end{equation*}
    %
    where the sum is taken over all $k_i = 1, \ldots, n$. Since $\phi$ is alternating we have $\phi(e_{k_1}, \ldots, e_{k_n}) = 0$ if two of the indices $k_1, \ldots, k_n$ are equal. Thus it suffices to sum over those sequences $(k_1, \ldots, k_n)$ that are permutations of $(1, \ldots, n)$, and so
    %
    \begin{equation*}
        \phi(A)
            = \sum_{\sigma \in S_n} a_{1 \sigma(1)} \cdots a_{n \sigma(n)} \phi(e_{\sigma(1)}, \ldots, e_{\sigma(n)}).
    \end{equation*}
    %
    Next notice that, since $\phi$ is also skew-symmetric by \cref{enum:alternating-implies-skew-symmetric}, we have $\phi(e_{\sigma(1)}, \ldots, e_{\sigma(n)}) = (-1)^m \phi(e_1, \ldots, e_n)$, where $m$ is the number of transpositions of $(1, \ldots, n)$ it takes to obtain the permutation $(\sigma(1), \ldots, \sigma(n))$. But then $(-1)^m$ is just the sign of $\sigma$, so
    %
    \begin{equation*}
        \phi(A)
            = \sum_{\sigma \in S_n} (\sign\sigma) a_{1 \sigma(1)} \cdots a_{n \sigma(n)} \phi(I_n).
    \end{equation*}
    %
    Finally, if $\phi$ is a determinant function, then $\phi(I_n) = 1$, so we must have $\phi = \det$. The rest of the theorem follows directly from this.
\end{proof}




\section{Properties of determinants}



\newpar

We begin with what is surely the most important property of determinants, which is also our first application of the uniqueness theorem for determinants:

\begin{theorem}
    \label{thm:determinant-multiplicative}
    Let $A,B \in \mat{n}{R}$. Then
    %
    \begin{equation*}
        \det AB
            = (\det A) (\det B).
    \end{equation*}
    %
    In particular, $\det \colon \matGL{n}{R} \to R^*$ is a group homomorphism.
\end{theorem}

\begin{proof}
    The map $\phi \colon \mat{n}{R} \to R$ given by $\phi(A) = \det AB$ is clearly $n$-linear and alternating. Hence $\phi(A) = (\det A) \phi(I)$, and $\phi(I) = \det B$.

    Furthermore, if $A$ is invertible, then $1 = \det I = (\det A) (\det A\inv)$. Thus $\det A \in R^*$, so $\det$ is a group homomorphism as claimed.
\end{proof}


\begin{corollary}
    \label{cor:determinant-similar-matrices}
    If $A,B \in \mat{n}{\field}$ are similar matrices, then $\det A = \det B$.
\end{corollary}

\begin{proof}
    Let $P \in \mat{n}{\field}$ be such that $B = P\inv AP$. \Cref{thm:determinant-multiplicative} then implies that
    %
    \begin{equation*}
        \det B
            = (\det P\inv)(\det A)(\det P)
            = (\det A)(\det P\inv P)
            = \det A.
    \end{equation*}
\end{proof}

\Cref{cor:determinant-similar-matrices} allows us to define the determinant of a general linear operator $T \colon V \to V$ on a finite-dimensional $\field$-vector space. If $\calV$ and $\calW$ are bases for $V$, then the matrix representations $\mr{\calV}{T}{\calV}$ and $\mr{\calW}{T}{\calW}$ are similar. This allows us to define the determinant $\det T$ of $T$ as the matrix representation $\mr{\calV}{T}{\calV}$ for any basis $\calV$.

Next the fairly obvious result that the determinant of a matrix equals the determinant of its transpose:

\begin{proposition}
    Let $A \in \mat{n}{R}$. Then $\det A = \det A\trans$.
\end{proposition}

\begin{proof}
    Writing $A = (a_{ij})$, first notice that
    %
    \begin{equation*}
        \det A\trans
            = \sum_{\sigma \in S_n} (\sign \sigma\inv) a_{\sigma(1)1} \cdots a_{\sigma(n)n},
    \end{equation*}
    %
    since $\sign \sigma = \sign \sigma\inv$. Next notice that, if $j = \sigma(i)$, then $a_{\sigma(i)i} = a_{j \sigma\inv(j)}$. Since $R$ is commutative, it follows that
    %
    \begin{equation*}
        \det A\trans
            = \sum_{\sigma \in S_n} (\sign \sigma\inv) a_{1\sigma\inv(1)} \cdots a_{n\sigma\inv(n)},
    \end{equation*}
    %
    and since $\sigma \mapsto \sigma\inv$ is a bijection on $S_n$, it follows that $\det A\trans = \det A$ as desired.
\end{proof}


\newpar

Let $A \in \mat{n}{R}$. For $1 \leq i,j \leq n$, the \keyword{$(i,j)$-th cofactor} of $A$ is the number $A_{i,j} = (-1)^{i+j} \det M(A)_{i,j}$, where we recall that $M(A)_{i,j}$ is the $(i,j)$-th minor of $A$. The \keyword{cofactor matrix} of $A$ is the matrix $\cof A \in \mat{n}{R}$ whose $(i,j)$-th entry is the cofactor $A_{i,j}$. Note that
%
\begin{equation*}
    (A\trans)_{i,j}
        = (-1)^{i+j} \det M(A\trans)_{i,j}
        = (-1)^{j+i} \det M(A)_{j,i}
        = A_{j,i},
\end{equation*}
%
so $\cof A\trans = (\cof A)\trans$. Of greater importance than the cofactor matrix is the \keyword{adjoint matrix} of $A$, written $\adj A$, which is just the transpose of $\cof A$. That is, the $(i,j)$-th entry of $\adj A$ is the cofactor $A_{j,i}$. Similar to the cofactor matrix we have
%
\begin{equation*}
    \adj A\trans
        = (\cof A\trans)\trans
        = \cof A
        = (\adj A)\trans.
\end{equation*}
%
We then have the following:

\begin{proposition}
    \label{thm:adjoint-matrix-product}
    Let $A \in \mat{n}{R}$. Then
    %
    \begin{equation*}
        (\adj A) A
            = (\det A) I
            = A (\adj A).
    \end{equation*}
\end{proposition}

\begin{proof}
    Writing $A = (a_{ij})$ and fixing some $j \in \{1, \ldots, n\}$, \cref{thm:determinant-recursive-definition} implies that
    %
    \begin{equation*}
        \det A
            = \sum_{i=1}^n (-1)^{i+j} a_{ij} \det M(A)_{i,j}
            = \sum_{i=1}^n a_{ij} A_{i,j},
    \end{equation*}
    %
    which is just the $(j,j)$-th entry in the product $(\adj A)A$.

    Next we claim that if $k \neq j$, then $\sum_{i=1}^n a_{ik} A_{i,j} = 0$. Let $B = (b_{ij}) \in \mat{n}{R}$ be the matrix obtained from $A$ by replacing the $j$th column of $A$ by its $k$th column. Then $B$ has two equal columns, so $\det B = 0$. Also, $b_{ij} = a_{ik}$ and $M(B)_{i,j} = M(A)_{i,j}$, so it follows that
    %
    \begin{align*}
        0
            &= \det B
             = \sum_{i=1}^n (-1)^{i+j} b_{ij} \det M(B)_{i,j} \\
            &= \sum_{i=1}^n (-1)^{i+j} a_{ik} \det M(A)_{i,j}
             = \sum_{i=1}^n a_{ik} A_{i,j}.
    \end{align*}
    %
    That is, the $(j,k)$-th entry of the product $(\adj A)A$ is zero, so the off-diagonal entries of $(\adj A)A$ are zero. In total we thus have $(\adj A)A = (\det A) I$.

    Finally we prove the equality $A(\adj A) = (\det A) I$, Applying the first equality to $A\trans$ yields
    %
    \begin{equation*}
        (\adj A\trans) A\trans
            = (\det A\trans)I
            = (\det A)I,
    \end{equation*}
    %
    and transposing we get
    %
    \begin{equation*}
        A (\adj A)
            = A (\adj A\trans)\trans
            = (\det A) I
    \end{equation*}
    %
    as desired.
\end{proof}


\begin{corollary}
    Let $A \in \mat{n}{R}$. The following are equivalent:
    %
    \begin{enumcor}
        \item $A$ is a (two-sided) unit in $\mat{n}{R}$.
        \item $A$ is a left- or right-unit in $\mat{n}{R}$.
        \item $\det A$ is a unit in $R$.
    \end{enumcor}
\end{corollary}

\begin{proof}
    If $A$ is e.g. a left-unit, then \cref{thm:determinant-multiplicative} implies that
    %
    \begin{equation*}
        1
            = \det I_n
            = (\det A)(\det A\inv),
    \end{equation*}
    %
    so $\det A$ is a unit in $R$. Conversely, if $\det A$ is a unit then \cref{thm:adjoint-matrix-product} implies that $(\det A)\inv(\adj A)$ is a two-sided inverse of $A$.
\end{proof}

Notice that this gives us a second proof of the fact that a matrix is invertible just when it has either a left- or right-inverse. In fact, we see that this holds for matrices with entries in any commutative ring.


\newpar

We close this section by proving a result on the determinant of a block matrix:

\begin{proposition}
    \label{prop:block-matrix-determinant}
    Let $A_{11}, \ldots, A_{nn}$ be square matrices with entries in $R$ and consider the block matrix
    %
    \begin{equation*}
        M
            = \begin{pmatrix}
                A_{11} & A_{12} & \cdots & A_{1n} \\
                0      & A_{22} & \cdots & A_{2n} \\
                \vdots & \ddots & \ddots & \vdots \\
                0      & \cdots & 0      & A_{nn}
            \end{pmatrix},
    \end{equation*}
    %
    where the remaining $A_{ij}$ are matrices of appropriate dimensions. Then $\det M = \bigprod_{i=1}^n \det A_{ii}$.
\end{proposition}

\begin{proof}
    By induction it suffices to consider the case where $M$ has the block form
    %
    \begin{equation*}
        M
            = \begin{pmatrix}
                A & C \\
                0 & B
            \end{pmatrix},
    \end{equation*}
    %
    where $A \in \mat{r}{R}$, $B \in \mat{s}{R}$ and $C \in \mat{r,s}{R}$ for appropriate integers $r,s$. Notice that if we define the matrices
    %
    \begin{equation*}
        M_1
            = \begin{pmatrix}
                I_r & 0 \\
                0   & B
            \end{pmatrix}
        \quad \text{and} \quad
        M_2
            = \begin{pmatrix}
                A & C   \\
                0 & I_s
            \end{pmatrix},
    \end{equation*}
    %
    then $M = M_1 M_2$. But using \cref{thm:determinant-recursive-definition} we easily see that $\det M_1 = \det B$ and $\det M_2 = \det A$, so it follows that
    %
    \begin{equation*}
        \det M
            = (\det M_1) (\det M_2)
            = (\det A) (\det B)
    \end{equation*}
    %
    as desired.
\end{proof}


\section{Cross products}

\newpar

We now study rigorously the well-known 

\begin{definition}[Cross products]
    Let $v = (\alpha_1, \alpha_2, \alpha_3)$ and $w = (\beta_1, \beta_2, \beta_3)$ be vectors in $\reals^3$. The \keyword{cross product} of $v$ and $w$ is the vector
    %
    \begin{equation*}
        v \crossp w =
        \begin{pmatrix}
            \alpha_2 \beta_3 - \alpha_3 \beta_2 \\
            \alpha_3 \beta_1 - \alpha_1 \beta_3 \\
            \alpha_1 \beta_2 - \alpha_2 \beta_1
        \end{pmatrix}.
    \end{equation*}
\end{definition}
%
Denote the standard basis on $\reals^3$ by $\calE = (e_1, e_2, e_3)$. We easily see that $e_i \crossp e_j = e_k$ when $(i,j,k)$ is a cyclic permutation of $(1,2,3)$. Perhaps surprisingly, there is an important connection between cross products and determinants which from which will follow most of the elementary properties of cross products:

\begin{lemma}
    \label{lem:cross-product-determinant}
    Let $v,w,u \in \reals^3$. Then
    %
    \begin{equation*}
        \inner{u}{v \crossp w}
            = \det(u,v,w).
    \end{equation*}
\end{lemma}

\begin{proof}
    By multilinearity of the inner product and of determinants, it suffices to prove the lemma when $u$ is a basis vector. But it is clear that
    %
    \begin{equation*}
        \inner{e_i}{v \crossp w}
            = \det(e_i,v,w),
    \end{equation*}
    %
    as desired.
\end{proof}
%
The product $\inner{u}{v \crossp w}$ is called the \keyword{(scalar) triple product} of $u$, $v$ and $w$, and is denoted $[u,v,w]$. We call it the \emph{scalar} triple product to distinguish it from the \emph{vector} triple product $u \crossp (v \crossp w)$, whose properties we will examine in \cref{cor:vector-triple-product}. The scalar triple product has some very nice properties summarised in the following proposition:

\begin{proposition}
    Let $u,v,w \in \reals^3$.
    %
    \begin{enumprop}
        \item The cross product map $(v,w) \mapsto v \crossp w$ is bilinear.

        \item $v \crossp w = - w \crossp v$.

        \item The triple product $[u,v,w]$ is invariant under cyclic permutations, i.e.
        %
        \begin{equation*}
            [u,v,w]
                = [v,w,u]
                = [w,u,v]
        \end{equation*}
        %
        and invariant under interchange of inner product and cross product, i.e.
        %
        \begin{equation*}
            \inner{u}{v \crossp w}
                = [u,v,w]
                = \inner{u \crossp v}{w}.
        \end{equation*}

        \item $v \crossp w = 0$ if and only if $v$ and $w$ are linearly dependent.

        \item $v \crossp w$ is orthogonal to both $v$ and $w$.
    \end{enumprop}
\end{proposition}

\begin{proof}
    The first three claims follow from \cref{lem:cross-product-determinant} since the determinant is multilinear and alternating (hence skew-symmetric).
    
    For the fourth claim, if $v$ and $w$ are linearly dependent then $\det(u,v,w) = 0$ for all $u \in \reals^3$, so $v \crossp w = 0$. Conversely, if $v$ and $w$ are linearly independent, then extending to a basis $(u,v,w)$ for $\reals^3$ we have $\det(u,v,w) \neq 0$, implying that $v \crossp w \neq 0$.

    To prove the final claim, notice that
    %
    \begin{equation*}
        \inner{v}{v \crossp w}
            = \det(v,v,w)
            = 0,
    \end{equation*}
    %
    and similarly for $w$.
\end{proof}


\begin{proposition}
    Let $a,b,v,w \in \reals^3$. Then
    %
    \begin{equation*}
        \inner{a \crossp b}{v \crossp w}
            = \det
            \begin{pmatrix}
                \inner{a}{v} & \inner{b}{v} \\
                \inner{a}{w} & \inner{b}{w}
            \end{pmatrix}.
    \end{equation*}
    %
    In particular,
    %
    \begin{equation}
        \label{eq:Lagrange-identity}
        \norm{v \crossp w}^2
            = \det
            \begin{pmatrix}
                \norm{v}^2 & \inner{v}{w} \\
                \inner{v}{w} & \norm{w}^2
            \end{pmatrix}.
    \end{equation}
\end{proposition}

\begin{proof}
    By linearity it suffices to prove the identity when the four vectors are basis vectors. If $a = b$ or $v = w$ then both sides are zero, so we may assume that $a = e_i$, $b = e_j$, $v = e_k$ and $v = e_l$ with $i \neq j$ and $k \neq l$. By potentially swapping $a$ and $b$ and/or $v$ and $w$ we may assume that $e_i \crossp e_j = e_p$ and $e_k \crossp e_l = e_q$ for some $p,q \in \{1,2,3\}$.

    If $p = q$ then $i = k$ and $j = l$, so both sides equal $1$. If instead $p \neq q$, then the two cross products on the left-hand side are orthogonal, so the inner product is zero. Furthermore, either $k$ or $l$ equals $p$, so one of the rows in the right-hand side matrix is zero, and hence the determinant is zero.
\end{proof}
%
The identity \cref{eq:Lagrange-identity} is just Lagrange's identity in three dimensions. If $\theta$ is the angle between $v$ and $w$, then $\inner{v}{w} = \norm{v} \, \norm{w} \cos\theta$, so
%
\begin{equation*}
    \norm{v \crossp w}^2
        = \norm{v}^2 \norm{w}^2 - \inner{v}{w}^2
        = \norm{v}^2 \norm{w}^2 (1 - \cos^2 \theta)
        = \norm{v}^2 \norm{w}^2 \sin^2 \theta.
\end{equation*}
%
Hence $\norm{v \crossp w} = \norm{v} \, \norm{w} \, \abs{\sin \theta}$, which is the area of the parallelogram spanned by $v$ and $w$. If $u \in \reals^3$ is another vector and $\phi$ is the angle between $u$ and the normal of the plane spanned by $v$ and $w$ (e.g. $v \crossp w$), then
%
\begin{equation*}
    \abs[\big]{[u,v,w]}
        = \abs{\inner{u}{v \crossp w}}
        = \norm{u} \, \norm{v \crossp w} \, \abs{\cos\phi}
        = \norm{u} \, \norm{v} \, \norm{w} \, \abs{\sin\theta \cos\phi}.
\end{equation*}
%
But this is the volume of the parallelepiped spanned by $u$, $v$ and $w$. This gives a geometric interpretation (or \enquote{proof}) of the invariance of the scalar triple product.


\begin{corollary}
    \label{cor:vector-triple-product}
    Let $u,v,w \in \reals^3$. Then
    %
    \begin{equation}
        \label{eq:bac-cab}
        u \crossp (v \crossp w)
            = v\inner{u}{w} - w\inner{u}{v}.
    \end{equation}
    %
    In particular, the cross product satisfies the \keyword{Jacobi identity}
    %
    \begin{equation}
        \label{eq:cross-product-Jacobi}
        u \crossp (v \crossp w)
            + v \crossp (w \crossp u)
            + w \crossp (u \crossp v)
            = 0.
    \end{equation}
\end{corollary}
%
The identity \cref{eq:bac-cab} is sometimes called the \enquote{bac-cab rule}, a name that would have been self-explanatory had we used the names $a$, $b$ and $c$ instead of $u$, $v$ and $w$. Note that to conform to this rule we need to write the vectors before the scalars.

\begin{proof}
    For $x \in \reals^3$ we have
    %
    \begin{align*}
        \inner{x}{u \crossp (v \crossp w)}
            &= [x, u, v \crossp w] \\
            &= \inner{x \crossp u}{v \crossp w} \\
            &= \det \begin{pmatrix}
                \inner{x}{v} & \inner{u}{v} \\
                \inner{x}{w} & \inner{u}{w}
            \end{pmatrix} \\
            &= \inner{x}{v} \inner{u}{w} - \inner{u}{v} \inner{x}{w} \\
            &= \inner[\big]{x}{ v\inner{u}{w} - w\inner{u}{v} }.
    \end{align*}
    %
    The claim then follows since $x$ was arbitrary.
\end{proof}


\newpar

We now study how the cross product transforms under linear transformations. Since $\mat{d}{\reals}$ is a finite-dimensional vector space, it has a unique vector space topology. More concretely, all norms on $\mat{d}{\reals}$ are Lipschitz equivalent, so we may choose whatever norm we wish. We choose the Euclidean norm, identifying $\mat{d}{\reals}$ with $\reals^{d^2}$. First a lemma:

\begin{lemma}
    \label{lem:GL-density}
    $\matGL{d}{\reals}$ is dense in $\mat{d}{\reals}$.
\end{lemma}

\begin{proof}
    Let $A \in \mat{d}{\reals}$, and let $t \in \reals \setminus \{0\}$. Then $A - tI$ is invertible if and only if $\det(A - tI) = 0$, but $\det(A - tI)$ is a polynomial in $t$, so it has finitely many roots. Hence the nonzero roots of $\det(A - tI)$ are bounded away from zero, so since $A - tI \to A$ as $t \to 0$, the claim follows.
\end{proof}


\begin{proposition}[Transformation of cross products]
    Let $u,v,w \in \reals^3$, and let $A \in \mat{3}{\reals}$. Then we have the following:
    %
    \begin{enumprop}
        \item \label{enum:triple-product-transformation} $[Au, Av, Aw] = (\det A) [u,v,w]$.
        
        \item \label{enum:cross-product-transformation} $Av \crossp Aw = (\cof A)(v \crossp w) = (\adj A)\trans (v \crossp w)$.

        \item \label{enum:cross-product-orthogonal-transformation} If $A$ is orthogonal, then $A(v \crossp w) = (\det A)(Av \crossp Aw)$.
    \end{enumprop}
\end{proposition}

\begin{proof}
\begin{proofsec*}
    \item[\Namesubcref{enum:triple-product-transformation}]
    Simply notice that
    %
    \begin{equation*}
        [Au, Av, Aw]
            = \det(Au, Av, Aw)
            = (\det A) \det(u,v,w)
            = (\det A) \inner{u}{v \crossp w},
    \end{equation*}
    %
    where the second equality follows since $\det(Au, Av, Aw)$ is also the determinant of the matrix
    %
    \begin{equation*}
        \bigl( Au \mid Av \mid Aw \bigr)
            = A \bigl( u \mid v \mid w \bigr),
    \end{equation*}
    %
    and the determinant is multiplicative.

    \item[\Namesubcref{enum:cross-product-transformation}]
    First assume that $A$ is invertible. Then replacing $u$ with $A\inv u$ in \subcref{enum:triple-product-transformation} we obtain
    %
    \begin{align*}
        \inner{u}{Av \crossp Aw}
            &= (\det A) \inner{A\inv u}{v \crossp w} \\
            &= (\det A) \inner{u}{(A\inv)\trans (v \crossp w)} \\
            &=  \inner{u}{(\cof A)(v \crossp w)},
    \end{align*}
    %
    where the last equality follows from \cref{thm:adjoint-matrix-product}. Hence we obtain the desired identity when $A$ is invertible. Finally notice that both the maps $A \mapsto \cof A$ and $A \mapsto Av \crossp Aw$ are continuous. Hence the claim for general $A$ follows from \cref{lem:GL-density}.

    \item[\Namesubcref{enum:cross-product-orthogonal-transformation}]
    Notice that $A\inv = A\trans$, so this follows immediately from \subcref{enum:cross-product-transformation}.
\end{proofsec*}
\end{proof}
%
This gives a geometric interpretation of the determinant. If $[u,v,w]$ is the signed volume of the parallelepiped spanned by $u$, $v$ and $w$, and $[Au,Av,Aw]$ is the signed volume of the parallelepiped spanned by $Au$, $Av$ and $Aw$, then $\det A$ is the factor by which this volume increasing when applying $A$ to each of $u$, $v$ and $w$. In particular, this explains why the determinant of $A$ is zero if and only if $A$ is singular: This means that $A$ sends a basis of $\reals^3$ to a linearly dependent set, and the parallelepiped spanned by such a set has zero volume.


\newpar

If $A$ is a proper rotation, i.e. if $A$ is orthogonal and $\det A = 1$, then \cref{enum:cross-product-orthogonal-transformation} implies that $A(v \crossp w) = Av \crossp Aw$. This allows us to define a cross product on any three-dimensional inner product space, when this is equipped with an orientation.

First, if $\calV$ and $\calW$ are ordered bases for any finite-dimensional real vector space $V$, then we say that $\calV$ and $\calW$ have the \keyword{same orientation} if the change of basis operator $\basischange{\calW}{\calV}$ has positive determinant. It follows that orientation partitions the set of ordered bases for $V$ into two \keyword{orientation classes}, each called an \keyword{orientation} of $V$. If $V$ is equipped with an orientation $\calO$, then we call this class the \keyword{positive orientation} of $V$, and the other class the \keyword{negative orientation} of $V$. An ordered basis for $V$ is called \keyword{positive} if it lies in $\calO$ and \keyword{negative} if it does not.

Returning to the case where $V$ is three-dimensional and equipped with an orientation, let $\calV$ and $\calW$ be positive ordered orthonormal bases for $V$. For vectors $v,w \in V$ we can then consider the cross products of their coordinate vectors, i.e.
%
\begin{equation*}
    \coordvec{v}{\calV} \crossp \coordvec{w}{\calV}
    \quad \text{and} \quad
    \coordvec{v}{\calW} \crossp \coordvec{w}{\calW}.
\end{equation*}
%
Since $\basischangemat{\calW}{\calV}$ is orthogonal with determinant $1$, we have
%
\begin{equation*}
    \basischangemat{\calW}{\calV}(\coordvec{v}{\calV} \crossp \coordvec{w}{\calV})
        = \basischangemat{\calW}{\calV} \cdot \coordvec{v}{\calV} \crossp \basischangemat{\calW}{\calV} \cdot \coordvec{w}{\calV}
        = \coordvec{v}{\calW} \crossp \coordvec{w}{\calW}.
\end{equation*}
%
Hence we have
%
\begin{equation*}
    \coordmap{\calV}\inv(\coordvec{v}{\calV} \crossp \coordvec{w}{\calV})
        = \coordmap{\calW}\inv(\coordvec{v}{\calW} \crossp \coordvec{w}{\calW}),
\end{equation*}
%
so we may define the cross product of $v$ and $w$ as $v \crossp w = \coordmap{\calV}\inv(\coordvec{v}{\calV} \crossp \coordvec{w}{\calV})$ where $\calV$ is any positive ordered orthonormal basis for $V$. Notice that this means that $\coordvec{v \crossp w}{\calV} = \coordvec{v}{\calV} \crossp \coordvec{w}{\calV}$.

This allows us to generalise most of the above results to abstract real vector spaces. For instance, using that the coordinate map $\coordmap{\calV}$ is an isometry, the scalar triple product of $u,v,w \in V$ is given by
%
\begin{equation*}
    [u,v,w]
        = \inner{u}{v \crossp w}
        = \inner{\coordvec{u}{\calV}}{\coordvec{v \crossp w}{\calV}}
        = \inner{\coordvec{u}{\calV}}{\coordvec{v}{\calV} \crossp \coordvec{w}{\calV}}
        = \bigl[ \coordvec{u}{\calV}, \coordvec{v}{\calV}, \coordvec{w}{\calV} \bigr],
\end{equation*}
%
and hence it has all the properties of the scalar triple product on $\reals^3$, such as invariance under cyclic permutations. Notice also that it is indeed a \emph{scalar} quantity, in the sense that it is invariant under a change of basis. Similarly, the \enquote{bac-cab rule} \cref{eq:bac-cab} becomes
%
\begin{align*}
    \coordvec{u \crossp (v \crossp w)}{\calV}
        &= \coordvec{u}{\calV} \crossp \coordvec{v \crossp w}{\calV} \\
        &= \coordvec{u}{\calV} \crossp (\coordvec{v}{\calV} \crossp \coordvec{w}{\calV}) \\
        &= \coordvec{v}{\calV} \inner{\coordvec{u}{\calV}}{\coordvec{w}{\calV}} - \coordvec{w}{\calV} \inner{\coordvec{u}{\calV}}{\coordvec{v}{\calV}} \\
        &= \coordvec{v}{\calV} \inner{u}{w} - \coordvec{w}{\calV} \inner{u}{v} \\
        &= \coordvec{ v \inner{u}{w} - w \inner{u}{v} }{\calV}.
\end{align*}
%
Hence $u \crossp (v \crossp w) = v \inner{u}{w} - w \inner{u}{v}$ since $\coordmap{\calV}$ is an isomorphism. In particular, the cross product on $V$ also satisfies the Jacobi identity \cref{eq:cross-product-Jacobi}, so $V$ becomes a Lie algebra whose Lie bracket is given by the cross product, i.e. $[v,w] = v \crossp w$.


\chapter{Eigenvalues and eigenvectors}

\section{Eigenvalues and spectra}

\newcommand{\geo}{\mathrm{Geo}}

\newpar

Let $V$ be a vector space, and let $T \in \calL(V)$. Recall that an \keyword{eigenvalue} of $T$ is an element $\lambda \in \field$ such that there is a nonzero vector $v \in V$ with $Tv = \lambda v$. Then $v$ is called an \keyword{eigenvector} of $T$ associated with $\lambda$. The set of eigenvectors associated with an eigenvalue $\lambda$ is called the \keyword{eigenspace} of $\lambda$ and is denoted $E_T(\lambda)$. This is clearly a subspace of $V$, and its dimension is called the \keyword{geometric multiplicity} of $\lambda$ and is denoted $\geo_T(\lambda)$. The set of eigenvalues of $T$ is called the \keyword{spectrum} of $T$ and is denoted $\spec T$. Clearly $\lambda \in \spec T$ if and only if $\lambda I - T$ is not injective.

On finite-dimensional spaces being injective is the same as being invertible, but on infinite-dimensional vector spaces this is not the case, so the above definition of the spectrum is usually not sufficient. If $V$ and $W$ are Banach spaces over $\fieldK$ and $U$ is a subspace of $V$, then a (not necessarily bounded) linear map $T \colon U \to W$ is said to be \keyword{boundedly invertible} if there is a bounded operator $S \colon W \to V$ such that $TS = \id_W$ and $ST = \iota_U$. The \keyword{resolvent set} $\rho(T)$ of $T$ is the set of $\lambda \in \fieldK$ such that $\lambda I - T$ is boundedly invertible. The \keyword{spectrum} of $T$ is then the set $\sigma(T) = \fieldK \setminus \rho(T)$.

If $T$ is in fact bounded and $U = V$, then another definition of the spectrum of $T$ does not require that $\lambda I - T$ is not \emph{boundedly} invertible, but that it is not invertible at all.\footnote{This agrees with the definition of the spectrum of an element of a unital Banach algebra.} But the bounded inverse theorem (cf. e.g. \cite[Corollary~5.11]{follandrealanalysis}) says that if $T$ is a bounded and invertible linear map between Banach spaces, then its inverse is also bounded.

In this setting it is usual to collect the eigenvalues of $T$ in a set $\sigma_{\mathrm{p}}(T)$ called the \keyword{point spectrum} of $T$. This thus agrees with the definition of $\spec T$ above.


\newpar

Let $V$ be a vector space, and let $T \in \calL(V)$.

\begin{proposition}
    If $\calV$ be a collection of eigenvectors for $T$ associated with distinct eigenvalues. Then $\calV$ is linearly independent.
\end{proposition}

\begin{proof}
    Note that it suffices to show that any finite subset of $\calV$ is linearly independent. Let $\calI \subseteq \calV$ be a finite subset with $n$ elements. If $n = 1$, then since the sole element of $\calI$ is an eigenvector, it is nonzero and hence $\calI$ is linearly independent.

    Assume now that $n > 1$ and that any $n-1$ element subset of $\calV$ is linearly independent. Write $\calI = \{v_1, \ldots, v_n\}$ and consider a linear relation
    %
    \begin{equation}
        \label{eq:eigenvalues-linearly-independent-1}
        \alpha_1 v_1 + \cdots + \alpha_n v_n = 0.
    \end{equation}
    %
    Applying $T$ to both sides yields
    %
    \begin{equation}
        \label{eq:eigenvalues-linearly-independent-2}
        \alpha_1 \lambda_1 v_1 + \cdots + \alpha_n \lambda_n v_n = 0.
    \end{equation}
    %
    Multiplying \cref{eq:eigenvalues-linearly-independent-1} by $\lambda_1$ and subtracting it from \cref{eq:eigenvalues-linearly-independent-2} then gives
    %
    \begin{equation*}
        \alpha_2 (\lambda_2 - \lambda_1) v_2 + \cdots + \alpha_n (\lambda_n - \lambda_1) v_n = 0.
    \end{equation*}
    %
    But the set $\{v_2, \ldots, v_n\}$ is linearly independent, so the coefficients above must all vanish. And since the eigenvalues are distinct, this implies that $\alpha_2 = \cdots \alpha_n = 0$. Hence $\alpha_1 v_1 = 0$, so also $\alpha_1 = 0$.
\end{proof}


\begin{corollarynoproof}
    The direct sum
    %
    \begin{equation*}
        \bigoplus_{\lambda \in \spec T} E_T(\lambda)
    \end{equation*}
    %
    exists.
\end{corollarynoproof}


\newpar

We end this section with the following result on eigenvalues of matrix representations:

\begin{lemma}
    \label{lemma:mr-eigenvalues}
    Let $V$ be a finite-dimensional vector space, let $T \in \calL(V)$, and let $\calV$ be an ordered basis for $V$. Then $v \in V$ is an eigenvector for $T$ if and only if $\coordvec{v}{\calV}$ is an eigenvector for $\mr{\calV}{T}{\calV}$ with the same eigenvalue.
\end{lemma}

\begin{proof}
    Let $\lambda \in \field$ be the eigenvalue of $v$. Then
    %
    \begin{equation*}
        \mr{\calV}{T}{\calV} \cdot \coordvec{v}{\calV}
            = \coordvec{Tv}{\calV}
            = \coordvec{\lambda v}{\calV}
            = \lambda \coordvec{v}{\calV}.
    \end{equation*}
    %
    For the converse, a similar calculation shows that  $\coordvec{Tv}{\calV} = \coordvec{\lambda v}{\calV}$. Since $\coordmap{\calV}$ is an isomorphism, it follows that $Tv = \lambda v$ as desired.
\end{proof}



\section{The characteristic polynomial}

\newpar

If $V$ is finite-dimensional, then $\lambda$ is an eigenvalue of $T$ just when $\det(\lambda \id_V - T) = 0$. This motivates the definition of the \keyword{characteristic polynomial} $p_T(t) \in \field[t]$ of $T$, given by $p_T(t) = \det(t \id_V - T)$. The eigenvalues of $T$ are then precisely the roots of $p_T(t)$.

It is also common to define the characteristic polynomial of $T$ as the polynomial $\det(T - t \id_V)$ in $t$. Nothing substantial hangs on this choice, but our convention has the benefit that $p_T(t)$ becomes a monic polynomial, as the following result shows:

\begin{proposition}
    \label{prop:determinant-eigenvalues}
    Let $T \in \calL(V)$.
    %
    \begin{enumprop}
        \item \label{enum:characteristic-polynomial-monic} $p_T(t)$ is a monic polynomial of degree $n$.
        \item \label{enum:characteristic-polynomial-constant-term} The constant term of $p_T(t)$ equals $(-1)^n \det T$.
        \item \label{enum:characteristic-polynomial-coefficient} The coefficient of $t^{n-1}$ in $p_T(t)$ equals $-\trace T$.
    \end{enumprop}
    %
    Assume further that $p_T(t)$ splits over $\field$. Then:
    %
    \begin{enumprop}[resume]
        \item \label{enum:eigenvalue-existence} $T$ has an eigenvalue.
        \item \label{enum:eigenvalue-product} $\det T$ is the product of the eigenvalues of $T$.
        \item \label{enum:eigenvalue-sum} $\trace T$ is the sum of the eigenvalues of $T$.
    \end{enumprop}
\end{proposition}
%
The condition that $p_T(t)$ splits over $\field$ means that $p_T(t)$ decomposes into a product of linear factors on the form $t - a \in \field[t]$ (up to multiplication by a constant). This is in particular the case if $\field$ is algebraically closed.

\begin{proof}
\begin{proofsec*}
    \item[\Namesubcref{enum:characteristic-polynomial-monic}]
    Let $A = (a_{ij}) \in \mat{n}{\field}$ be a matrix representation of $T$. The $(i,j)$-th entry of $tI - A$ is then $t\delta_{ij} - a_{ij}$, so
    %
    \begin{equation}
        \label{eq:characteristic-polynomial-Leibniz}
        \det(t \id_V - T)
            = \sum_{\sigma \in S_n} (\sign\sigma) (t\delta_{1\sigma(1)} - a_{1\sigma(1)}) \cdots (t\delta_{n\sigma(n)} - a_{n \sigma(n)})
    \end{equation}
    %
    by \cref{thm:determinant-uniqueness}. Thus $p_T(t)$ is a polynomial in $t$. Furthermore, the only entries in $tI - A$ containing $t$ are the diagonal entries, and the largest number of such entries occurring in a single term of \cref{eq:characteristic-polynomial-Leibniz} is $n$, so $\deg p_T(t) \leq n$. But notice that there is only one term in which $t$ appears $n$ times, namely the term corresponding to the identity permutation in $S_n$, giving the product of the diagonal entries in $tI-A$. This term equals
    %
    \begin{equation}
        \label{eq:characteristic-polynomial-diagonal-product}
        (t-a_{11})(t-a_{22}) \cdots (t-a_{nn}),
    \end{equation}
    %
    and multiplying out we see that the only resulting term containing $t^n$ is $t^n$ itself. Hence $p_T(t)$ is monic and of degree $n$. Thus we may write $p_T(t) = \sum_{i=0}^n c_i t^i$ for appropriate $c_0, \ldots, c_n \in \field$.

    \item[\Namesubcref{enum:characteristic-polynomial-constant-term}]
    Simply notice that
    %
    \begin{equation*}
        (-1)^n \det T
            = \det(-T)
            = p_T(0)
            = c_0
    \end{equation*}
    %
    by $n$-linearity of $\det$ and the definition of $p_T(t)$.

    \item[\Namesubcref{enum:characteristic-polynomial-coefficient}]
    Consider a term in the sum \cref{eq:characteristic-polynomial-Leibniz}. The only way for this term to contain the factor $t^{n-1}$ is for at least $n-1$ of the $t\delta_{i\sigma(i)} - a_{i\sigma(i)}$ to be a diagonal element. But in choosing $n-1$ elements along the diagonal we are forced to also choose the final diagonal element, since otherwise $\sigma$ would not be a permutation. Thus $\sigma$ is forced to be the identity permutation, and in particular the only term in \cref{eq:characteristic-polynomial-Leibniz} that contains the factor $t^{n-1}$ is the diagonal term \cref{eq:characteristic-polynomial-diagonal-product}. Multiplying out the factors in this term, it is then clear that the coefficient of $t^{n-1}$ is
    %
    \begin{equation*}
        c_{n-1}
            = - (a_{11} + \cdots + a_{nn})
            = - \trace T
    \end{equation*}
    %
    as claimed.

    \item[\Namesubcref{enum:eigenvalue-existence}]
    Now assume that $p_T(t)$ splits over $\field$. Then some linear factor $t-\lambda \in \field[t]$ divides $p_T(t)$, which implies that $\lambda \in \field$ is an eigenvalue of $T$.
    
    \item[\Namesubcref{enum:eigenvalue-product}]
    Since $p_T(t)$ is monic we have
    %
    \begin{equation*}
        p_T(t)
            = (t - \lambda_1) (t - \lambda_2) \cdots (t - \lambda_n)
    \end{equation*}
    %
    for appropriate $\lambda_1, \ldots, \lambda_n \in \field$. These are then the (not necessarily distinct) eigenvalues of $T$. Thus $p_T(0) = (-1)^n \lambda_1 \cdots \lambda_n$, and the claim follows from \subcref{enum:characteristic-polynomial-constant-term}.

    \item[\Namesubcref{enum:eigenvalue-sum}]
    We similarly find that $c_{n-1} = -(\lambda_1 + \cdots + \lambda_n)$, so the final claim follows from \subcref{enum:characteristic-polynomial-coefficient}.
\end{proofsec*}
\end{proof}


\newpar

\newcommand{\alg}{\mathrm{Alg}}

Above we defined the geometric multiplicity of an eigenvalue. The characteristic polynomial gives rise to another kind of multiplicity: The \keyword{algebraic multiplicity} of $\lambda$ is the multiplicity of $\lambda$ as a root of the characteristic polynomial $p_T$. We denote this by $\alg_T(\lambda)$.

\begin{proposition}
    If $\lambda$ is an eigenvalue of $T$, then $\geo_T(\lambda) \leq \alg_T(\lambda)$.
\end{proposition}

\begin{proof}
    Let $d = \geo_T(\lambda)$. Choose any basis for the eigenspace $E_T(\lambda)$ and extend it to a basis $\calV$ for $V$. The corresponding matrix representation of $T$ then has the block form
    %
    \begin{equation*}
        \mr{\calV}{T}{\calV} =
        \begin{pmatrix}
            \lambda I_d & A \\
            0 & B
        \end{pmatrix},
    \end{equation*}
    %
    for appropriate matrices $A$ and $B$. It follows from \cref{prop:block-matrix-determinant} that
    %
    \begin{align*}
        p_T(t)
            &= \det(t \id_V - T) \\
            &= \det(t I_d - \lambda I_d) \det(t I_{n-d} - B) \\
            &= (t - \lambda)^d \det(t I_{n-d} - B).
    \end{align*}
    %
    This proves the claim.
\end{proof}


\section{Diagonalisability}

\newpar\label{par:diagonalisability}

If $V$ is finite-dimensional and $T \in \calL(V)$, then we say that $T$ is \keyword{diagonalisable} if there is a basis for $V$ consisting of eigenvectors for $T$. That is, $V$ has a basis $\calV = (v_1, \ldots, v_n)$ such that $Tv_i = \lambda_i v_i$ for appropriate $\lambda_i$. It is then obvious that

\begin{propositionnoproof}
    \label{prop:diagonalisability-equivalent-properties}
    Let $T \in \calL(V)$. The following are equivalent:
    %
    \begin{enumerate}
        \item $T$ is diagonalisable.
        
        \item $V$ has an ordered basis $\calV$ such that $\mr{\calV}{T}{\calV}$ is diagonal.
        
        \item $V$ has the form
        %
        \begin{equation*}
            V
                = \bigoplus_{\lambda \in \spec T} E_T(\lambda).
        \end{equation*}
        
        \item If $\spec T = \{\lambda_1, \ldots, \lambda_k\}$ and $P_i$ is projection onto $E_T(\lambda_i)$ along $\bigoplus_{j \neq i} E_T(\lambda_j)$, then $\id_V = P_1 + \cdots + P_k$ is a resolution of the identity.
        
        \item $T$ is similar to a multiplication operator $M_A$, where $A \in \mat{n}{\field}$ is a diagonal matrix whose diagonal contains the eigenvalues of $T$ with multiplicity:
        %
        \begin{equation*}
            T
                = \coordmap{\calV}\inv \circ M_A \circ \coordmap{\calV}.
        \end{equation*}
    \end{enumerate}
\end{propositionnoproof}
%
Note that the last two properties are equivalent by \cref{prop:resolution-of-the-identity-characterisation}.


\newpar

There is a different way of characterising diagonalisability using resolutions of the identity. If $T \in \calL(V)$, then a \keyword{spectral resolution} of $T$ is a decomposition
%
\begin{equation*}
    T
        = \lambda_1 P_1 + \cdots + \lambda_k P_k,
\end{equation*}
%
where $\id_V = P_1 + \cdots + P_k$ is a resolution of the identity and $\lambda_1, \ldots, \lambda_k \in \field$. Note that any resolution of the identity is itself a spectral resolution of $\id_V$ with all coefficients equal to $1$. We then have the following result, which follows from \cref{prop:diagonalisability-equivalent-properties} and \cref{prop:projection-characterisation}:

\begin{propositionnoproof}
    A linear operator $T \in \calL(V)$ is diagonalisable if and only if it has a spectral resolution
    %
    \begin{equation*}
        T
            = \lambda_1 P_1 + \cdots + \lambda_k P_k.
    \end{equation*}
    %
    In this case $\spec T = \{\lambda_1, \ldots, \lambda_k\}$, and
    %
    \begin{equation*}
        \im P_i = E_T(\lambda_i),
        \quad \text{and} \quad
        \ker P_i = \bigoplus_{j \neq i} E_T(\lambda_j).
    \end{equation*}
\end{propositionnoproof}



\section{Proofs without determinants}

We now show how to obtain the results in \cref{prop:determinant-eigenvalues} without using determinants. Since we do not have access to the characteristic polynomial, we must assume that $V$ is a (finite-dimensional) vector space over an algebraically closed field $\field$. Consider $T \in \calL(V)$.


\newpar

We begin by showing that $T$ has an eigenvalue. For $d \in \naturals$, let $\field[t]_d$ denote the vector space of polynomials in $\field[t]$ with degree strictly less than $d$, such that $\dim \field[t]_d = d$. Consider the linear map $\ev_T \colon \field[t]_{n^2+1} \to \calL(V)$ given by $\ev_T(p) = p(T)$. This cannot be injective, so there is some nonzero $p(t) \in \field[t]_{n^2+1}$ such that $p(T) = 0$. Note that $p(t)$ cannot be constant.

Since $\field$ is algebraically closed, there exist $c, \lambda_1, \ldots, \lambda_m \in \field$ such that $p(t) = c \bigprod_{i=1}^m (t - \lambda_i)$. But then
%
\begin{equation*}
    0
        = p(T)
        = c \bigprod_{i=1}^m (T - \lambda_i I),
\end{equation*}
%
so at least one $T - \lambda_i I$ is not injective. Hence $\lambda_i$ is an eigenvalue of $T$.


\newpar

And for the remaining of the promised results:

\begin{corollary}
    Let $\field$ be algebraically closed, and let $T \in \calL(V)$. Then the sum of the eigenvalues of $T$ is $\trace T$, and the product of the eigenvalues of $T$ is $\det T$.
\end{corollary}

\begin{proof}
    Let $A \in \mat{n}{\field}$ be an upper triangular matrix for $T$. As we will see in \cref{prop:upper-triangular-basis-exists}, such a matrix always exists. The diagonal elements of $A$ are then the eigenvalues, and the trace of $T$ is of course the sum of these elements.

    For the second claim, simply notice that if $A$ is upper triangular then the Leibniz formula for $\det A$ only contains a single term, namely the one corresponding to the identity permutation.
\end{proof}






\chapter{Complexification}



\newpar

If $W$ is a complex vector space, then we may restrict the scalar multiplication $\complex \prod W \to W$ to a map $\reals \prod W \to W$. When we equip $W$ with this restricted scalar multiplication instead of the original one, we call the resulting space the \keyword{real version} of $W$ and denote it by $W_\reals$.

Conversely, if $V$ is a real vector space then we define the \keyword{complexification} of $V$ as the vector space $V^\complex$ whose underlying set is $V \prod V$, and which is equipped with componentwise addition and the complex scalar multiplication
%
\begin{equation*}
    (\alpha + \iu \beta)(v,u)
        = (\alpha v - \beta u, \alpha u + \beta v),
\end{equation*}
%
for $\alpha,\beta \in \reals$ and $v,u \in V$. Notice that the map $v \mapsto (v,0)$ is injective (and real linear), that $(v,0) + (w,0) = (v+w,0)$, and that $\alpha(v,0) = (\alpha v,0)$ for $\alpha \in \reals$, so $V^\complex$ contains an isomorphic copy of $V$, and we may identify elements $v \in V$ with elements $(v,0) \in V^\complex$. Furthermore, notice that $(v,u) = (v,0) + \iu (u,0)$, so by the above identification we may write $(v,u) = v + \iu u$.


\newpar

We briefly study the relationship between a real vector space and its complexification.

\begin{proposition}
    If $\calB$ is a basis for $V$, then $\calB^\complex = \set{b + \iu 0}{b \in \calB}$ is a basis for $V^\complex$. In particular, $\dim_\reals V = \dim_\complex V^\complex$.
\end{proposition}

\begin{proof}
    Let $v + \iu u \in V^\complex$. Then there are real numbers $\alpha_b$ and $\beta_b$ (finitely many nonzero) such that $v = \sum_{b \in \calB} \alpha_b b$ and $u = \sum_{b \in \calB} \beta_b b$. But then
    %
    \begin{equation*}
        v + \iu u
            = \sum_{b \in \calB} \alpha_b b
                + \iu \sum_{b \in \calB} \beta_b b
            = \sum_{b \in \calB} (\alpha_b + \iu \beta_b) b
            = \sum_{b \in \calB} (\alpha_b + \iu \beta_b) (b + \iu 0),
    \end{equation*}
    %
    so $\calB^\complex$ spans $V^\complex$. Furthermore, if $v + \iu u = 0$, then the previous computation shows that $\sum_{b \in \calB} \alpha_b b = 0 = \sum_{b \in \calB} \beta_b b$. Linear independence of $\calB$ then implies that $\alpha_b = \beta_b = 0$ for all $b \in \calB$.
\end{proof}

\begin{example}
    Notice that $(\reals^n)^\complex \cong \complex^n$. The above proposition then implies that the standard basis for $\reals^n$ gives rise to a basis for $\complex^n$, and we notice that this is precisely the standard basis.
\end{example}


\newpar

We now show how to extend linear maps defined between real vector spaces to the complexifications of those spaces. If $T \colon V \to W$ is a linear map between real vector spaces, then we define the complexification of $T$ by
%
\begin{align*}
    T^\complex \colon V^\complex &\to W^\complex, \\
    v + \iu u &\mapsto Tv + \iu Tu.
\end{align*}
%
That is, $T^\complex$ is just the product map $T \prod T$. This is easily seen to be complex-linear.

\begin{proposition}
    \label{prop:complexification-eigenvalue}
    Let $V$ be a real vector space, and let $T \in \calL(V)$. If $\lambda \in \reals$ is an eigenvalue of the complexification $T^\complex$ of $T$, then $\lambda$ is also an eigenvalue of $T$. Furthermore, if $v + \iu u \in E_{T^\complex}(\lambda)$ then $v,u \in E_T(\lambda)$.
\end{proposition}
%
Note that this does not mean that $v$ and $u$ are eigenvectors of $T$ since they might be zero. But if $v + \iu u$ is an eigenvector of $T^\complex$, then at least one of $v$ and $u$ is nonzero and hence an eigenvector of $T$.

\begin{proof}
    Let $v + \iu u \in V^\complex$ be an eigenvector of $T^\complex$ corresponding to $\lambda$. Then
    %
    \begin{equation*}
        Tv + \iu Tu
            = T^\complex (v + \iu u)
            = \lambda(v + \iu u)
            = \lambda v + \iu \lambda u.
    \end{equation*}
    %
    It follows that $Tv = \lambda v$ and $Tu = \lambda u$ as desired.
\end{proof}


If $V$ is finite-dimensional and $\calV$ is an ordered basis for $V$, then $\calV^\complex$ carries the obvious ordering. Since $V$ and $V^\complex$ have the same dimension, the following result is not surprising:

\begin{proposition}
    Let $V$ and $W$ be a finite-dimensional real vector spaces, and consider $T \colon V \to W$. If $\calV = (v_1, \ldots, v_n)$ and $\calW$ are ordered bases of $V$ and $W$ respectively, then
    %
    \begin{equation*}
        \mr{\calW^\complex}{T^\complex}{\calV^\complex}
            = \mr{\calW}{T}{\calV}.
    \end{equation*}
\end{proposition}

\begin{proof}
    By \cref{enum:mr-explicit-formula}, the $i$th column of $\mr{\calW^\complex}{T^\complex}{\calV^\complex}$ is given by
    %
    \begin{equation*}
        \coordvec{T^\complex (v_i + \iu 0)}{\calW^\complex}
            = \coordvec{Tv_i + \iu 0}{\calW^\complex}
            = \coordvec{Tv_i}{\calW},
    \end{equation*}
    %
    that is, the $i$th column of $\mr{\calW}{T}{\calV}$, which proves the claim.
\end{proof}


\newpar

Finally, if $V$ is a real normed space, then we define a norm on $V^\complex$ by the equation
%
\begin{equation}
    \label{eq:complexification-norm}
    \norm{v + \iu u}^2
        = \norm{v}^2 + \norm{u}^2.
\end{equation}
%
Furthermore, if $V$ is an inner product space, then we define an inner product on $V^\complex$ by
%
\begin{equation}
    \label{eq:complexification-inner-product}
    \inner{v + \iu u}{x + \iu y}
        = \inner{v}{x}
          + \inner{u}{y}
          + \iu (\inner{v}{y} - \inner{u}{x}).
\end{equation}
%
The norm induced by this inner product agrees with the norm defined by \cref{eq:complexification-norm}. Notice that the identity \cref{eq:complexification-inner-product} holds in any \emph{complex} inner product space, where the notation $v + \iu u$ instead means the sum of $v$ and the scalar product of $\iu$ and $u$ (recalling that our sesquilinear forms are linear in the \emph{second} entry).




\chapter{Triangularisation}

Recall that a matrix $A = (a_{ij}) \in \mat{n}{R}$ is called \emph{upper triangular} if $a_{ij} = 0$ whenever $i > j$. If $V$ is an $n$-dimensional $\field$-vector space and $\calV$ is an ordered basis for $V$, then we say that the operator $T \in \calL(V)$ is \emph{upper triangular with respect to $\calV$} if the matrix representation $\mr{\calV}{T}{\calV}$ is upper triangular.

A subspace $U$ of a vector space $V$ is said to be \emph{invariant under $T \in \calL(T)$} if $T(U) \subseteq U$.

\begin{proposition}
    \label{prop:upper-triangular-criterion}
    Let $V$ be an $\field$-vector space with $n = \dim V < \infty$, and let $\calV = (v_1, \ldots, v_n)$ be an ordered basis for $V$. An operator $T \in \calL(V)$ is upper triangular with respect to $\calV$ if and only if $\Span(v_1, \ldots, v_i)$ is invariant under $T$ for all $i \in \{1, \ldots, n\}$.
\end{proposition}

\begin{proof}
    This is obvious.
\end{proof}


\begin{lemma}
    Let $V$ be an $\field$-vector space, and let $T \in \calL(V)$ be an isomorphism. If $U$ is a finite-dimensional subspace of $V$ that is invariant under $T$, then $U$ is also invariant under $T\inv$.
\end{lemma}

\begin{proof}
    Since $U$ is finite-dimensional and $T|_U \colon U \to U$ is injective, applying the rank--nullity theorem implies that $T|_U$ is also surjective. Hence if $u \in U$, then there exists a $v \in U$ such that $Tv = u$. It follows that
    %
    \begin{equation*}
        T\inv u
            = T\inv Tv
            = v
            \in U,
    \end{equation*}
    %
    so $U$ is invariant under $T\inv$.
\end{proof}


\begin{proposition}
    Let $V$ be a finite-dimensional $\field$-vector space, and let $\calV$ be an ordered basis for $V$. If $T \in \calL(V)$ is an isomorphism that is upper triangular with respect to $\calV$, then $T\inv$ is also upper triangular with respect to $\calV$.

    In particular, the subset of $\matGL{n}{\field}$ consisting of upper triangular matrices is a subgroup.
\end{proposition}

\begin{proof}
    This is an obvious consequence of the above two results.
\end{proof}


\begin{lemma}
    \label{lem:upper-triangular-invertible}
    Let $A \in \mat{n}{\field}$ be upper triangular. Then $A$ is invertible if and only if all its diagonal elements are nonzero.
\end{lemma}

\begin{proof}
    Denote the diagonal elements of $A$ by $\lambda_1, \ldots, \lambda_n$, and let $(e_1, \ldots, e_n)$ be the standard basis of $\field^n$. First assume that the diagonal elements are nonzero. Then notice that $e_1 \in R(A)$, and that
    %
    \begin{equation*}
        A e_i
            = a_1 e_1 + \cdots + a_{i-1} e_{i-1} + \lambda_i e_i
    \end{equation*}
    %
    for appropriate $a_1, \ldots, a_{i-1} \in \field$. By induction we then have $e_i \in R(A)$. Since $(e_1, \ldots, e_n)$ is a basis, this implies that $R(A) = \field^n$.

    Conversely, assume that some diagonal element $\lambda_i$ is zero. Then
    %
    \begin{equation*}
        A \Span(e_1, \ldots, e_i)
            \subseteq \Span(e_1, \ldots, e_{i-1}),
    \end{equation*}
    %
    so the null-space of $A$ is nontrivial, and hence $A$ is singular.
\end{proof}


\begin{lemma}
    Let $A \in \mat{n}{\field}$ be upper triangular. Then the eigenvalues of $A$ are its diagonal elements.
\end{lemma}

\begin{proof}
    Let $\lambda \in \field$, and denote the diagonal elements of $A$ by $\lambda_1, \ldots, \lambda_n$. By \cref{lem:upper-triangular-invertible}, the matrix $\lambda I - A$ is singular if and only if $\lambda - \lambda_i = 0$ for some $i$, and hence $\lambda_1, \ldots, \lambda_n$ are the eigenvalues of $A$.
\end{proof}


\begin{proposition}
    \label{prop:upper-triangular-basis-exists}
    Let $\field$ be algebraically closed, and let $V$ be a finite-dimensional $\field$-vector space. If $T \in \calL(V)$, then $V$ has an ordered basis with respect to which $T$ is upper triangular.
\end{proposition}

\begin{proof}
    This is obvious if $\dim V = 1$, so assume that $n = \dim V > 1$, and assume that the claim is true for $\field$-vector spaces of dimension $n-1$. Since $\field$ is algebraically closed, $T$ has an eigenvector $v_1 \in V$. Then $U = \Span(v_1)$ is invariant under $T$, so we may define a linear operator\footnote{The operator $\tilde T$ may arise as follows: Let $\pi \colon V \to V/U$ be the quotient map. Then $U \subseteq \ker (\pi \circ T)$ since $U$ is invariant under $T$, so $\pi \circ T$ descends to a linear map $\tilde T \colon V/U \to V/U$.} $\tilde T \in \calL(V/U)$ by $\tilde T(v + U) = Tv + U$. Since $\dim V/U = n-1$, by induction there is a basis $v_2 + U, \ldots, v_n + U$ of $V/U$ with respect to which the matrix of $\tilde T$ is upper triangular. It is easy to show that the collection $v_1, \ldots, v_n$ is linearly independent, hence a basis for $V$.

    Now notice that
    %
    \begin{equation*}
        Tv_i + U
            = \tilde T(v_i + U)
            \in \Span(v_2 + U, \ldots, v_i + U)
    \end{equation*}
    %
    for $i \in \{2, \ldots, n\}$. That is, there exist $a_2, \ldots, a_i \in \field$ such that
    %
    \begin{equation*}
        Tv_i + U
            = (a_2 v_2 + \cdots + a_i v_i) + U.
    \end{equation*}
    %
    But then $Tv_i \in \Span(v_1, \ldots, v_i)$ for all $i \in \{2, \ldots, n\}$, and since $U$ is invariant under $T$ this also holds for $i = 1$. Hence $T$ is upper triangular with respect to the basis $v_1, \ldots, v_n$ of $V$.
\end{proof}


\begin{theorem}[Schur's Theorem]
    Let $V$ be a finite-dimensional complex inner product space. If $T \in \calL(V)$, then $V$ has an ordered orthonormal basis with respect to which $T$ is upper triangular.
\end{theorem}

\begin{proof}
    By \cref{prop:upper-triangular-basis-exists} $V$ has an ordered basis $\calV = (v_1, \ldots, v_n)$ with respect to which $\mr{\calV}{T}{\calV}$ is upper triangular. Now apply the Gram--Schmidt procedure to $\calV$ and obtain an orthonormal basis $\calU = (u_1, \ldots, u_n)$ for $V$ such that
    %
    \begin{equation*}
        \Span(u_1, \ldots, u_i)
            = \Span(v_1, \ldots, v_i)
    \end{equation*}
    %
    for all $i \in \{1, \ldots, n\}$. Then \cref{prop:upper-triangular-criterion} shows that $\mr{\calU}{T}{\calU}$ is also upper triangular, proving the claim.
\end{proof}


% \chapter{Orthonormal diagonalisation}

% \section{Hilbert space adjoints}\label{mylabel2}

% \newpar\label{mylabel}

% If $V$ is a (real or complex) Hilbert space, for $u \in V$ let $\phi_u$ denote the element in the continuous dual $V^*$ given by $\phi_u(v) = \inner{u}{v}$. Further, let $\Phi_V \colon V \to V^*$ denote the (conjugate-)linear isomorphism $u \mapsto \phi_u$. Then
% %
% \begin{equation*}
%     \abs{\phi_u(v)}
%         = \abs{\inner{u}{v}}
%         \leq \norm{u} \, \norm{v},
% \end{equation*}
% %
% implying that $\norm{\phi_u} \leq \norm{u}$. Furthermore,
% %
% \begin{equation*}
%     \abs{\phi_u(u)}
%         = \norm{u}^2,
% \end{equation*}
% %
% so in fact $\norm{\phi_u} = \norm{u}$. In other words, $\Phi_V$ is an isometry and in particular continuous.

% \begin{definition}[Hilbert space adjoints]
%     Let $V$ and $W$ be Hilbert spaces, and let $T \in \calB(V,W)$. The \emph{(Hilbert space) adjoint} of $T$ is the operator $T^* \colon W \to V$ given by
%     %
%     \begin{equation*}
%         T^*
%             = \Phi_V\inv \circ T^\dagger \circ \Phi_W.
%     \end{equation*}
% \end{definition}
% %
% Properties of the operator adjoint $T^\dagger$ are often inherited by the Hilbert space adjoint: By \cref{enum:operator-adjoint-continuous-normed} $T^\dagger$ is continuous, so $T^*$ is also continuous. And $\Phi_V$ and $\Phi_W$ are \emph{conjugate}-linear, so $T^*$ is linear. Furthermore, if $S \in \calB(W,U)$ then
% %
% \begin{align*}
%     (ST)^*
%         &= \Phi_V\inv \circ (ST)^\dagger \circ \Phi_U \\
%         &= \Phi_V\inv \circ T^\dagger \circ S^\dagger \circ \Phi_U \\
%         &= (\Phi_V\inv \circ T^\dagger \circ \Phi_W) \circ (\Phi_W\inv \circ S^\dagger \circ \Phi_U) \\
%         &= T^* S^*.
% \end{align*}
% %
% Finally, notice that
% %
% \begin{equation*}
%     \norm{T^*}
%         \leq \norm{\Phi_V\inv} \, \norm{T^\dagger} \, \norm{\Phi_W}
%         = \norm{T^\dagger},
% \end{equation*}
% %
% since $\Phi_V$ and $\Phi_W$ are isometric isomorphisms. The opposite inequality follows similarly, so in total $\norm{T^*} = \norm{T^\dagger} = \norm{T}$ by \cref{enum:operator-adjoint-continuous-normed}.

% \begin{proposition}
%     \label{prop:adjoint-inner-product}
%     Let $V,W$ be Hilbert spaces, and let $T \in \calB(V,W)$. For all $w \in W$ we have $T^\dagger \phi_w = \phi_{T^* w}$. In particular, $T^*$ is the unique linear operator $W \to V$ with the property that
%     %
%     \begin{equation*}
%         \inner{v}{T^*w}_V = \inner{Tv}{w}_W,
%     \end{equation*}
%     %
%     for all $v \in V$ and $w \in W$. Furthermore, $T^{**} = T$, i.e. the map $T \mapsto T^*$ is an involution.
% \end{proposition}
% %
% The adjoint $T^*$ is equivalently characterised by the identity
% %
% \begin{equation*}
%     \inner{T^*w}{v}_V = \inner{w}{Tv}_W,
% \end{equation*}
% %
% by complex conjugation. Note that either of these two identities is often taken as the definition of $T^*$, and existence is proved without appealing to the operator adjoint. In this case, many of the above properties are proved using the uniqueness part of this proposition.

% \begin{proof}
%     First notice that $T^*$ indeed has this property. For $w \in W$ we have
%     %
%     \begin{equation*}
%         \phi_{T^* w}
%             = \Phi_V (T^* w)
%             = (T^\dagger \circ \Phi_W)(w)
%             = T^\dagger \phi_w,
%     \end{equation*}
%     %
%     so for $v \in V$ it thus follows that
%     %
%     \begin{equation*}
%         \inner{v}{T^* w}_V
%             = \phi_{T^* w}(v)
%             = T^\dagger \phi_w (v)
%             = \phi_w(Tv)
%             = \inner{Tv}{w}_W,
%     \end{equation*}
%     %
%     as desired. Furthermore, if $S \colon W \to V$ is another such operator, then $\inner{v}{Sw}_V = \inner{v}{T^*w}_V$ for all $v$ and $w$, so $S = T^*$. The final claim that $T^{**} = T$ follows by uniqueness.
% \end{proof}


% \newpar

% An operator $U \colon V \to W$ is an \emph{isometry} if
% %
% \begin{equation*}
%     \inner{Uv}{Uu}_W
%         = \inner{v}{u}_V
% \end{equation*}
% %
% for all $v,u \in V$. Clearly $U$ is injective. If $U$ is also surjective (e.g. if $\dim V = \dim W < \infty$), then it is called \emph{unitary}. Notice that if $U$ is an isometry, then
% %
% \begin{equation*}
%     \inner{U^*Uv}{u}_V
%         = \inner{Uv}{Uu}_W
%         = \inner{v}{u}_V,
% \end{equation*}
% %
% implying that $U^*U = \id_V$, and the converse clearly also holds. If $U$ is also surjective, then it is an isomorphism and so also $UU^* = \id_W$ (an operator with this property is called a \emph{coisometry}). In this case $U^* = U\inv$.

% In the case $W = V$ we say that $T$ is \emph{normal} if $TT^* = T^*T$, and that $T$ is \emph{self-adjoint} if $T^* = T$. Clearly both self-adjoint and unitary operators (with $V = W$) are normal.


% \section{Properties of adjoints}

% \begin{proposition}
%     \label{prop:complexification-adjoint}
%     Let $V$ and $W$ be real Hilbert spaces, and let $T \in \calB(V,W)$. Then we have
%     %
%     \begin{equation*}
%         (T^\complex)^*
%             = (T^*)^\complex,
%     \end{equation*}
%     %
%     i.e., the adjoint of the complexification of $T$ is the complexification of the adjoint of $T$. In particular
%     %
%     \begin{enumprop}
%         \item $T$ is normal if and only if $T^\complex$ is normal, and
%         \item $T$ is self-adjoint if and only if $T^\complex$ is self-adjoint.
%     \end{enumprop}
% \end{proposition}

% \begin{proof}
%     For $v,u,x,y \in V$ we have
%     %
%     \begin{align*}
%         \inner{(T^*)^\complex(x + \iu y)}{v + \iu u}
%             &= \inner{ T^*x + \iu T^*y }{v + \iu u} \\
%             &= \inner{T^*x}{v}
%                 + \inner{T^*y}{u}
%                 + \iu ( \inner{T^*x}{u} - \inner{T^*y}{v} ) \\
%             &= \inner{x}{Tv}
%                 + \inner{y}{Tu}
%                 + \iu ( \inner{x}{Tu} - \inner{y}{Tv} ) \\
%             &= \inner{x + \iu y}{Tv + \iu Tu} \\
%             &= \inner{x + \iu y}{T^\complex(v + \iu u)}.
%     \end{align*}
%     %
%     Uniqueness of adjoints thus yields the claim.

%     Assume that $T$ is normal. Then
%     %
%     \begin{equation*}
%         T^\complex (T^\complex)^*
%             = T^\complex (T^*)^\complex
%             = (TT^*)^\complex
%             = (T^*T)^\complex
%             = (T^*)^\complex T^\complex
%             = (T^\complex)^* T^\complex,
%     \end{equation*}
%     %
%     so $T^\complex$ is normal. The converse follows similarly. If $T$ is self-adjoint, then
%     %
%     \begin{equation*}
%         (T^\complex)^*
%             = (T^*)^\complex
%             = T^\complex,
%     \end{equation*}
%     %
%     and similarly if $T^\complex$ is self-adjoint.
% \end{proof}


% \begin{proposition}
%     Let $V$ be a finite-dimensional inner product space, and let $T \in \calL(V)$ and $\lambda \in \fieldK$. Then $\lambda \id_V - T$ is invertible if and only if $\conj{\lambda} \id_V - T^*$ is invertible. In other words, $\lambda$ is an eigenvalue of $T$ if and only if $\conj{\lambda}$ is an eigenvalue of $T^*$.
% \end{proposition} % TODO: Infinite dimension? Boundedly invertible?

% \begin{proof}
%     Since the map $T \mapsto T^*$ is an involution it suffices to prove one implication, so assume that $\lambda \id_V - T$ is invertible. Then there exists an $S \in \calL(V)$ such that
%     %
%     \begin{equation*}
%         S(\lambda \id_V - T)
%             = (\lambda \id_V - T)S
%             = \id_V,
%     \end{equation*}
%     %
%     and taking adjoints we find that
%     %
%     \begin{equation*}
%         (\conj{\lambda} \id_V - T^*)S^*
%             = S^*(\conj{\lambda} \id_V - T^*)
%             = \id_V.
%     \end{equation*}
%     %
%     That is, $\conj{\lambda} \id_V - T^*$ is invertible as claimed.
% \end{proof}

% \begin{remark}
%     Note that this does \emph{not} say that $v \in V$ is an eigenvector of $T^*$ if it is an eigenvector of $T$. A counterexample is given by the matrix
%     %
%     \begin{equation*}
%         A =
%         \begin{pmatrix}
%             1 & 1 \\
%             0 & 0
%         \end{pmatrix},
%     \end{equation*}
%     %
%     which has the eigenvector $(1,0)$ with eigenvalue $1$. However, while $1$ is also an eigenvalue of the transpose $A\trans$ (with eigenvector $(1,1)$), $(1,0)$ is not an eigenvector of $A\trans$.

%     While this does not hold in general, recall that in \cref{enum:normal-adjoint-eigenvalues} we saw that it holds for \emph{normal} operators.
% \end{remark}


% \begin{proposition}
%     Let $T \in \calB(V)$ be a normal operator.
%     %
%     \begin{enumprop}
%         \item \label{enum:normal-adjoint-norm} $\norm{Tv} = \norm{T^*v}$ for all $v \in V$.
        
%         \item \label{enum:normal-adjoint-eigenvalues} If $\lambda \in \fieldK$ is an eigenvalue of $T$, then $\conj{\lambda}$ is an eigenvalue of $T^*$ with the same eigenvectors. In other words, $E_T(\lambda) = E_{T^*}(\conj{\lambda})$.

%         \item \label{enum:normal-orthogonal-eigenspaces} If $\mu \in \fieldK$ is another eigenvalue of $T$ distinct from $\lambda$, then $E_T(\lambda)$ and $E_T(\mu)$ are orthogonal.

%         \item \label{enum:self-adjoint-eigenvalues-exists-and-real} If $T$ is self-adjoint, then it has an eigenvalue and all its eigenvalues are real.

%         \item \label{enum:unitary-eigenvalues-unit-circle} If $T$ is unitary, then all its eigenvalues lie on the unit circle $\sphere^1 \subseteq \complex$.
%     \end{enumprop}
% \end{proposition}
% %
% In \cref{cor:self-adjoint-unitary-eigenvalue-characterisation} we will prove the converses of \subcref{enum:self-adjoint-eigenvalues-exists-and-real} and \subcref{enum:unitary-eigenvalues-unit-circle} under the assumption that $V$ is finite-dimensional and that $T$ is normal, using the spectral theorem (cf. \cref{thm:spectral-theorem}). We will use \subcref{enum:self-adjoint-eigenvalues-exists-and-real} in the proof of the spectral theorem, and we have proved \subcref{enum:unitary-eigenvalues-unit-circle} already to make explicit that it does not depend on the spectral theorem, and that the proof does not require that $V$ is finite-dimensional.

% \begin{proof}
% \begin{proofsec*}
%     \item[\Namesubcref{enum:normal-adjoint-norm}]
%     Notice that
%     %
%     \begin{equation*}
%         \norm{Tv}^2
%             = \inner{Tv}{Tv}
%             = \inner{T^*Tv}{v}
%             = \inner{TT^*v}{v}
%             = \inner{T^*v}{T^*v}
%             = \norm{T^*v}^2.
%     \end{equation*}

%     \item[\Namesubcref{enum:normal-adjoint-eigenvalues}]
%     If $T$ is normal then so is $\lambda \id_V - T$, so \subcref{enum:normal-adjoint-norm} implies that
%     %
%     \begin{equation*}
%         \norm{(\lambda \id_V - T)v}
%             = \norm{(\conj{\lambda} \id_V - T^*)v},
%     \end{equation*}
%     %
%     so $v \in V$ is an eigenvector for $T$ with eigenvalue $\lambda$ if and only if $v$ is an eigenvector for $T^*$ with eigenvalue $\conj{\lambda}$.

%     \item[\Namesubcref{enum:normal-orthogonal-eigenspaces}]
%     Let $v \in E_T(\lambda)$ and $u \in E_T(\mu)$. Since $w$ is also an eigenvector for $T^*$ with eigenvalue $\conj{\mu}$, we have
%     %
%     \begin{equation*}
%         \lambda \inner{v}{u}
%             = \inner{v}{Tu}
%             = \inner{T^*v}{u}
%             = \mu \inner{v}{u}.
%     \end{equation*}
%     %
%     Since $\lambda \neq \mu$ we must have $\inner{v}{u} = 0$ as claimed.

%     \item[\Namesubcref{enum:self-adjoint-eigenvalues-exists-and-real}]
%     If $T$ is self-adjoint and $v \in V$ is an eigenvector for $T$ with $\lambda \in \fieldK$, then
%     %
%     \begin{equation*}
%         \lambda \inner{v}{v}
%             = \inner{v}{Tv}
%             = \inner{Tv}{v}
%             = \conj{\lambda} \inner{v}{v},
%     \end{equation*}
%     %
%     and since $v \neq 0$ we must have $\lambda = \conj{\lambda}$. Hence $\lambda$ is real.

%     If $\fieldK = \complex$ then $V$ has a complex eigenvalue, which is real by the above argument. Assume instead that $\fieldK = \reals$ and consider the complexification $T^\complex$ of $T$. This is self-adjoint by \cref{prop:complexification-adjoint}, so it has a real eigenvalue by the above. But then \cref{prop:complexification-eigenvalue} implies that this also is an eigenvalue of $T$.

%     \item[\Namesubcref{enum:unitary-eigenvalues-unit-circle}]
%     Let $\lambda \in \fieldK$ be an eigenvalue of $T$ with eigenvector $v$. Then
%     %
%     \begin{equation*}
%         \inner{v}{v}
%             = \inner{Tv}{Tv}
%             = \inner{\lambda v}{\lambda v}
%             = \conj{\lambda} \lambda \inner{v}{v}
%             = \abs{\lambda}^2 \inner{v}{v},
%     \end{equation*}
%     %
%     so $\abs{\lambda} = 1$.
% \end{proofsec*}
% \end{proof}


% \section{Adjoints and coordinates}

% Now let $V$ and $W$ be finite-dimensional inner product spaces, and let $\Sigma \in \mat{n}{\fieldK}$ and $\Gamma \in \mat{m}{\fieldK}$ be the matrices of the inner products of $V$ and $W$ with respect to ordered bases $\calV$ and $\calW$, respectively. We then have the following characterisation of adjoints of linear maps $V \to M$.

% \begin{proposition}
%     \label{prop:adjoint-formula-IP-matrix}
%     Let $T \colon V \to W$ be a linear map. Its adjoint $T^* \colon W \to V$ is the unique linear map satisfying
%     %
%     \begin{equation*}
%         \Sigma \, \mr{\calV}{T^*}{\calW}
%             = (\mr{\calW}{T}{\calV})^* \, \Gamma.
%     \end{equation*}
% \end{proposition}

% \begin{proof}
%     \Cref{prop:adjoint-inner-product} implies that
%     %
%     \begin{equation*}
%         \coordvec{v}{\calV}^* \, \Sigma \, \mr{\calV}{T^*}{\calW} \, \coordvec{w}{\calW}
%             = \inner{v}{T^* w}_V
%             = \inner{Tv}{w}_W
%             = \coordvec{v}{\calV}^* \, (\mr{\calW}{T}{\calV})^* \, \Gamma \, \coordvec{w}{\calW},
%     \end{equation*}
%     %
%     for all $v \in V$ and $w \in W$, and $T^*$ is clearly unique with this property.
% \end{proof}
% %
% This result has various important consequences in the case where $\calV$ and $\calW$ are orthonormal:

% \begin{corollarynoproof}
%     If $\calV$ and $\calW$ are orthonormal, then
%     %
%     \begin{equation*}
%         \mr{\calV}{T^*}{\calW}
%             = (\mr{\calW}{T}{\calV})^*.
%     \end{equation*}
% \end{corollarynoproof}

% \begin{corollarynoproof}
%     If $T \colon \fieldK^n \to \fieldK^m$, then
%     %
%     \begin{equation*}
%         \smr{T^*}
%             = \smr{T}^*.
%     \end{equation*}
% \end{corollarynoproof}

% \begin{corollarynoproof}
%     If $A \in \mat{m,n}{\fieldK}$, then
%     %
%     \begin{equation*}
%         M_{A^*}
%             = (M_A)^*.
%     \end{equation*}
% \end{corollarynoproof}


% \section{The spectral theorem}

% \newpar

% Before we state and prove the spectral theorem, we need two lemmas. The first is a result on coordinate maps with respect to orthonormal bases. We equip $\fieldK^n$ with the Euclidean inner product.

% \begin{lemma}
%     \label{lem:coordinate-map-isometry}
%     Let $V$ and $W$ be finite-dimensional inner product spaces, and let $\calV$ and $\calW$ be ordered orthonormal bases for $V$ and $W$. Then the coordinate map $\coordmap{\calV}$ is unitary, i.e.
%     %
%     \begin{equation}
%         \label{eq:coordinate-map-isometry}
%         \inner{\coordvec{v}{\calV}}{\coordvec{u}{\calV}}
%             = \inner{v}{u}
%     \end{equation}
%     %
%     for all $v,u \in V$.
% \end{lemma}

% \begin{proof}
%     By bi- or sesquilinearity of the inner product it suffices to prove \cref{eq:coordinate-map-isometry} for a basis for $V$. And writing $\calV = (v_1, \ldots, v_n)$ we find that
%     %
%     \begin{equation*}
%         \inner{\coordvec{v_i}{\calV}}{\coordvec{v_j}{\calV}}
%             = \inner{e_i}{e_j}
%             = \delta_{ij}
%             = \inner{v_i}{v_j}
%     \end{equation*}
%     %
%     for $1 \leq i,j \leq n$.
% \end{proof}

% To motivate the second lemma, let $T \colon V \to V$ be an operator on an $\field$-vector space $V$, and let $U$ be a subspace of $V$ that is invariant under $T$ (i.e., $T(U) \subseteq U$). If $W$ is a complement of $U$, i.e. $V = U \oplus W$, then $W$ is not necessarily invariant under $T$. However, we have the following, which we state for general Hilbert spaces for completeness:

% \begin{lemma}
%     \label{lem:adjoint-invariant-subspace}
%     Let $V$ be a Hilbert space and let $T \in \calB(V)$. If a subspace $U$ of $V$ is invariant under $T$, then $U^\perp$ is invariant under $T^*$.
% \end{lemma}

% \begin{proof}
%     Let $v \in U^\perp$. For $u \in U$ we have $Tu \in U$, so
%     %
%     \begin{equation*}
%         \inner{T^*v}{u}
%             = \inner{v}{Tu}
%             = 0.
%     \end{equation*}
%     %
%     Since this holds for all $u \in U$, it follows that $T^*v \in U^\perp$ as desired.
% \end{proof}


% \newpar

% We are now in a position to state and prove

% \begin{theorem}[The spectral theorem]
%     \label{thm:spectral-theorem}
%     Let $V$ be a finite-dimensional inner product space over $\fieldK$, and let $T \in \calL(V)$. Then the following are equivalent:
%     %
%     \begin{enumthm}
%         \item \label{enum:spectral-selfadjoint-normal} $\fieldK = \reals$ and $T$ is self-adjoint, or $\fieldK = \complex$ and $T$ is normal.
        
%         \item \label{enum:spectral-orthogonally-diagonalisable} $T$ is orthogonally diagonalisable.

%         \item \label{enum:spectral-operator-decomposition} $T$ has the orthogonal spectral resolution
%         %
%         \begin{equation*}
%             T
%                 = \sum_{\lambda \in \spec T} \lambda P_\lambda,
%         \end{equation*}
%         %
%         where $P_\lambda$ is the orthogonal projection onto the eigenspace $E_T(\lambda)$. In particular, $V$ is an orthogonal direct sum of the eigenspaces of $T$, i.e.
%         %
%         \begin{equation*}
%             V
%                 = \bigodot_{\lambda \in \spec T} E_T(\lambda).
%         \end{equation*}

%         \item \label{enum:spectral-multiplication-operator} $T$ is unitarily (when $\fieldK = \complex$) or orthogonally (when $\fieldK = \reals$) equivalent to a multiplication operator $M_A \in \calL(\fieldK^n)$ where $A$ is a diagonal matrix, and the diagonal of $A$ contains the eigenvalues of $T$ with multiplicity. If $\calV$ is an ordered orthonormal basis for $V$ consisting of eigenvectors for $T$, then we may choose $A = \mr{\calV}{T}{\calV}$ and
%         %
%         \begin{equation*}
%             T
%                 = \coordmap{\calV}\inv \circ M_A \circ \coordmap{\calV},
%         \end{equation*}
%         %
%         with $\coordmap{\calV}$ unitary.
%     \end{enumthm}
% \end{theorem}
% %
% Note that the first part of property \subcref{enum:spectral-operator-decomposition} means that
% %
% \begin{equation*}
%     \id_V
%         = \sum_{\lambda \in \spec T} P_\lambda
% \end{equation*}
% %
% is a resolution of the identity, i.e. that $P_\lambda P_\mu = 0$ for $\lambda \neq \mu$, and that this is composed of orthogonal projections.

% Most of the equivalences above are simply a matter of unrolling the definitions. The main technical work is done in the following:

% \begin{lemma}
%     \label{lem:spectral-lemma}
%     $V$ be a finite-dimensional inner product space over $\fieldK$, and consider $T \in \calL(V)$. If either
%     %
%     \begin{enumthm}
%         \item $\fieldK = \reals$ and $T$ is self-adjoint, or
%         \item $\fieldK = \complex$ and $T$ is normal,
%     \end{enumthm}
%     %
%     then $T$ is orthogonally diagonalisable.
% \end{lemma}

% \begin{proof}
%     Assume that either $\fieldK = \reals$ and $T$ is self-adjoint, or that $\fieldK = \complex$ and $T$ is normal. We prove by induction in $n = \dim V$ that $T$ is orthogonally diagonalisable. If $n = 1$ then this follows since $T$ has an eigenvalue, so assume that the claim is proved for operators on spaces of dimension strictly less than $n$.

%     Let $\lambda \in \spec T$, and consider the corresponding eigenspace $E_T(\lambda)$. If $d \defn \dim E_T(\lambda) = n$, then any orthonormal basis of $E_T(\lambda)$ will suffice. Assume therefore that $0 < d < n$.

%     The space $E_T(\lambda) = E_{T^*}(\conj{\lambda})$ is clearly invariant under both $T$ and $T^*$. It follows from \cref{lem:adjoint-invariant-subspace} that $E_T(\lambda)^\perp$ is also invariant under both $T$ and $T^*$. We furthermore have $\dim E_T(\lambda)^\perp = n-d$ and $0 < n-d < n$. Let $T_\parallel \in \calL(E_T(\lambda))$ and $T_\perp \in \calL(E_T(\lambda)^\perp)$ denote the restrictions of $T$ to $E_T(\lambda)$ and $E_T(\lambda)^\perp$ respectively. Both $T_\parallel$ and $T_\perp$ are also self-adjoint or normal, depending on the hypothesis, so the induction hypothesis furnishes orthonormal bases $\calU$ and $\calW$ for $E_T(\lambda)$ and $E_T(\lambda)^\perp$ consisting of eigenvectors of $T$. But then $\calV = \calU \union \calW$ is an orthonormal basis for $V$ as desired.
% \end{proof}


% \begin{proofof}[Proof of \cref{thm:spectral-theorem}]
% \begin{proofsec*}
%     \item[\Namesubcref{enum:spectral-selfadjoint-normal} $\implies$ \namesubcref{enum:spectral-orthogonally-diagonalisable}]
%     This is just \cref{lem:spectral-lemma}.

%     \item[\Namesubcref{enum:spectral-selfadjoint-normal} \& \namesubcref{enum:spectral-orthogonally-diagonalisable} $\implies$ \namesubcref{enum:spectral-operator-decomposition}]
%     The first claim says that distinct eigenspaces are orthogonal, which is just a restatement of \cref{enum:normal-orthogonal-eigenspaces}. To prove the second, let $\calV = (v_1, \ldots, v_n)$ be an orthonormal basis for $V$ consisting of eigenvectors for $T$, and let $\lambda_1, \ldots, \lambda_n$ be the corresponding eigenvalues. Then for any $v = \alpha_1 v_1 + \cdots + \alpha_n v_n$ we have $P_{\lambda_i} v = \alpha_i v_i$, so
%     %
%     \begin{equation*}
%         \biggl( \sum_{\lambda \in \spec T} P_\lambda \biggr) v
%             = \sum_{\lambda \in \spec T} P_\lambda v
%             = \sum_{i=1}^n \alpha_i v_i
%             = v.
%     \end{equation*}
%     %
%     For the third claim, notice that
%     %
%     \begin{equation*}
%         \biggl( \sum_{\lambda \in \spec T} \lambda P_\lambda \biggr) v
%             = \sum_{\lambda \in \spec T} \lambda P_\lambda v
%             = \sum_{i=1}^n \lambda_i \alpha_i v_i
%             = \sum_{i=1}^n \alpha_i Tv_i
%             = Tv.
%     \end{equation*}
%     %
%     The final claim follows from the first two.

%     \item[\Namesubcref{enum:spectral-operator-decomposition} $\implies$ \namesubcref{enum:spectral-orthogonally-diagonalisable}]
%     This follows from the decomposition of $V$ into an orthogonal sum of eigenspaces, by constructing an orthonormal basis for each eigenspace.

%     \item[\Namesubcref{enum:spectral-orthogonally-diagonalisable} $\implies$ \namesubcref{enum:spectral-multiplication-operator}]
%     Let $\calV = (v_1, \ldots, v_n)$ be an ordered orthonormal basis for $\calV$ consisting of eigenvectors for $T$ with corresponding eigenvalues $\lambda_1, \ldots, \lambda_n$, and consider the matrix representation $\mr{\calV}{T}{\calV}$. If $(e_1, \ldots, e_n)$ is the standard basis on $\fieldK^n$, then \cref{lemma:mr-eigenvalues} implies that the vectors $\coordvec{v_i}{\calV} = e_i$ are eigenvectors for $\mr{\calV}{T}{\calV}$. Hence $\mr{\calV}{T}{\calV}$ is diagonal, so the basis representation $\coordmap{\calV} \circ T \circ \coordmap{\calV}\inv$ is multiplication by a diagonal matrix. Next notice that
%     %
%     \begin{equation*}
%         T
%             = \coordmap{\calV}\inv \circ (\coordmap{\calV} \circ T \circ \coordmap{\calV}\inv) \circ \coordmap{\calV},
%     \end{equation*}
%     %
%     so it suffices to show that $\coordmap{\calV}$ is unitary (orthogonal). But this follows by \cref{lem:coordinate-map-isometry}.

%     \item[\Namesubcref{enum:spectral-multiplication-operator} $\implies$ \namesubcref{enum:spectral-selfadjoint-normal}]
%     First assume that $\fieldK = \complex$. Since $\coordmap{\calV}$ is unitary we have $\coordmap{\calV}\inv = \coordmap{\calV}^*$, so
%     %
%     \begin{equation*}
%         T^*
%             = (\coordmap{\calV}^* \circ M_A \circ \coordmap{\calV})^*
%             = \coordmap{\calV}^* \circ M_A^* \circ \coordmap{\calV}
%             = \coordmap{\calV}\inv \circ M_{A^*} \circ \coordmap{\calV}.
%     \end{equation*}
%     %
%     Since $A$ is diagonal, $T$ clearly commutes with $T^*$, hence is normal.

%     If instead $\fieldK = \reals$, the same argument shows that $T^* = \coordmap{\calV}\inv \circ M_{A\trans} \circ \coordmap{\calV}$, but since $A$ is diagonal this is just $T$, so $T$ is self-adjoint.
% \end{proofsec*}
% \end{proofof}


% \begin{corollary}
%     \label{cor:self-adjoint-unitary-eigenvalue-characterisation}
%     Let $T \in \calL(V)$ be a normal operator on a complex vector space $V$.
%     %
%     \begin{enumcor}
%         \item \label{enum:self-adjoint-eigenvalue-characterisation} $T$ is self-adjoint if and only if $\spec T \subseteq \reals$.
%         \item \label{enum:unitary-eigenvalue-characterisation} $T$ is unitary if and only if $\spec T \subseteq \sphere^1$.
%     \end{enumcor}
% \end{corollary}
% %
% Note that this does not hold on a real vector space, since then a normal operator is not necessarily diagonalisable.

% \begin{proof}
% \begin{proofsec*}
%     \item[\Namesubcref{enum:self-adjoint-eigenvalue-characterisation}]
%     The \enquote{only if} part follows from \cref{enum:self-adjoint-eigenvalues-exists-and-real}, so assume that $\spec T \subseteq \reals$ and notice that
%     %
%     \begin{equation*}
%         T^*
%             = \biggl( \sum_{\lambda \in \spec T} \lambda P_\lambda \biggr)^*
%             = \sum_{\lambda \in \spec T} \conj{\lambda} P_\lambda^*
%             = \sum_{\lambda \in \spec T} \lambda P_\lambda,
%     \end{equation*}
%     %
%     since each $\lambda \in \reals$, and each $P_\lambda$ is an orthogonal projection, hence self-adjoint.
    
%     Alternatively, choose a diagonal matrix $A \in \mat{n}{\fieldK}$ in accordance with \cref{enum:spectral-multiplication-operator}. Since the diagonal of $A$ contains the eigenvalues of $T$, we have $A^* = A$, and so it follows that $T^* = T$.

%     \item[\Namesubcref{enum:unitary-eigenvalue-characterisation}]
%     Similarly, the \enquote{only if} part is just \cref{enum:unitary-eigenvalues-unit-circle}. Assume that $\spec T \subseteq \sphere^1$ and notice that
%     %
%     \begin{equation*}
%         T^*
%             = \sum_{\lambda \in \spec T} \conj{\lambda} P_\lambda.
%     \end{equation*}
%     %
%     Since the projections $P_\lambda$ are pairwise orthogonal, we have
%     %
%     \begin{equation*}
%         T^*T
%             = \sum_{\lambda \in \spec T} \conj{\lambda} \lambda P_\lambda
%             = \sum_{\lambda \in \spec T} \abs{\lambda}^2 P_\lambda
%             = \sum_{\lambda \in \spec T} P_\lambda
%             = \id_V,
%     \end{equation*}
%     %
%     so $U$ is unitary.
    
%     Alternatively, let $A$ be as above. Then all diagonal elements in $A$ are nonzero, so $A$ is invertible, and we clearly have $A^* A = I_n$. Hence also $T^* T = \id_V$, so $T$ is unitary.
% \end{proofsec*}
% \end{proof}


\chapter{Sesquilinear forms}\label{ch:sesquilinear-forms}

\newcommand{\Aut}{\mathrm{Aut}}

\section{Definitions}

\newpar\label{par:sigma-linearity}

If $V$ is a complex vector space, recall that a map $\phi \colon V \prod V \to \complex$ is called \keyword{sesquilinear} if it is linear in one entry and conjugate-linear in the other. Opinions differ as to in which entry $\phi$ should be linear, but our sesquilinear forms will be linear in the \emph{second} entry. Similarly, if $V$ is a vector space over an arbitrary field $\field$, recall that $\phi \colon V \prod V \to \field$ is \keyword{bilinear} if it is linear in each entry (i.e., if it is $2$-linear in the terminology of \cref{sec:determinant-existence-uniqueness}).

In order to collect these two properties under one concept, we first note explicitly the difference between linearity
%
\begin{equation*}
    T(\alpha u + v)
        = \alpha Tu + Tv
\end{equation*}
%
of a map $T \colon V \to W$ between $\field$-vector spaces, and conjugate linearity
%
\begin{equation*}
    T(\alpha u + v)
        = \conj{\alpha} Tu + Tv
\end{equation*}
%
in the case where $\field = \complex$. We notice that both of the right-hand side expressions are on the form
%
\begin{equation*}
    T(\alpha u + v)
        = \sigma(\alpha) Tu + Tv,
\end{equation*}
%
where in the first case $\sigma$ is just the identity function on $\field$, and in the latter case it is complex conjugation. Both of these maps are field automorphisms, and in fact they are involutions, in the sense that $\sigma^2 = \id$. We denote the automorphisms on $\field$ by $\Aut(\field)$.

If $\sigma \in \Aut(\field)$ and $T \colon V \to W$ is a map between $\field$-vector spaces, then we will say that $T$ is \keyword{$\sigma$-linear} if
%
\begin{equation*}
    T(\alpha u + v)
        = \sigma(\alpha) Tu + Tv
\end{equation*}
%
for all $u,v \in V$ and $\alpha \in \field$. We note that if $T$ is $\sigma$-linear and bijective, then its inverse is $\sigma\inv$-linear: For if $w,z \in W$ with $w = Tu$ and $z = Tv$, then
%
\begin{equation*}
    \alpha w + z
        = \alpha Tu + Tv
        = T(\sigma\inv(\alpha)u + v),
\end{equation*}
%
and applying $T\inv$ to both sides we find that $T\inv(\alpha w + z) = \sigma\inv(\alpha)Tw + Tz$. If $T$ is $\sigma$-linear and bijective, we will call $T$ a \keyword{$\sigma$-isomorphism}, though this is really most useful when $\sigma$ is an involution, since then $T\inv$ is then also a $\sigma$-isomorphism and not just a $\sigma\inv$-isomorphism. Furthermore, if $S \colon V \to W$ is also $\sigma$-linear, then it is easy to see that $T + S$ is $\sigma$-linear, and that $\alpha T$ is $\sigma$-linear for all $\alpha \in \field$. Hence the set of $\sigma$-linear maps $V \to W$ is a vector space. Finally, if $R \colon W \to U$ is $\rho$-linear for an automorphism $\rho$, then $R \circ T$ is $\rho \circ \sigma$-linear.


\newpar

We wish to study forms whose first entry respect such an automorphism $\sigma$. However, it is not ideal to consider the automorphism $\sigma$ as part of the underlying structure on which we define the form, say as a pair $(V,\sigma)$. Notice for instance that if we did so, then we could not define a sesquilinear and a bilinear form on the same complex vector space $V$, since in the former case $\sigma$ is complex conjugation and in the latter it is the identity. But notice also that since we wish to consider sesquilinear forms in general, and not just sesquilinear forms that respect a particular automorphism, we also do not wish to consider $\sigma$ as part of the form itself. Hence we arrive at the following definition:

\begin{definition}[Sesquilinear form]
    Let $V$ be an $\field$-vector space. A map $\phi \colon V \prod V \to \field$ is called a \keyword{sesquilinear form} on $V$ if there exists a $\sigma \in \Aut(\field)$ such that $\phi(\,\cdot\,,v)$ is $\sigma$-linear and $\phi(v,\,\cdot\,)$ is linear for all $v \in V$. If $\sigma$ is such an automorphism, then we also call $\phi$ a \keyword{$\sigma$-sesquilinear form}.
\end{definition}
%
The usual notion of sesquilinear forms on complex vector spaces is recovered if $\sigma$ is complex conjugation. Notice also that a bilinear form is just a $\id_V$-sesquilinear form. We should also note that there is a generalisation of this concept to modules over a division ring, but we shall not need this.


\newpar

We next describe the different kinds of sesquilinear forms that are of interest to us.

TODO sigma epsilon hermitian without being sigma sesquilinear????

\begin{definition}
    \label{def:sesquilinear-form-types}
    The sesquilinear form $\phi$ on $V$ is said to be
    %
    \begin{enumdef}
        \item \keyword{nontrivial} if $\phi(u,v) \neq 0$ for some $u,v \in V$.

        \item \keyword{reflexive} if $\phi(u,v) = 0$ implies $\phi(v,u) = 0$ for all $u,v \in V$.

        \item \keyword{$(\sigma,\epsilon)$-Hermitian} if there is a $\sigma \in \Aut(\field)$ and an $\epsilon \in \field$ such that $\phi$ is $\sigma$-sesquilinear, and such that $\phi(u,v) = \epsilon \sigma(\phi(v,u))$ for all $u,v \in V$. Furthermore, $\phi$ is called
        %
        \begin{center}
            \begin{tabular}{lcl}
                \keyword{$\sigma$-Hermitian} & \multirow{4}{*}{if it is} & $(\sigma,1)$- \\
                \keyword{$\sigma$-anti-Hermitian} && $(\sigma,-1)$- \\
                \keyword{symmetric} && $(\id_\field,1)$- \\
                \keyword{skew-symmetric} && $(\id_\field,-1)$- \\
            \end{tabular}
        \end{center}
        %
        Hermitian. If $\phi$ is $(\sigma,\epsilon)$-Hermitian for some $\sigma$ and $\epsilon$, then it is simply called \keyword{Hermitian}.

        \item \keyword{alternating} if $\phi(v,v) = 0$ for all $v \in V$.
    \end{enumdef}
\end{definition}
%
More concretely, $\phi$ is symmetric if $\phi(u,v) = \phi(v,u)$ for all $u,v \in V$, and skew-symmetric if $\phi(u,v) = -\phi(v,u)$ for all $u,v \in V$. Usually the element $\epsilon$ above will be nonzero; indeed it is most often $\pm 1$. Furthermore, as mentioned in \cref{par:sigma-linearity} the case where $\sigma$ is an involution is particularly nice, since then a bijective map $T$ is a $\sigma$-isomorphism just when its inverse $T\inv$ is a $\sigma$-isomorphism. On the other hand, Hermitian forms are also of great interest, not least because they generalise the many important cases mentioned above. But it also turns out that there is a connection between Hermitian forms and involutions:

\begin{lemma}
    \label{lem:Hermitian-implies-involution}
    If $\phi$ is a nontrivial $(\sigma,\epsilon)$-Hermitian form, then $\sigma$ is an involution.
\end{lemma}

\begin{proof}
    There exist $u,v \in V$ such that $\phi(u,v) \neq 0$. If $\alpha \in \field$, then
    %
    \begin{align*}
        \alpha \phi(u,v)
            &= \phi(\sigma(\alpha)u, v) \\
            &= \epsilon \sigma \bigl( \phi(v, \sigma(\alpha)u) \bigr) \\
            &= \epsilon \sigma^2(\alpha) \sigma \bigl( \phi(v,u) \bigr) \\
            &= \sigma^2(\alpha) \phi(u,v).
    \end{align*}
    %
    Dividing through by $\phi(u,v)$ yields $\sigma^2(\alpha) = \alpha$, so $\sigma$ is an involution.
\end{proof}



Now note the following relationships between the properties in \cref{def:sesquilinear-form-types}:

\begin{lemma}
    \begin{enumlem}
        \item If $\chr \field = 2$, then
        %
        \begin{equation*}
            \text{alternating}
                \implies \text{symmetric}
                \iff \text{skew-symmetric}.
        \end{equation*}

        \item If $\chr \field \neq 2$, then
        %
        \begin{equation*}
            \text{alternating}
                \iff \text{skew-symmetric}.
        \end{equation*}

        \item \label{enum:Hermitian-implies-reflexive} For all $\sigma \in \Aut(\field)$ and $\epsilon \in \field$,
        %
        \begin{equation*}
            \text{$(\sigma,\epsilon)$-Hermitian}
                \implies \text{reflexive}.
        \end{equation*}
    \end{enumlem}
\end{lemma}
%
It turns out that every reflexive $\sigma$-sesquilinear form is also $(\sigma,\epsilon)$-Hermitian for some $\epsilon \in \field$, so the implication in \subcref{enum:Hermitian-implies-reflexive} is actually an equivalence. [TODO reference to proof]

\begin{proof}
    Note that if $\phi$ is alternating then it is skew-symmetric: For
    %
    \begin{equation*}
        0
            = \phi(u + v, u + v)
            = \phi(u,v) + \phi(v,u),
    \end{equation*}
    %
    implying that $\phi$ is skew-symmetric. If $\chr \field = 2$ then $1 = -1$, and so symmetry and skew-symmetry are clearly equivalent. If instead $\chr \field \neq 2$ and $\phi$ is skew-symmetric, then $\phi(v,v) = -\phi(v,v)$, so $2\phi(v,v) = 0$ which implies that $\phi(v,v) = 0$.

    For the final claim, assume that $\phi$ is $(\sigma,\epsilon)$-Hermitian, and let $u,v \in V$ satisfy $\phi(u,v) = 0$. It follows that
    %
    \begin{equation*}
        \phi(v,u)
            = \epsilon \sigma(\phi(u,v))
            = \epsilon \sigma(0)
            = 0,
    \end{equation*}
    %
    since $\sigma$ is a field homomorphism.
\end{proof}
%
In particular, a skew-symmetric form is either symmetric or alternating, and we thus do not need to explicitly study skew-symmetric forms.


\newpar

If $\phi$ is a sesquilinear form on $V$, then the \keyword{associated quadratic form} is the map $Q \colon V \to \field$ given by $Q(v) = \phi(v,v)$. If $\phi$ is $\sigma$-sesquilinear, then it follows that $Q(\alpha v) = \alpha \sigma(\alpha) Q(v)$. In particular, if $\phi$ is bilinear then $Q(\alpha v) = \alpha^2 Q(v)$.

It is well-known that the real and complex inner products can be recovered from their quadratic forms, and we will return to these in TODO ref. For now we assume that $\phi$ is bilinear and that $\chr \field \neq 2$, and we let
%
\begin{equation*}
    \phi_s(u,v)
        = \tfrac{1}{2} \bigl( \phi(u,v) + \phi(v,u) \bigr)
    \quad \text{and} \quad
    \phi_a(u,v)
        = \tfrac{1}{2} \bigl( \phi(u,v) - \phi(v,u) \bigr),
\end{equation*}
%
so that $\phi = \phi_s + \phi_a$. Then both $\phi_s$ and $\phi_a$ are bilinear, $\phi_s$ is symmetric and $\phi_a$ anti-symmetric [TODO decide between anti and skew], hence alternating. If $Q_s$ is the quadratic form associated with $\phi_s$, then
%
\begin{equation*}
    Q(v)
        = \phi(v,v)
        = \phi_s(v,v) + \phi_a(v,v)
        = \phi_s(v,v)
        = Q_s(v)
\end{equation*}
%
for all $v \in V$. That is, the quadratic form associated with a bilinear form can only determine its \textquote{symmetric part}. Indeed, we could replace $\phi_a$ with any anti-symmetric bilinear form and we would still have $Q = Q_s$. Hence we cannot hope to find a polarisation identity for arbitrary bilinear forms, a fortiori for sesquilinear forms. But if $\phi$ is already symmetric, then we have the following:

\begin{propositionnoproof}
    Assume that $\phi$ is bilinear and symmetric, and that $\chr \field \neq 2$. Then
    %
    \begin{equation*}
        \phi(u,v)
            = \tfrac{1}{2} \bigl( Q(u+v) - Q(u) - Q(v) \bigr)
            = \tfrac{1}{4} \bigl( Q(u+v) - Q(u-v) \bigr)
    \end{equation*}
    %
    for all $u,v \in V$.
\end{propositionnoproof}
%
The proof is simply a case of inserting the definition of $Q$.

The real case is thus taken care of. There is of course also a polarisation identity for inner products on complex vector spaces, but since this is not bilinear it takes a slightly different form. We will return to this in TODO ref.


\newpar

We make the following conventions:

\begin{assumption}
    For the remainder of this chapter, let $V$ be an $\field$-vector space $V$ equipped with a sesquilinear form denoted $\inner{\,\cdot\,}{\,\cdot\,}$. Assume that $\inner{\,\cdot\,}{\,\cdot\,}$ is $\sigma$-sesquilinear for some $\sigma \in \Aut(\field)$. We denote the associated quadratic form by $Q$. We also write $\inner{\,\cdot\,}{\,\cdot\,}_V$ for the form on $V$ and $Q_V$ for the associated quadratic form for clarity, to distinguish this from forms on other vector spaces.
    
    If $V$ is a topological vector space, we further assume that the form on $V$ is continuous in each entry separately.
\end{assumption}

% TODO: Necessary that form is jointly continuous?


\section{Properties of sesquilinear forms}

\newpar

We begin with a concept familiar from the theory of inner product spaces:

\begin{definition}[Orthogonality]
    Let $M,N \subseteq V$ be subsets. If $\inner{u}{v} = 0$ for all $u \in M$ and $v \in N$, then we say that $M$ is \keyword{orthogonal} to $N$, written $M \perp N$. The \keyword{orthogonal complement} of $M$ is the set $M^\perp = \set{v \in V}{v \perp M}$.
\end{definition}
%
If either $M$ or $N$ (or both) is a singleton, then we write e.g. $u \perp N$ for $\{u\} \perp N$, and we say that $u$ is orthogonal to $N$. Note that if $M \subseteq N$, then $N^\perp \subseteq M^\perp$.

We usually only study orthogonality in the context of reflexive forms, since these are precisely the forms for which the orthogonality relation is symmetric. Hence:
%
\begin{assumption}
    For the remainder of this section, the form on $V$ is reflexive.
\end{assumption}

\begin{lemma}
    If $M \subseteq V$, then $M^\perp$ is a subspace of $V$.
\end{lemma}

\begin{proof}
    If $u,v \in M^\perp$ and $\alpha \in \field$, then for all $w \in M$ we have
    %
    \begin{equation*}
        \inner{w}{\alpha u + v}
            = \alpha \inner{w}{u} + \inner{w}{v}
            = 0,
    \end{equation*}
    %
    so $\alpha u + v \in M^\perp$.
\end{proof}


\newpar

\newcommand{\rad}[1]{\sqrt{#1}}

In an inner product space, only the zero vector is orthogonal to itself. However, in the context of general sesquilinear forms this is not so, and it is in fact possible for a nonzero vector to be orthogonal to \emph{every} vector in the space.

First of all, a vector $v \in V$ is \keyword{isotropic} if $v \perp v$, and \keyword{nonisotropic} otherwise. If $V$ contains at least one isotropic vector, then $V$ itself is called isotropic, and otherwise nonisotropic. In case every vector in $V$ is isotropic, then $V$ is said to be \keyword{totally isotropic}.

On the other hand, $v$ is \keyword{degenerate} if $v \perp V$. The set of all degenerate vectors in $V$ is called the \keyword{radical} of $V$ and is denoted $\rad{V}$. (Note that $\rad{V}$ is simply the orthogonal complement $V^\perp$, so in particular it is a subspace of $V$.) If $\rad{V} = 0$ then $V$ is called \keyword{nonsingular/nondegenerate}, if $\rad{V} \neq 0$ it is \keyword{singular/degenerate}, and if $\rad{V} = V$ it is \keyword{totally singular/degenerate}.

That is, a degenerate vector is isotropic, so a nonisotropic space is nonsingular. On pseudo inner product spaces, the converse is also the case due to the Cauchy--Schwarz inequality, but this is not so for general sesquilinear forms. Notice that a subspace of a nonisotropic space is also nonisotropic, while a subspace of a nondegenerate space might still be degenerate. Notice also that the form on $V$ is alternating if and only if $V$ is totally isotropic.

Note that if $U$ is a subspace of $V$, then the notation $U^\perp$ is ambiguous. It might refer to the subset of $U$ of vectors orthogonal to $U$, or the subset of $V$ of vectors orthogonal to $U$. However, the notation $\rad{U}$ always refers to the former. In either interpretation of $U^\perp$, note that $\rad{U} = U \intersect U^\perp$.

To better understand the significance of singularity, we note the following result:

\begin{lemma}
    \label{lem:nonsingular-vectors-equal}
    Assume that $V$ is nonsingular and let $u,v \in V$. If $\inner{u}{w} = \inner{v}{w}$ for all $w \in V$, then $u = v$.

    In particular, if $X$ is a set and the functions $f,g \colon X \to V$ satisfy $\inner{f(x)}{w} = \inner{g(x)}{w}$ for all $x \in X$ and $w \in V$, then $f = g$.
\end{lemma}

\begin{proof}
    If the assumption holds, then $\inner{u-v}{w}$ for all $w \in V$, i.e., $u-v$ is degenerate. But then we must have $u-v = 0$ since $V$ is nonsingular.
\end{proof}


\newpar

Every subspace of a vector space has a complement. However, in the present context we can wish for something more:

\begin{definition}[Orthogonal direct sum]
    The space $V$ is the \keyword{orthogonal direct sum} of the subspaces $U$ and $W$, written
    %
    \begin{equation*}
        V
            = U \odot W,
    \end{equation*}
    %
    if $V = U \oplus W$ and $U \perp W$.
\end{definition}
%
We recall that we have assumed that the form on $V$ is reflexive, so $U \odot W = W \odot U$.


\begin{proposition}
    If $U$ is a complement of $\rad{V}$, then $U$ is nonsingular and $V = \rad{V} \odot U$.
\end{proposition}

\begin{proof}
    Clearly $\rad{V} \perp U$. Now notice that if $v \in \rad{U}$, then $v \perp U$, and we obviously also have $v \perp \rad{V}$. Hence $v \perp \rad{V} \oplus U = V$, so $\rad{U} \subseteq \rad{V}$. And since also $\rad{U} \subseteq U$, we must have $\rad{U} = 0$, so $U$ is nonsingular.
\end{proof}
%
This result shows that we can find a subspace of $V$ that is nonsingular, and every element of $V$ almost lies in $U$: we just have to add a degenerate vector to it. And if degenerate vectors are somehow insignificant, then a restriction to nonsingular spaces is not very severe. And of course, we can always obtain a complement $U$ of $\rad{V}$ as the quotient $V/\rad{V}$ by \cref{prop:complement-iso-to-quotient}.

If $V = U \odot W$, then we regrettably cannot call $U$ an \textquote{orthogonal complement} of $W$, since we have already used this term for the set $U^\perp$, as is standard, and while $U$ and $U^\perp$ are certainly orthogonal, it is not generally the case that $V = U \oplus U^\perp$.


\newpar\label{par:Riesz-map}

If $u \in V$, define a map\footnote{We begin to see why it is useful that sesquilinear forms are linear in the \emph{second} entry. In this way, the vectors $u$ and $v$ appear in the same order in the expressions $\phi_u(v)$ and $\inner{u}{v}$. Notice that this is simply a consequence of the fact that functions are written to the left of their arguments.} $\phi_u \colon V \to \field$ by $\phi_u(v) = \inner{u}{v}$. If $V$ is a topological vector space, recall that $V^*$ denotes its topological dual, and that $\inner{\,\cdot\,}{\,\cdot\,}$ is assumed to be continuous in each entry, so $\phi_u$ is continuous. Hence this gives rise to the $\sigma$-linear \keyword{Riesz map} $\Phi_V \colon V \to V^*$ given by $\Phi_V(u) = \phi_u$. If a functional $\phi \in V^*$ is on the form $\phi_u$ for some $u \in V$, then $u$ is called a \keyword{Riesz vector} for $\phi$. If $\Phi_V$ is bijective, then the unique Riesz vector for $\phi$ is denoted $r_\phi$, and the map $\phi \mapsto r_\phi$ is of course the inverse of $\Phi_V$, so it is $\sigma\inv$-linear. Hence $r_{\phi_u} = u$ and $\phi_{r_\psi} = \psi$ for $u \in V$ and $\psi \in V^*$.

It is sometimes the case that if there is a $\sigma \in \Aut(\field)$ such that $\Phi_V$ is a $\sigma$-isomorphism, then $V$ is in fact reflexive. This is for instance the case for Hilbert spaces. We cannot pursue the theory of duality for general topological spaces here in any detail.


\newpar

We now make the further assumption that $V$ is finite-dimensional. In this case we have the following fundamental theorem:

\begin{theorem}[The Riesz representation theorem]
    \label{thm:Riesz-representation-theorem}
    The Riesz map $\Phi_V$ is injective if and only if $V$ is nonsingular. In particular, if $V$ is finite-dimensional and nonsingular, then $\Phi_V$ is a $\sigma$-isomorphism.
\end{theorem}

\begin{proof}
    Notice that $\phi_u(v) = 0$ for all $v \in V$ just when $u \perp V$, i.e. when $u$ is degenerate. This is the case if and only if $u \in \rad{V}$, so $\ker \Phi_V = \rad{V}$.
\end{proof}

While the Riesz representation theorem only applies to nonsingular spaces, there is also a version for singular subspaces of nonsingular spaces which is sometimes useful. If $U$ is a subspace of $V$, define a map $\Psi^V_U \colon V \to U^*$ by $\Psi^V_U(v) = \phi_v|_U$. Note that $\Psi^V_U = \iota_U^\dagger \circ \Phi_V$, so $\Psi^V_U$ is $\sigma$-linear. We then have the following:

\begin{corollary}
    \label{thm:Riesz-representation-theorem-for-subspaces}
    Let $V$ be finite-dimensional, and let $U$ be a subspace of $V$. Then $\ker \Psi^V_U = U^\perp$, and if either $V$ or $U$ is nonsingular, then $\Psi^V_U$ is surjective.
\end{corollary}

\begin{proof}
    Notice that $\phi_v|_U$ is zero just when $\inner{v}{u} = 0$ for all $u \in U$. But this says that $v \perp U$, i.e. $v \in U^\perp$. Hence $\ker \Psi^V_U = U^\perp$ as claimed.
    
    Next assume that $V$ is nonsingular. Let $\phi \in U^*$ and extend this by $0$ to a linear functional $\overline{\phi}$ on $V$. \Cref{thm:Riesz-representation-theorem} then furnishes a vector $v \in V$ such that $\overline{\phi} = \phi_v$. But then $\phi = \phi_v|_U$ as desired. If instead $U$ is nonsingular, then we can simply apply \cref{thm:Riesz-representation-theorem} to $U$ directly.
\end{proof}
%
As an example of an application of this corollary, we have the following result, which itself has an important corollary:

\begin{proposition}
    \label{prop:nonsingular-space-decomposition}
    Let $V$ be finite-dimensional, and let $U$ be a subspace of $V$. If either $V$ or $U$ is nonsingular, then
    %
    \begin{equation*}
        \dim V
            = \dim U + \dim U^\perp.
    \end{equation*}
    %
    Hence the following are equivalent:
    %
    \begin{enumprop}
        \item $V = U + U^\perp$.
        \item $V = U \odot U^\perp$.
        \item $U$ is nonsingular, i.e., $U \intersect U^\perp = \{0\}$.
    \end{enumprop}
\end{proposition}

\begin{proof}
    By \cref{thm:Riesz-representation-theorem-for-subspaces}, the map $\Psi^V_U$ is surjective onto $U$ with kernel $U^\perp$. Hence \cref{cor:rank-nullity} implies that
    %
    \begin{equation*}
        \dim V
            = \dim U^* + \dim U^\perp.
    \end{equation*}
    %
    But since $U$ is finite-dimensional, it follows that $\dim U = \dim U^*$ by \cref{prop:dual-basis}, so the claim follows.
\end{proof}


\begin{corollary}
    \label{cor:orthogonal-basis-existence}
    Let $V$ be finite-dimensional and nonisotropic. Then $V$ has an orthogonal basis.
\end{corollary}

\begin{proof}
    Every subspace of $V$ is also nonisotropic, in particular nondegenerate, so \cref{prop:nonsingular-space-decomposition} applies. We prove the claim by induction on $n = \dim V$. If $n = 0$, then the claim is obvious, so assume that $n > 0$ and that the claim holds for $n-1$. Choose some nonzero $v \in V$ and let $U = \Span(v)$. Then $\dim U^\perp = n-1$, so $U^\perp$ has an orthogonal basis $\calU$ by induction. Then $\calU \union \{v\}$ is clearly an orthogonal basis for $V$.
\end{proof}


\newpar

We next study maps that respect sesquilinear forms.

\begin{definition}[Isometry]
    \label{def:isometry}
    Let $V$ and $W$ be equipped with sesquilinear forms. A linear map $T \colon V \to W$ is an \keyword{isometry} if
    %
    \begin{equation*}
        \inner{Tu}{Tv}_W
            = \inner{u}{v}_V
    \end{equation*}
    %
    for all $u,v \in V$. If $T$ is also bijective, we say that it is \keyword{unitary} and say that $V$ and $W$ are \keyword{isometric}.
\end{definition}
%
Clearly the composition of two isometries is an isometry, and the inverse of a unitary map is also unitary. In particular, the set of unitary maps on $V$ is a group under composition.

If $T$ is an isometry, then notice that it also respects the associated quadratic forms, i.e. that $Q_W(Tv) = Q_V(v)$ for all $v \in V$. [TODO converse when quadratic form determines sesquilinear form.] Furthermore, we are used to isometries being injective, and the analogous result for sesquilinear forms is the following:

\begin{lemma}
    \label{lem:isometry-injective}
    If $V$ is nonisotropic and $T \colon V \to W$ is an isometry, then $T$ is injective.
\end{lemma}

\begin{proof}
    Let $u,v \in V$ and assume that $Tu = Tv$. Then
    %
    \begin{equation*}
        0
            = Q_W(Tu - Tv)
            = Q_W \bigl( T(u-v) \bigr)
            = Q_V(u-v),
    \end{equation*}
    %
    and since $V$ is nonisotropic, this is only possible when $u = v$.
\end{proof}


% If $V$ is also nondegenerate [TODO why??], an isometric isomorphism $T$ gets a distinctive name depending on the type of geometry:
% %
% \begin{enumerate}
%     \item If $V$ is a unitary geometry, $T$ is called a \keyword{unitary transformation}. The set $\vecU{V}$ of unitary transformations on $V$ is called the \keyword{unitary group} on $V$.

%     \item If $V$ is an orthogonal geometry, $T$ is called an \keyword{orthogonal transformation}. The set $\vecO{V}$ of orthogonal transformations on $V$ is called the \keyword{orthogonal group} on $V$.

%     \item If $V$ is a symplectic geometry, $T$ is called a \keyword{symplectic transformation}. The set $\vecSp{V}$ of symplectic transformations on $V$ is called the \keyword{symplectic group} on $V$.
% \end{enumerate}
% %
% In each case these sets are obviously groups under composition.



\section{Hilbert space adjoints}

\newpar\label{par:Hilbert-space-adjoints}

In \cref{sec:operator-adjoints} we defined one notion of adjoint of a linear map $T \colon V \to W$, namely the pullback $T^\dagger \colon W^* \to V^*$. We now define a different kind of adjoint, denoted $T^*$, that only makes sense for linear maps between spaces equipped with sesquilinear forms. For $T^*$ to be linear we need $V$ and $W$ to both be equipped with a $\sigma$-sesquilinear form for the same $\sigma \in \Aut(\field)$, so we make this assumption from the onset. We also assume that $V$ and $W$ are nonsingular; this is not strictly necessary to define $T^*$, but we lose fundamental properties of adjoints if this is not the case (cf. \cref{prop:adjoint-inner-product}).\footnote{Also strictly speaking, it is only necessary that one of $V$ and $W$ is nonsingular, but we assume that both are for simplicity.} Later we will also need to assume that $V$ and $W$ are nonisotropic (cf. \cref{prop:normal-operator-properties}).

It will also turn out to be useful to assume that the forms on $V$ and $W$ are $(\sigma,\epsilon)$-Hermitian for possibly different $\epsilon$. In $V$ and $W$ are nontrivial, then since they are assumed nonsingular, we must have $\epsilon \neq 0$. Under the same assumptions the forms must be nontrivial, so \cref{lem:Hermitian-implies-involution} implies that $\sigma$ is an involution. The case where $V$ and $W$ is trivial is -- of course -- trivial, so to avoid having to deal this special case, we simply assume that $\sigma$ is always an involution, and that $\epsilon \neq 0$.

Finally, we require that the Riesz maps $\Phi_V$ and $\Phi_W$ (cf. \cref{par:Riesz-map}) are $\sigma$-isomorphisms. It is not strictly necessary for this to be true of $\Phi_W$ to define $T^*$, but we again lose important properties without this assumption. For later use we also let $U$ denote another vector space with the same properties. In total:

\begin{assumption}
    In this section, $V,W,U$ are $\field$-vector spaces that are
    %
    \begin{enumerate}
        \item equipped with $(\sigma,\epsilon)$-Hermitian forms for the same $\sigma \in \Aut(\field)$ but possibly different $\epsilon \in \field$, and
        \item nonsingular,
    \end{enumerate}
    %
    such that
    %
    \begin{enumerate}[resume]
        \item $\sigma$ is an involution,
        \item $\epsilon \neq 0$, and
        \item the Riesz maps $\Phi_V$, $\Phi_W$ and $\Phi_U$ are $\sigma$-isomorphisms.
    \end{enumerate}
\end{assumption}


\newpar

We are now ready for the main definition in this section:

\begin{definition}[Hilbert space adjoints]
    Let $T \in \calL(V,W)$. TODO notation, bounded vs. general linear The \keyword{(Hilbert space) adjoint} of $T$ is the operator $T^* \colon W \to V$ given by
    %
    \begin{equation*}
        T^*
            = \Phi_V\inv \circ T^\dagger \circ \Phi_W.
    \end{equation*}
\end{definition}
%
Properties of the operator adjoint $T^\dagger$ are often inherited by the Hilbert space adjoint: Since $\Phi_V$ and $\Phi_W$ are both $\sigma$-linear and $T^\dagger$ is linear, it follows that $T^*$ is linear. Furthermore, if $S \in \calL(W,U)$ then
%
\begin{align*}
    (ST)^*
        &= \Phi_V\inv \circ (ST)^\dagger \circ \Phi_U \\
        &= \Phi_V\inv \circ T^\dagger \circ S^\dagger \circ \Phi_U \\
        &= (\Phi_V\inv \circ T^\dagger \circ \Phi_W) \circ (\Phi_W\inv \circ S^\dagger \circ \Phi_U) \\
        &= T^* S^*.
\end{align*}

\begin{proposition}
    \label{prop:adjoint-inner-product}
    Let $T \in \calL(V,W)$. For all $w \in W$ we have $T^\dagger \phi_w = \phi_{T^* w}$. In particular, $T^*$ is the unique linear operator $W \to V$ with the property that
    %
    \begin{equation*}
        \inner{T^*w}{v}_V = \inner{w}{Tv}_W,
    \end{equation*}
    %
    or equivalently
    \begin{equation*}
        \inner{v}{T^*w}_V = \inner{Tv}{w}_W,
    \end{equation*}
    %
    for all $v \in V$ and $w \in W$. Furthermore, $T^{**} = T$, i.e., the map $T \mapsto T^*$ is an involution.
\end{proposition}
%
Note that we cannot (at least prima facie) prove that $T^{**} = T$ just by using the definition, since $V^*$ and $W^*$ are not equipped with sesquilinear forms.

\begin{proof}
    First notice that $T^*$ indeed has this property. For $w \in W$ we have
    %
    \begin{equation*}
        \phi_{T^* w}
            = \Phi_V (T^* w)
            = (T^\dagger \circ \Phi_W)(w)
            = T^\dagger \phi_w,
    \end{equation*}
    %
    so for $v \in V$ it thus follows that
    %
    \begin{equation*}
        \inner{T^* w}{v}_V
            = \phi_{T^* w}(v)
            = T^\dagger \phi_w (v)
            = \phi_w(Tv)
            = \inner{w}{Tv}_W,
    \end{equation*}
    %
    as desired. The other identity follows since the forms are Hermitian. Furthermore, if $S \colon W \to V$ is another such operator, then $\inner{Sw}{v}_V = \inner{T^*w}{v}_V$ for all $v$ and $w$, so $S = T^*$ by \cref{lem:nonsingular-vectors-equal} since $V$ is nonsingular. The final claim that $T^{**} = T$ follows by uniqueness, by replacing $T$ with $T^*$ in the identities above.
\end{proof}


\newpar

In \cref{def:isometry} we defined what it means for a linear map to be an isometry or to be unitary. In the present context we can also characterise isometries using adjoints. Furthermore, recall from \cref{lem:isometry-injective} that an isometry between \emph{nonisotropic} spaces is automatically injective. We have not (yet, see \cref{par:spectral-theorem}) assumed that the spaces in question are nonisotropic, only that they are nonsingular, but in the current setting we still get injectivity:

\begin{lemma}
    A linear map $T \colon V \to W$ is an isometry if and only if $T^*T = \id_V$. In particular, isometries are injective, and $T$ is unitary if and only if $T$ is bijective with $T\inv = T^*$.
\end{lemma}

\begin{proof}
    If $T$ is an isometry, then
    %
    \begin{equation*}
        \inner{T^*Tv}{u}_V
            = \inner{Tv}{Tu}_W
            = \inner{v}{u}_V,
    \end{equation*}
    %
    implying that $T^*T = \id_V$ by \cref{lem:nonsingular-vectors-equal}. The converse is obvious.

    For the final claims, notice that the above says that $T^*$ is a left-inverse of $T$, so $T$ is injective. Furthermore, if $T$ is unitary it is bijective, so $T^*$ is also a right-inverse of $T$. Conversely, if $TT^* = \id_W$ then $T$ is surjective and hence unitary.
\end{proof}
%
An operator $TT^* = \id_W$ with the property is called a \keyword{coisometry}.

In the case $W = V$ we say that $T$ is \keyword{normal} if $TT^* = T^*T$, and that $T$ is \keyword{self-adjoint} if $T^* = T$. Clearly both self-adjoint and unitary operators (with $V = W$) are normal.


\newpar

Properties of adjoints TODO text

\begin{proposition} 
    Let $V$ be a finite-dimensional inner product space, and let $T \in \calL(V)$ and $\lambda \in \mathbb{F}$. Then $\lambda \id_V - T$ is invertible if and only if $\sigma(\lambda) \id_V - T^*$ is invertible. In other words, $\lambda$ is an eigenvalue of $T$ if and only if $\sigma(\lambda)$ is an eigenvalue of $T^*$.
\end{proposition}

TODO what about infinite dimension? Boundedly invertible? Also need $\sigma$ to be an involution for the iff.

\begin{proof}
    Since the maps $T \mapsto T^*$ and $\sigma$ are involutions it suffices to prove one implication, so assume that $\lambda \id_V - T$ is invertible. Then there exists an $S \in \calL(V)$ such that
    %
    \begin{equation*}
        S(\lambda \id_V - T)
            = (\lambda \id_V - T)S
            = \id_V,
    \end{equation*}
    %
    and taking adjoints we find that
    %
    \begin{equation*}
        (\sigma(\lambda) \id_V - T^*)S^*
            = S^*(\sigma(\lambda) \id_V - T^*)
            = \id_V.
    \end{equation*}
    %
    That is, $\sigma(\lambda) \id_V - T^*$ is invertible as claimed.
\end{proof}

\begin{remark}
    Note that this does \emph{not} say that $v \in V$ is an eigenvector of $T^*$ if it is an eigenvector of $T$. A counterexample is given by the matrix
    %
    \begin{equation*}
        A =
        \begin{pmatrix}
            1 & 1 \\
            0 & 0
        \end{pmatrix},
    \end{equation*}
    %
    which has the eigenvector $(1,0)$ with eigenvalue $1$. However, while $1$ is also an eigenvalue of the transpose $A\trans$ (with eigenvector $(1,1)$), $(1,0)$ is not an eigenvector of $A\trans$.

    While this does not hold in general, recall that in \cref{enum:normal-adjoint-eigenvalues} we saw that it holds for \emph{normal} operators.
\end{remark}


\begin{proposition}
    \label{prop:normal-operator-properties}
    Let $T \in \calL(V)$ be a normal operator.
    %
    \begin{enumprop}
        \item \label{enum:normal-adjoint-norm} $Q(Tv) = Q(T^*v)$ for all $v \in V$.
        
        \item \label{enum:normal-adjoint-eigenvalues} Assume that $V$ is nonisotropic. Then $E_T(\lambda) = E_{T^*}(\sigma(\lambda))$ for all $\lambda \in \field$.

        \item \label{enum:normal-orthogonal-eigenspaces} Assume that $V$ is nonisotropic. If $\lambda,\mu \in \mathbb{F}$ are distinct eigenvalues of $T$, then $E_T(\lambda)$ and $E_T(\mu)$ are orthogonal.
    \end{enumprop}
\end{proposition}

\begin{proof}
\begin{proofsec*}
    \item[\Namesubcref{enum:normal-adjoint-norm}]
    Notice that
    %
    \begin{equation*}
        Q(Tv)
            = \inner{Tv}{Tv}
            = \inner{T^*Tv}{v}
            = \inner{TT^*v}{v}
            = \inner{T^*v}{T^*v}
            = Q(T^*v).
    \end{equation*}

    \item[\Namesubcref{enum:normal-adjoint-eigenvalues}]
    If $T$ is normal then so is $\lambda \id_V - T$, so \subcref{enum:normal-adjoint-norm} implies that
    %
    \begin{equation*}
        Q\bigl( (\lambda \id_V - T)v \bigr)
            = Q\bigl( (\sigma(\lambda) \id_V - T^*)v \bigr),
    \end{equation*}
    %
    and since $V$ is assumed nonisotropic, this implies that $(\lambda \id_V - T)v = 0$ if and only if $(\sigma(\lambda) \id_V - T^*)v = 0$. Hence $v$ is an eigenvector for $T$ with eigenvalue $\lambda$ if and only if $v$ is an eigenvector for $T^*$ with eigenvalue $\sigma(\lambda)$.

    \item[\Namesubcref{enum:normal-orthogonal-eigenspaces}]
    Let $v \in E_T(\lambda)$ and $u \in E_T(\mu)$. Since $v$ is also an eigenvector for $T^*$ with eigenvalue $\sigma(\lambda)$ by \subcref{enum:normal-adjoint-eigenvalues}, we have
    %
    \begin{equation*}
        \mu \inner{v}{u}
            = \inner{v}{Tu}
            = \inner{T^*v}{u}
            = \inner{\sigma(\lambda)v}{u}
            = \lambda \inner{v}{u}.
    \end{equation*}
    %
    Since $\lambda \neq \mu$ we must have $\inner{v}{u} = 0$ as claimed.
\end{proofsec*}
\end{proof}


In the proof of \cref{thm:spectral-theorem} we will need to restrict normal operators to subspaces of $V$. In the discussion above we have assumed that $V$ is nonsingular, but since this is not a hereditary property, we need to assume that relevant subspaces of $V$ are also nonsingular. In \cref{par:spectral-theorem} we will assume that $V$ is nonisotropic, in which case this is automatically satisfied.

If $U$ is a subspace of $V$ and $T \in \calL(V)$, then we say that $U$ is \keyword{invariant} under $T$. In this case we define an operator $T_U \in \calL(U)$ by $T_U u = Tu$ for $u \in U$. We then have the following results:

\begin{lemma}
    \label{lem:adjoint-invariant-subspace}
    Let $T \in \calL(V)$. If a subspace $U$ of $V$ is invariant under $T$, then $U^\perp$ is invariant under $T^*$.
\end{lemma}

\begin{proof}
    Let $v \in U^\perp$. For $u \in U$ we have $Tu \in U$, so
    %
    \begin{equation*}
        \inner{T^*v}{u}
            = \inner{v}{Tu}
            = 0.
    \end{equation*}
    %
    Since this holds for all $u \in U$, it follows that $T^*v \in U^\perp$ as desired.
\end{proof}


\begin{lemma}
    \label{lem:normal-operator-restriction-is-normal}
    If $T \in \calL(V)$ is normal and the subspace $U \subseteq V$ is nonsingular and invariant under both $T$ and $T^*$, then $T_U \in \calL(U)$ is also normal. To wit, $(T_U)^* = (T^*)_U$.
\end{lemma}

\begin{proof}
    This is obvious from \cref{prop:adjoint-inner-product}.
\end{proof}


\section{Orthogonal diagonalisation}

\newpar

If $V$ is a vector space equipped with a sesquilinear form, then a basis $\calV$ for $V$ is \keyword{orthogonal} if for $u,v \in \calV$, $\inner{u}{v} = 0$ when $u \neq v$. Furthermore, $\calV$ is called \keyword{orthonormal} if also $Q(v) = 1$ for all $v \in \calV$. In real or complex vector spaces any orthogonal basis can be modified to obtain an orthonormal basis, by dividing each basis vector by its norm. But for general $\sigma$-sesquilinear forms this is not possible, since even if $\sigma = \id_V$ we would need to divide $v$ by a square root of $Q(v)$, and this might not exist; indeed $Q(v)$ might not even be nonzero.

To state the spectral theorem we need a definition. Recall that we in \cref{par:diagonalisability} defined what it means for an operator $T$ to be diagonalisable. In the context of sesquilinear forms we have the following stronger properties: If $V$ is finite-dimensional and equipped with a sesquilinear form, then an operator $T \in \calL(V)$ is \keyword{orthogonally diagonalisable} if there is an orthogonal basis for $V$ consisting of eigenvectors for $T$. If there is an \emph{orthonormal} basis for $V$ of eigenvectors, then $T$ is \keyword{orthonormally diagonalisable}. Clearly the latter is the stronger of the two properties.

Furthermore, if $\id_V = P_1 + \cdots + P_k$ is a resolution of the identity, then we say that it is \keyword{orthogonal} if each $P_i$ is an orthogonal projection. We then have the following analogue of \cref{prop:diagonalisability-equivalent-properties}:

\begin{propositionnoproof}
    \label{prop:orthogonal-diagonalisability-equivalent-properties}
    Let $T \in \calL(V)$. The following are equivalent:
    %
    \begin{enumerate}
        \item $T$ is orthogonally diagonalisable.
        
        \item $V$ has an ordered orthogonal basis $\calV$ such that $\mr{\calV}{T}{\calV}$ is diagonal.
        
        \item $V$ has the form
        %
        \begin{equation*}
            V
                = \bigodot_{\lambda \in \spec T} E_T(\lambda).
        \end{equation*}
        
        \item If $\spec T = \{\lambda_1, \ldots, \lambda_k\}$ and $P_i$ is projection onto $E_T(\lambda_i)$ along $\bigoplus_{j \neq i} E_T(\lambda_j)$, then $\id_V = P_1 + \cdots + P_k$ is an orthogonal resolution of the identity.
        
        \item $T$ is orthogonally similar\footnote{Cf. \cref{par:equivalent-similar-congruent-maps}.} to a multiplication operator $M_A$, where $A \in \mat{n}{\field}$ is a diagonal matrix whose diagonal contains the eigenvalues of $T$ with multiplicity:
        %
        \begin{equation*}
            T
                = \coordmap{\calV}\inv \circ M_A \circ \coordmap{\calV}.
        \end{equation*}
    \end{enumerate}
\end{propositionnoproof}


\newpar\label{par:spectral-theorem}

We now arrive at the main theorem in this chapter, the spectral theorem. This is usually proved for normal operators on complex vector spaces, but as we show in this section, we can get away with assuming somewhat less. Apart from the assumptions we made in \cref{par:Hilbert-space-adjoints}, we will need the vector space $V$ to be nonisotropic in order to access the results in \cref{prop:normal-operator-properties}. We also need $V$ to be finite-dimensional, partly since our proof of the spectral theorem will be by induction in the dimension of $V$, and partly since we will also need every (normal) operator on $V$ to have an eigenvalue. For the latter reason we also assume that $\field$ is algebraically closed. In total:

\begin{assumption}
    For the remainder of this section, $V$ denotes an $\field$-vector space that is
    %
    \begin{enumerate}
        \item finite-dimensional,
        \item equipped with a $(\sigma,\epsilon)$-Hermitian form, and
        \item nonisotropic,
    \end{enumerate}
    %
    such that
    %
    \begin{enumerate}[resume]
        \item $\field$ is algebraically closed,
        \item $\sigma$ is an involution, and
        \item $\epsilon \neq 0$.
    \end{enumerate}
\end{assumption}
%
Note that since $V$ is finite-dimensional, we do not need to assume that the Riesz map $\Phi_V$ is a $\sigma$-isomorphism.


\begin{theorem}[The spectral theorem]
    \label{thm:spectral-theorem}
    An operator $T \in \calL(V)$ is normal if and only if it is orthogonally diagonalisable.
\end{theorem}

\begin{proof}
    First assume that $T$ is normal. We prove by induction in $n = \dim V$ that $T$ is orthogonally diagonalisable. If $n = 1$ then this follows since $T$ has an eigenvalue, so assume that the claim is proved for operators on spaces of dimension strictly less than $n$.

    Let $\lambda \in \spec T$, and consider the corresponding eigenspace $E_T(\lambda)$. If $d \defn \dim E_T(\lambda) = n$, then any orthogonal basis of $E_T(\lambda)$ will suffice (and such a basis exists by \cref{cor:orthogonal-basis-existence}). Assume therefore that $0 < d < n$.

    The space $E_T(\lambda) = E_{T^*}(\sigma(\lambda))$ is clearly invariant under both $T$ and $T^*$. It follows from \cref{lem:adjoint-invariant-subspace} that $E_T(\lambda)^\perp$ is also invariant under both $T$ and $T^*$. We furthermore have $\dim E_T(\lambda)^\perp = n-d$ and $0 < n-d < n$ by \cref{prop:nonsingular-space-decomposition}. Let $T_\parallel \in \calL(E_T(\lambda))$ and $T_\perp \in \calL(E_T(\lambda)^\perp)$ denote the restrictions of $T$ to $E_T(\lambda)$ and $E_T(\lambda)^\perp$ respectively. Both $T_\parallel$ and $T_\perp$ are also normal by \cref{lem:normal-operator-restriction-is-normal}, so the induction hypothesis furnishes orthogonal bases $\calU$ and $\calW$ for $E_T(\lambda)$ and $E_T(\lambda)^\perp$ consisting of eigenvectors of $T$. But then $\calV = \calU \union \calW$ is an orthogonal basis for $V$ as desired.

    Conversely, let $\calV$ be an orthogonal basis for $V$ consisting of eigenvectors for $T$, so that the matrix representation $\mr{\calV}{T}{\calV}$ is diagonal. Hence this commutes with $\mr{\calV}{T^*}{\calV}$, so $T$ and $T^*$ also commute.
\end{proof}



% \begin{corollary}
%     \label{cor:self-adjoint-unitary-eigenvalue-characterisation}
%     Let $T \in \calL(V)$ be a normal operator on a complex vector space $V$.
%     %
%     \begin{enumcor}
%         \item \label{enum:self-adjoint-eigenvalue-characterisation} $T$ is self-adjoint if and only if $\spec T \subseteq \reals$.
%         \item \label{enum:unitary-eigenvalue-characterisation} $T$ is unitary if and only if $\spec T \subseteq \sphere^1$.
%     \end{enumcor}
% \end{corollary}
% %
% Note that this does not hold on a real vector space, since then a normal operator is not necessarily diagonalisable.

% \begin{proof}
% \begin{proofsec*}
%     \item[\Namesubcref{enum:self-adjoint-eigenvalue-characterisation}]
%     The \enquote{only if} part follows from \cref{enum:self-adjoint-eigenvalues-exists-and-real}, so assume that $\spec T \subseteq \reals$ and notice that
%     %
%     \begin{equation*}
%         T^*
%             = \biggl( \sum_{\lambda \in \spec T} \lambda P_\lambda \biggr)^*
%             = \sum_{\lambda \in \spec T} \conj{\lambda} P_\lambda^*
%             = \sum_{\lambda \in \spec T} \lambda P_\lambda,
%     \end{equation*}
%     %
%     since each $\lambda \in \reals$, and each $P_\lambda$ is an orthogonal projection, hence self-adjoint.
    
%     Alternatively, choose a diagonal matrix $A \in \mat{n}{\fieldK}$ in accordance with \cref{enum:spectral-multiplication-operator}. Since the diagonal of $A$ contains the eigenvalues of $T$, we have $A^* = A$, and so it follows that $T^* = T$.

%     \item[\Namesubcref{enum:unitary-eigenvalue-characterisation}]
%     Similarly, the \enquote{only if} part is just \cref{enum:unitary-eigenvalues-unit-circle}. Assume that $\spec T \subseteq \sphere^1$ and notice that
%     %
%     \begin{equation*}
%         T^*
%             = \sum_{\lambda \in \spec T} \conj{\lambda} P_\lambda.
%     \end{equation*}
%     %
%     Since the projections $P_\lambda$ are pairwise orthogonal, we have
%     %
%     \begin{equation*}
%         T^*T
%             = \sum_{\lambda \in \spec T} \conj{\lambda} \lambda P_\lambda
%             = \sum_{\lambda \in \spec T} \abs{\lambda}^2 P_\lambda
%             = \sum_{\lambda \in \spec T} P_\lambda
%             = \id_V,
%     \end{equation*}
%     %
%     so $U$ is unitary.
    
%     Alternatively, let $A$ be as above. Then all diagonal elements in $A$ are nonzero, so $A$ is invertible, and we clearly have $A^* A = I_n$. Hence also $T^* T = \id_V$, so $T$ is unitary.
% \end{proofsec*}
% \end{proof}






\section{Coordinate representations}

\newpar

In the sequel we fix a finite-dimensional $\field$-vector space, and we also fix a $\sigma$-sesquilinear form $\inner{\,\cdot\,}{\,\cdot\,}$ on $V$. If $A = (a_{ij}) \in \mat{m,n}{\field}$ is a matrix, we denote by $A^\sigma$ the $n \prod m$ matrix whose $(i,j)$th element is $\sigma(a_{ji})$ and call this the \keyword{$\sigma$-conjugate} of $A$. That is, $A^\sigma$ is obtained from $A$ by transposition and entrywise application of $\sigma$. If $B \in \mat{n,k}{\field}$ is another matrix, notice that $(AB)^\sigma = B^\sigma A^\sigma$.

In the case where $\sigma = \id_V$, we recover the transpose $A\trans$, and when $\field = \complex$ and $\sigma$ is complex conjugation, $A^\sigma$ is the conjugate transpose.


\newpar\label{par:sesquilinear-form-matrix-representation}

Consider an ordered basis $\calV = (v_1, \ldots, v_n)$ for $V$, and let $u = \sum_{i=1}^n \alpha_i v_i$ and $v = \sum_{i=1}^n \beta_i v_i$ be vectors in $V$. Notice that
%
\begin{equation*}
    \inner{u}{v}
        = \sum_{i=1}^n \sum_{j=1}^n \sigma(\alpha_i) \inner{v_i}{v_j} \beta_j.
\end{equation*}
%
Hence if we define the matrix $\Sigma = (\inner{v_i}{v_j})_{i,j} \in \mat{n}{\field}$, then\footnote{Again we see the usefulness of sesquilinear forms being linear in the second entry: For then we simply $\sigma$-transpose the coordinate vector of the left argument and leave the coordinate vector of the right argument untouched.}
%
\begin{equation}
    \label{eq:sesquilinear-form}
    \inner{u}{v} =
    \begin{pmatrix}
        \sigma(\alpha_1) & \cdots & \sigma(\alpha_n)
    \end{pmatrix}
    \begin{pmatrix}
        \inner{v_1}{v_1} & \cdots & \inner{v_1}{v_n} \\
        \vdots & \ddots & \vdots \\
        \inner{v_n}{v_1} & \cdots & \inner{v_n}{v_n}
    \end{pmatrix}
    \begin{pmatrix}
        \beta_1 \\ \vdots \\ \beta_n
    \end{pmatrix}
        = \coordvec{v}{\calV}^\sigma \, \Sigma \, \coordvec{w}{\calV}.
\end{equation}
%
We call $\Sigma$ the \keyword{matrix representation} of the form $\inner{\,\cdot\,}{\,\cdot\,}$ with respect to the basis $\calV$. [TODO $\Sigma$ singular iff $V$ singular, right?]

Conversely, notice that after fixing a basis $\calV$, any square matrix $\Sigma$ gives rise to a sesquilinear form by the identity \cref{eq:sesquilinear-form}. Furthermore, such a matrix is uniquely determined (once a basis has been chosen), so there is a one-to-one correspondence between sesquilinear forms and square matrices, given a choice of basis. [TODO is there? Just double check.]


\newpar\label{par:sesquilinear-matrix-transformation}

Next we study how a matrix representation of transforms under a change of basis. If $\calW$ is another basis for $V$ then the results in [TODO ref] show that e.g. $\coordvec{v}{\calW} = \basischangemat{\calW}{\calV} \, \coordvec{v}{\calV}$. If $\Sigma$ and $\Gamma$ are the matrix representation of $\phi$ with respect to $\calV$ and $\calW$, respectively, then
%
\begin{equation*}
    \inner{u}{v}
        = \coordvec{u}{\calW}^\sigma \, \Gamma \, \coordvec{v}{\calW}
        = \coordvec{u}{\calV}^\sigma \, \basischangemat{\calW}{\calV}^\sigma \, \Gamma \, \basischangemat{\calW}{\calV} \, \coordvec{v}{\calV},
\end{equation*}
%
so $\Sigma = \basischangemat{\calW}{\calV}^\sigma \, \Gamma \, \basischangemat{\calW}{\calV}$ by uniqueness. Two matrices $A,B \in \mat{n}{\field}$ are said to be \keyword{$\sigma$-congruent} if there is an invertible matrix $P$ such that $A = P^\sigma BP$. We have thus shown that matrix representations of a form with respect to different bases are $\sigma$-congruent. [TODO converse, there exists a basis for which...]


\newpar

Now let $V$ and $W$ be finite-dimensional inner product spaces, and let $\Sigma \in \mat{n}{\fieldK}$ and $\Gamma \in \mat{m}{\fieldK}$ be the matrices of the inner products of $V$ and $W$ with respect to ordered bases $\calV$ and $\calW$, respectively. We then have the following characterisation of adjoints of linear maps $V \to M$.

\begin{proposition}
    \label{prop:adjoint-formula-IP-matrix}
    Let $T \colon V \to W$ be a linear map. Its adjoint $T^* \colon W \to V$ is the unique linear map satisfying
    %
    \begin{equation*}
        \Sigma \, \mr{\calV}{T^*}{\calW}
            = (\mr{\calW}{T}{\calV})^* \, \Gamma.
    \end{equation*}
\end{proposition}

\begin{proof}
    \Cref{prop:adjoint-inner-product} implies that
    %
    \begin{equation*}
        \coordvec{v}{\calV}^* \, \Sigma \, \mr{\calV}{T^*}{\calW} \, \coordvec{w}{\calW}
            = \inner{v}{T^* w}_V
            = \inner{Tv}{w}_W
            = \coordvec{v}{\calV}^* \, (\mr{\calW}{T}{\calV})^* \, \Gamma \, \coordvec{w}{\calW},
    \end{equation*}
    %
    for all $v \in V$ and $w \in W$, and $T^*$ is clearly unique with this property.
\end{proof}
%
This result has various important consequences in the case where $\calV$ and $\calW$ are orthonormal:

\begin{corollarynoproof}
    If $\calV$ and $\calW$ are orthonormal, then
    %
    \begin{equation*}
        \mr{\calV}{T^*}{\calW}
            = (\mr{\calW}{T}{\calV})^\sigma.
    \end{equation*}
\end{corollarynoproof}

\begin{corollary}
    \label{cor:adjoint-mr-orthogonal-basis}
    If $V = W$, and if $\calV$ is orthogonal and consists of nonisotropic vectors, then
    %
    \begin{equation*}
        \mr{\calV}{T^*}{\calV}
            = (\mr{\calV}{T}{\calV})^\sigma.
    \end{equation*}
\end{corollary}

\begin{proof}
    In this case $\Sigma = \Gamma$ is diagonal, so it commutes with $(\mr{\calV}{T}{\calV})^\sigma$. Its diagonal elements are also nonzero, so it is invertible, and hence
    %
    \begin{equation*}
        \mr{\calV}{T^*}{\calV}
            = \Sigma\inv (\mr{\calV}{T}{\calV})^\sigma \Sigma
            = \Sigma\inv \Sigma (\mr{\calV}{T}{\calV})^\sigma
            = (\mr{\calV}{T}{\calV})^\sigma,
    \end{equation*}
    %
    as claimed.
\end{proof}


\begin{corollarynoproof}
    If $T \colon \fieldK^n \to \fieldK^m$, then
    %
    \begin{equation*}
        \smr{T^*}
            = \smr{T}^*.
    \end{equation*}
\end{corollarynoproof}

\begin{corollarynoproof}
    If $A \in \mat{m,n}{\fieldK}$, then
    %
    \begin{equation*}
        M_{A^*}
            = (M_A)^*.
    \end{equation*}
\end{corollarynoproof}


\section{Projections II}\label{sec:projections-2}

\newpar

If $V$ is equipped with a sesquilinear form, then a projection $P \colon V \to V$ is \keyword{orthogonal} if $\im P$ and $\ker P$ are orthogonal subspaces of $V$.

In this section we make the same further assumptions as in \cref{par:Hilbert-space-adjoints}. In particular, operators on $V$ have Hilbert space adjoints. 

\begin{proposition}
    A projection $P \colon V \to V$ is orthogonal if and only if $P$ is self-adjoint.
\end{proposition}

\begin{proof}
    Assume that $P$ is orthogonal so that $\im P \perp \ker P$ and let $u,v \in V$. Since then $Pu \in \im P$ and $u - Pu \in \ker P$, and similarly for $v$, we get
    %
    \begin{equation*}
        \inner{u - Pu}{Pv}
            = 0
            = \inner{Pu}{v - Pv}.
    \end{equation*}
    %
    This implies that
    %
    \begin{equation*}
        \inner{u}{Pv}
            = \inner{Pu}{Pv}
            = \inner{Pu}{v}
            = \inner{u}{P^* v},
    \end{equation*}
    %
    which shows that $P = P^*$.

    Conversely assume that $P$ is self-adjoint. For $u \in \im P$ and $v \in \ker P$ we then have
    %
    \begin{equation*}
        \inner{u}{v}
            = \inner{Pu}{v}
            = \inner{u}{Pv}
            = \inner{u}{0}
            = 0,
    \end{equation*}
    %
    so $\im P$ and $\ker P$ are orthogonal.
\end{proof}


\newpar

TODO Next we consider finite-dimensional inner product spaces $V$ and $W$. If $U$ is a subspace of $V$, then the inclusion map $\iota_U \colon U \to V$ is injective and its image is $\im \iota_U$. Hence the following gives a formula for orthogonal projection operators onto any subspace:

\begin{proposition}
    \label{prop:projection-formula}
    Let $T \colon W \to V$ be an injective linear operator, and let $P$ be the orthogonal projection onto $\im T$. Then $P = T(T^* T)\inv T^*$.
\end{proposition}

\begin{proof}
    First note that $T^* T$ is indeed injective (hence invertible) since $T$ is. This follows from the identity $\ker T^* = (\im T)^\perp$. TODO prove this

    Next notice that the rank of $P$ is $\dim \im T$. But $T^*$ is surjective since $T$ is injective, so the rank of $T(T^* T)\inv T^*$ is also $\dim \im T$. It thus suffices to show that $P$ and $T(T^* T)\inv T^*$ agree on $\im T$, and writing $v = Tw$ we have
    %
    \begin{equation*}
        T(T^* T)\inv T^* v
            = T(T^* T)\inv (T^* T) w
            = Tw
            = v,
    \end{equation*}
    %
    as desired.
\end{proof}
%
Note that the proof does not go through in the infinite-dimensional case, for then we cannot simply use dimension arguments.

We specialise to the case where $V = \reals^n$, and the inner product on $\reals^n$ has the matrix representation $\Sigma$ with respect to the standard basis.\footnote{Note that the standard basis is not necessarily orthonormal with respect to the given inner product.} In this case we may also assume that $W = \reals^k$ where $k = \dim U$: Simply precompose $\iota_U$ with any isomorphism $\reals^k \to U$.

Furthermore, let $A \in \mat{n,k}{\reals}$ be the standard matrix representation of $T \colon \reals^k \to \reals^n$. In this case, $U$ is of course the column space of $A$, and $A$ is of full rank. We then have the following result:

\begin{proposition}
    The orthogonal projection $P$ onto $R(A)$ is given by
    %
    \begin{equation*}
        \smr{P}
            = A(A\trans \Sigma A)\inv A\trans \Sigma.
    \end{equation*}
\end{proposition}

\begin{proof}
    Equip $\reals^k$ with the standard inner product. Its matrix representation with respect to the standard basis is then just the identity matrix, so \cref{prop:adjoint-formula-IP-matrix} implies that
    %
    \begin{equation*}
        \smr{T^*}
            = \smr{T}\trans \Sigma
            = A\trans \Sigma.
    \end{equation*}
    %
    Applying this to \cref{prop:projection-formula} we thus obtain
    %
    \begin{align*}
        \smr{P}
            &= A \bigl( \smr{T^*} A \bigr)\inv \smr{T^*} \\
            &= A (A\trans \Sigma A)\inv A\trans \Sigma,
    \end{align*}
    %
    as claimed.
\end{proof}


\section{Real and complex sesquilinear forms}

The spectral theorem in the form stated in TODO ref applies directly to inner products on complex vector spaces, but there is of course also a version of the spectral theorem for real vector spaces. In order to formulate it in the desired generality, we first study how to extend the constructions developed in this chapter from real to complex vector spaces.

\newpar

We first consider how to extend a field automorphism on $\reals$ to an automorphism on $\complex$. Note that since an endomorphism on $\reals$ is in particular an $\reals$-linear map, it is either surjective or the zero map. But the latter is impossible since it must send $1$ to $1$, so every field endomorphism is automatically an automorphism.

\begin{lemma}
    Let $\sigma \in \Aut(\reals)$. Then the maps $\sigma^\complex_{\pm} \colon \complex \to \complex$ given by
    %
    \begin{equation*}
        \sigma^\complex_{\pm}(\alpha + \iu \beta)
            = \sigma(\alpha) \pm \iu \sigma(\beta)
    \end{equation*}
    %
    for $\alpha,\beta \in \reals$ are the only endomorphisms on $\complex$ that extend $\sigma$, and they are also automorphisms. Furthermore, $\sigma$ is an involution if and only if the $\sigma^\complex_\pm$ are.
\end{lemma}

\begin{proof}
    It is easy to see that they are in fact field homomorphisms, hence $\complex$-linear and thus bijective. To prove uniqueness, let $\tau \colon \complex \to \complex$ be a field homomorphism extending $\sigma$. For $\alpha,\beta \in \reals$ we then have
    %
    \begin{equation*}
        \tau(\alpha + \iu \beta)
            = \tau(\alpha) + \tau(\iu) \tau(\beta)
            = \sigma(\alpha) + \tau(\iu) \sigma(\beta).
    \end{equation*}
    %
    But since $\iu^2 = -1$, we have
    %
    \begin{equation*}
        \tau(\iu)^2
            = \tau(\iu^2)
            = \tau(-1)
            = -1,
    \end{equation*}
    %
    so $\tau(\iu) \in \{\pm \iu\}$. The final claim is obvious.
\end{proof}
%
We will see shortly which of $\sigma^\complex_+$ and $\sigma^\complex_-$ is the right choice.

Next we must consider how to extend sesquilinear forms from a real vector space $V$ to its complexification $V^\complex$. There is of course a canonical way of extending inner products as mentioned in TODO ref, so we extend general sesquilinear forms to be consistent with that procedure: Namely, if $\phi$ is a sesquilinear form on $V$, then we define $\phi^\complex \colon V^\complex \prod V^\complex \to \complex$ by
%
\begin{equation*}
    \phi^\complex(u + \iu v, x + \iu y)
        = \phi(u,x) + \phi(v,y) + \iu \bigl( \phi(u,y) - \phi(v,x) \bigr),
\end{equation*}
%
for $u,v,x,y \in V$. It is easy to see that this is bi-additive and $\complex$-linear in its second entry, and that it agrees with $\phi$ on $V \prod V$. But assume that $\phi$ is a $\sigma$-sesquilinear form. Then we might hope that $\phi^\complex$ is either a $\sigma^\complex_+$- or a $\sigma^\complex_-$-sesquilinear form. But a short calculation shows that it is indeed $\sigma^\complex_-$-sesquilinear. This we might have expected, since the canonical automorphism on $\complex$ is complex conjugation which precisely sends $\iu$ to $-\iu$.

\begin{proposition}
    \begin{enumprop}
        \item $\phi$ is nontrivial if and only if $\phi^\complex$ is nontrivial.
        % \item TODO reflexive?
        \item $\phi$ is $(\sigma,\epsilon)$-Hermitian if and only if $\phi^\complex$ is $(\sigma^\complex_-,\epsilon)$-Hermitian.
    \end{enumprop}
\end{proposition}

\begin{proof}
    TODO
\end{proof}


\begin{proposition}
    TODO Isotropic?
\end{proposition}


\newpar

The trick is to take the complexification $T^\complex$ of a self-adjoint operator $T$ on a real vector space. We first show that $T^\complex$ is then also self-adjoint:

\begin{proposition}
    \label{prop:complexification-adjoint}
    Let $V$ and $W$ be real Hilbert spaces, and let $T \in \calB(V,W)$. Then we have
    %
    \begin{equation*}
        (T^\complex)^*
            = (T^*)^\complex,
    \end{equation*}
    %
    i.e., the adjoint of the complexification of $T$ is the complexification of the adjoint of $T$. In particular
    %
    \begin{enumprop}
        \item $T$ is normal if and only if $T^\complex$ is normal, and
        \item $T$ is self-adjoint if and only if $T^\complex$ is self-adjoint.
    \end{enumprop}
\end{proposition}

\begin{proof}
    For $v,u,x,y \in V$ we have
    %
    \begin{align*}
        \inner{(T^*)^\complex(x + \iu y)}{v + \iu u}
            &= \inner{ T^*x + \iu T^*y }{v + \iu u} \\
            &= \inner{T^*x}{v}
                + \inner{T^*y}{u}
                + \iu ( \inner{T^*x}{u} - \inner{T^*y}{v} ) \\
            &= \inner{x}{Tv}
                + \inner{y}{Tu}
                + \iu ( \inner{x}{Tu} - \inner{y}{Tv} ) \\
            &= \inner{x + \iu y}{Tv + \iu Tu} \\
            &= \inner{x + \iu y}{T^\complex(v + \iu u)}.
    \end{align*}
    %
    Uniqueness of adjoints thus yields the claim.

    Assume that $T$ is normal. Then
    %
    \begin{equation*}
        T^\complex (T^\complex)^*
            = T^\complex (T^*)^\complex
            = (TT^*)^\complex
            = (T^*T)^\complex
            = (T^*)^\complex T^\complex
            = (T^\complex)^* T^\complex,
    \end{equation*}
    %
    so $T^\complex$ is normal. The converse follows similarly. If $T$ is self-adjoint, then
    %
    \begin{equation*}
        (T^\complex)^*
            = (T^*)^\complex
            = T^\complex,
    \end{equation*}
    %
    and similarly if $T^\complex$ is self-adjoint.
\end{proof}


\newpar

As promised we prove a polarisation identity for certain sesquilinear forms on complex vector spaces. Let $V$ be a complex vector space, and denote complex conjugation by $\alpha \mapsto \alpha^*$. Then a ${}^*$-sesquilinear form is just a form that is conjugate-linear in its first entry. Its quadratic form $Q$ then satisfies $Q(\alpha v) = \abs{\alpha}^2 Q(v)$ for all $v \in V$ and $\alpha \in \complex$.

\begin{propositionnoproof}
    Let $\phi$ be a ${}^*$-sesquilinear form on $V$ with quadratic form $Q$. Then
    %
    \begin{equation*}
        \phi(u,v)
            = \frac{1}{4} \sum_{k=0}^3 \iu^k Q(\iu^k u + v)
    \end{equation*}
    %
    for all $u,v \in V$.
\end{propositionnoproof}
%
As in the proof of TODO ref, this follows simply by inserting the definition of $Q$.

The usual complex inner product is in fact also $({}^*,1)$-Hermitian, and this property allows us to give a different proof of TODO ref which illustrates the connection between the polarisation identity for bilinear forms and the one for ${}^*$-sesquilinear forms. The first step is to notice that whether or not a form is $({}^*,1)$-Hermitian is determined by its quadratic form:
%
\begin{lemma}
    Let $\phi$ be a ${}^*$-sesquilinear form on $V$ with quadratic form $Q$. Then $\phi$ is $({}^*,1)$-Hermitian if and only if $Q(v) \in \reals$ for all $v \in V$.
\end{lemma}

\begin{proof}
    If $\phi$ is $({}^*,1)$-Hermitian, then
    %
    \begin{equation*}
        \conj{Q(v)}
            = \conj{\phi(v,v)}
            = \phi(v,v)
            = Q(v).
    \end{equation*}
    %
    Conversely, if $Q$ is real-valued then
    %
    \begin{equation*}
        0
            = \Im Q(u+v)
            = \Im \phi(u,v) + \Im \phi(v,u)
    \end{equation*}
    %
    and
    %
    \begin{equation*}
        0
            = \Im Q(u+\iu v)
            = - \Re \phi(u,v) + \Re \phi(v,u)
    \end{equation*}
    %
    for all $u,v \in V$, implying that $\phi$ is $({}^*,1)$-Hermitian.
\end{proof}
%
Now notice that if $\phi$ is ${}^*$-sesquilinear and $({}^*,1)$-Hermitian, then the quadratic forms corresponding to $\phi$ and $\Re \phi$ coincide since they are real-valued. It thus follows from TODO ref that
%
\begin{equation*}
    \Re \phi(u,v)
        = \frac{1}{4} \bigl( Q(u+v) - Q(u-v) \bigr)
\end{equation*}
%
for all $u,v \in V$. Further notice that for $u \in V$ the map $v \mapsto \phi(u,v)$ is a linear functional, and hence on the form [TODO ref]
%
\begin{align*}
    \phi(u,v)
        &= \Re \phi(u,v) - \iu \Re \phi(u, \iu v) \\
        &= \Re \phi(u,v) + \iu \Re \phi(\iu u, v) \\
        &= \frac{1}{4} \bigl( Q(u+v) - Q(u-v) + \iu Q(\iu u + v) - \iu Q(\iu u - v) \bigr) \\
        &= \frac{1}{4} \bigl( Q(u+v) - Q(-u+v) + \iu Q(\iu u + v) - \iu Q(-\iu u + v) \bigr),
\end{align*}
%
where we have used that $Q(-w) = Q(w)$. And this is precisely TODO ref.





\chapter{TODO lambda calculus}

\newcommand{\varset}{\mathit{Vars}}
\newcommand{\vars}{\mathit{V}}
\newcommand{\freevars}{\mathit{FV}}
\newcommand{\boundvars}{\mathit{BV}}
\newcommand{\subterm}{\mathrm{Sub}}
\newcommand{\syntaxdefn}{\vcentcolon\equiv}


\section{Syntax}

\newpar

Let $\varset$ denote a countable collection of variables. Along with these variables, the alphabet $\Sigma$ also contains the lambda symbol \textquote{$\lambda$}, the full stop \textquote{.}, and the left and right parentheses \textquote{(} and \textquote{)}. If $x$ and $y$ are words over $\Sigma$, then we write $x \equiv y$ to denote that $x$ and $y$ contain the same symbols, i.e. that they are \keyword{syntactically equal}. The notation $M \syntaxdefn N$ means that we define $M$ to be syntactically equal to $N$. We will define different types of equality between terms of the lambda calculus below, hence the perhaps strange disuse of the equals sign.

\begin{definition}[$\lambda$-terms]
    The collection $\Lambda$ of \keyword{$\lambda$-terms} is the smallest collection of words over $\Sigma$ satisfying the following:
    %
    \begin{enumdef}
        \item If $x \in \varset$, then $x \in \Lambda$.
        \item If $M,N \in \Lambda$, then $(MN) \in \Lambda$.
        \item If $x \in \varset$ and $M \in \Lambda$, then $(\lambda x.M) \in \Lambda$.
    \end{enumdef}
\end{definition}
%
Here we fall into the usual abuse of notation of mixing object and meta language symbols. Being precise for a moment, if \textquote{$x$} is a variable (that is, the symbol itself is a variable), then \textquote{$\lambda x . xx$} is a $\lambda$-term. If the metavariable \textquote{$M$} denotes the term \textquote{$xx$}, then we we will usually write \textquote{$\lambda x . M$} for \textquote{$\lambda x . xx$}. That is, the $\lambda$-term beginning with \textquote{$\lambda x .$}, followed by $M$.

We also omit parentheses whenever it does not lead to confusion. Furthermore, application is left-associative, and we define $MNL \syntaxdefn (MN)L$. Similarly, abstraction is right-associative, and we use the shorthand
%
\begin{equation*}
    \lambda x_1 x_2 \cdots x_n . M
        \syntaxdefn (\lambda x_1 ( \lambda x_2 (\cdots (\lambda x_n . M)))).
\end{equation*}


\newpar

The collection of \keyword{free variables} in a $\lambda$-term is defined by
%
\begin{align*}
    \freevars(x) &= \{x\}, \\
    \freevars(MN) &= \freevars(M) \union \freevars(N), \\
    \freevars(\lambda x . M) &= \freevars(M) \setminus \{x\}.
\end{align*}
%
Strictly speaking, for this definition to make sense we would need to show that $\lambda$-terms satisfy \keyword{unique parsing}, i.e. that a $\lambda$-term can be on exactly one of the forms $x$, $(MN)$ or $(\lambda x . M)$. Furthermore, the value of $\freevars$ on some term $M$ is defined in terms of the value that $\freevars$ takes on terms that contain strictly fewer symbols than $M$. Similarly, the collection of \keyword{bound variables} is defined by
%
\begin{align*}
    \boundvars(x) &= \emptyset, \\
    \boundvars(MN) &= \boundvars(M) \union \boundvars(N), \\
    \boundvars(\lambda x . M) &= \boundvars(M) \union \{x\}.
\end{align*}
%
Notice that $\freevars(M)$ and $\boundvars(M)$ are not necessarily disjoint. The collection of all variables occurring in a $\lambda$-term $M$ is clearly given by $\freevars(M) \union \boundvars(M)$ and is denoted $\vars(M)$.


\newpar

Finally, we define the set of \keyword{subterms} of a $\lambda$-term:
%
\begin{align*}
    \subterm(x) &= \{x\}, \\
    \subterm(MN) &= \subterm(M) \union \subterm(N) \union \{(MN)\}, \\
    \subterm(\lambda x . M) &= \subterm(M) \union \{(\lambda x . M)\}.
\end{align*}


\section{Reduction and equivalence of $\lambda$-terms}

\newpar

\newcommand{\dom}{\operatorname{dom}}

We first review the basics of binary relations. If $X$ and $Y$ are sets, then a \keyword{(binary) relation} from $X$ to $Y$ is a subset $R \subseteq X \times Y$. If $(x,y) \in R$ then we also write $xRy$. We will later need the \keyword{domain of definition} of $R$, defined as the set
%
\begin{equation*}
    \dom R
        = \set{x \in X}{\exists y \in Y: (x,y) \in R}.
\end{equation*}
%
The \keyword{converse relation} of $R$ is the relation $R\trans \subseteq Y \times X$ with the property that $y R\trans x$ just when $xRy$. Given relations $R_i \subseteq X \times Y$ for $i$ in some index set $I$, it is easy to see that $(\bigunion_{i \in I} R_i)\trans = \bigunion_{i \in I} R_i\trans$. We also write $R_1 R_2$ for the union $R_1 \union R_2$.

If $S \subseteq Y \times Z$ is another relation, then the composition of $R$ with $S$ is given by
%
\begin{equation*}
    S \circ R
        = \set{(x,z) \in X \times Z}{\exists y \in Y \colon xRy \text{ and } ySz}.
\end{equation*}
%
We see that $(S \circ R)\trans = R\trans \circ S\trans$. The \keyword{diagonal relation} on $X$ is given by $\Delta_X = \set{(x,x)}{x \in X}$. Notice that $\Delta_X$ is just equality, so it is in particular an equivalence relation. If $Y = X$, then we define $R^0 = \Delta_X$ and $R^n = R^{n-1} \circ R$ for $n \in \naturals$. Finally we let\footnote{Note that $0 \not\in \naturals$.} $R^+ = \bigunion_{n \in \naturals} R^n$.

If $R \in X \times X$, then the \keyword{reflexive closure} of $R$ is the smallest relation on $X$ that is reflexive and contains $R$. We define the symmetric and transitive closures similarly. It is then easy to see that
%
\begin{equation*}
    R_r
        = R \union \Delta_X,
    \quad
    R_s
        = R \union R\trans,
    \quad \text{and} \quad
    R_t
        = R^+,
\end{equation*}
%
are the reflexive, symmetric and transitive closures of $R$, respectively. From these definitions it is easy to show that $(R_r)_s = (R_s)_r$, and that $R_t$ is reflexive/symmetric if $R$ is reflexive/symmetric.


\newpar

If $R$ is a binary relation on $\Lambda$, there is another desirable property $R$ may have:

\begin{definition}[Compatibility]
    The relation $R$ is \keyword{compatible} if for $M,N,L \in \Lambda$ and $x \in \varset$, $(M,N) \in R$ implies that
    %
    \begin{enumdef}
        \item $(ML, NL) \in R$,
        \item $(LM, LN) \in R$, and
        \item $(\lambda x . M, \lambda x . N) \in R$.
    \end{enumdef}
    %
    If $R$ is compatible, reflexive and transitive, then we call it a \keyword{reduction}.
\end{definition}
%
TODO weak extensionality.

The \keyword{compatible closure} of a relation $R$ is the smallest compatible relation containing $R$, denoted $R_\lambda$. This clearly exists since the intersection of a collection of compatible relations is itself compatible. We may also give an explicit characterisation of $R_\lambda$: First let $S_0 = R$ and recursively define
%
\begin{equation*}
    S_n
        = S_{n-1} \union \set{ (ML, NL), (LM, LN), (\lambda x.M, \lambda x.N) }{(M,N) \in S_{n-1}}
\end{equation*}
%
for $n \in \naturals$. It is then easy to see that $R_\lambda = \bigunion_{n \in \naturals} S_n$.


\newpar

If $R$ is any binary relation on $\Lambda$ that we think of as providing a way of \textquote{reducing} terms, then we call $R$ a \keyword{notion of reduction}. The compatible closure $R_\lambda$ is then called \keyword{one-step $R$-reduction} and is denoted $\to_R$. The reflexive and transitive closure of $\to_R$ is simply called \keyword{$R$-reduction} and is denoted $\twoheadrightarrow_R$. Instead taking the equivalence closure of $\to_R$ yields a relation $=_R$ called \keyword{$R$-equivalence}. Notice that if we instead take the symmetric closure of $\twoheadrightarrow_R$, then the resulting relation is not necessarily transitive. Afterwards taking the transitive closure we again obtain the relation $=_R$.

\begin{lemma}
    If $R$ is compatible, then so are $\to_R$, $\twoheadrightarrow_R$ and $=_R$.
\end{lemma}

\begin{proof}
    It suffices to show that the closures $R_r$, $R_s$ and $R_t$ are also compatible. For the reflexive closure $R_r$, this is obvious. For the symmetric closure $R_s$, let $(M,N) \in R_s = R \union R\trans$. If $(M,N) \in R$ then we clearly have e.g. $(ML,NL) \in R$. If instead $(M,N) \in R\trans$, then $(N,M) \in R$ which implies that $(NL,ML) \in R \subseteq R_s$. The other properties are proved similarly.

    For the transitive closure $R_t$, simply notice that $R^n$ is compatible (which is easily shown by induction).
\end{proof}


\begin{definition}
    Let $R$ be a notion of reduction. An element of $\dom R$ is called an \keyword{$R$-redex}. If $M \in \Lambda$ and no subterm of $M$ is an $R$-redex, then $M$ is an \keyword{$R$-normal form}. If also $N \in \Lambda$ and $M =_R N$, then we say that $M$ is an $R$-normal form of $N$.
\end{definition}
%
The following lemma is intuitively very obvious, but it is relatively difficult to prove rigorously, so we omit the proof:

\begin{lemmanoproof}
    \label{lem:normal-form-lemma}
    Let $M$ be an $R$-normal form. Then $M \twoheadrightarrow_R N$ implies that $M \equiv N$. In other words, there is no $N$ such that $M \to_R N$.
\end{lemmanoproof}


\begin{definition}[The diamond property]
    A binary relation $\vartriangleright$ on $\Lambda$ is said to satisfy the \keyword{diamond property} if $M \vartriangleright N_1$ and $M \vartriangleright N_2$ implies the existence of an $L$ such that $N_1 \vartriangleright L$ and $N_2 \vartriangleright L$.
\end{definition}
%
Put diagrammatically, if we represent the relationship $M \vartriangleright N$ with an arrow $M \to N$:
%
\begin{equation*}
    \begin{tikzcd}[column sep=small, row sep=small]
        & N_1
            \ar[dr] \\
        M
            \ar[ur]
            \ar[dr]
        && L \\
        & N_2
            \ar[ur]
    \end{tikzcd}
\end{equation*}

\begin{definition}[Church--Rosser]
    A notion of reduction $R$ is said to be \keyword{Church--Rosser} if $\twoheadrightarrow_R$ has the diamond property.
\end{definition}


\begin{theorem}[The Church--Rosser theorem]
    \label{thm:Church-Rosser}
    If $R$ is Church--Rosser and $M =_R N$, then there exists an $L \in \Lambda$ such that
    %
    \begin{equation*}
        M \twoheadrightarrow_R L
        \quad \text{and} \quad
        N \twoheadrightarrow_R L
    \end{equation*}
\end{theorem}

\begin{proof}
    This follows easily by structural induction on the second definition of $=_R$ as the symmetric and transitive closure of $\twoheadrightarrow_R$.
\end{proof}


\begin{corollary}
    If $R$ is Church--Rosser and $N$ is an $R$-normal form of $M$, then $M \twoheadrightarrow_R N$.
\end{corollary}

\begin{proof}
    By \cref{thm:Church-Rosser} there is an $L$ such that $M \twoheadrightarrow_R L$ and $N \twoheadrightarrow_R L$. But since $N$ is an $R$-normal form, \cref{lem:normal-form-lemma} implies that $N \equiv L$.
\end{proof}


\begin{corollary}
    If $R$ is Church--Rosser, then a term can have at most one $R$-normal form.
\end{corollary}

\begin{proof}
    If $N_1$ and $N_2$ are both $R$-normal forms of $M$, then $N_1 =_R M =_R N_2$. \Cref{thm:Church-Rosser} then yields an $L$ to which both normal forms reduce, but then $N_1 \equiv L \equiv N_2$ by \cref{lem:normal-form-lemma}.
\end{proof}


\section{$\alpha$-equivalence and substitution}

\begin{definition}[Renaming]
    Let $M,N \in \Lambda$ and $x,y,z \in \varset$. Define the \keyword{renaming} operation $M \mapsto M^{x \to y}$ on $\Lambda$ by
    %
    \begin{alignat*}{2}
        x^{x \to y} &\syntaxdefn y, && \\
        z^{x \to y} &\syntaxdefn z && \quad \text{if $z \not\equiv x$}, \\
        (MN)^{x \to y} &\syntaxdefn M^{x \to y} N^{x \to y}, && \\
        (\lambda x.M)^{x \to y} &\syntaxdefn \lambda x.M, && \\
        (\lambda z.M)^{x \to y} &\syntaxdefn \lambda z.(M^{x \to y}) && \quad \text{if $z \not\equiv x$}.
    \end{alignat*}
\end{definition}
%
The renaming operation replaces all \textquote{free occurrences} of $x$ with $y$. This allows us to define the following: Let
%
\begin{equation*}
    \alpha
        = \set{(\lambda x.M, \lambda y.M^{x \to y})}{M \in \Lambda, x \in \varset, y \in \varset \setminus V(M)}.
\end{equation*}
%
The corresponding equivalence relation $=_\alpha$ is of course called \keyword{$\alpha$-equivalence}. This is supposed to capture changes of bound variables, since the particular choice of bound variables is immaterial. When performing such a change, we require that the new variable $y$ does not already occur in the body $M$ of the term. If it occurred in $M$ as a free variable, then the new binding $\lambda y$ would inadvertently bind these free occurrences. On the other hand, if $Y$ occurred in $M$ as a bound variable, then renaming a free $x$ to $y$ would also inadvertently bind the renamed variable.

With $\alpha$-equivalence at our disposal, we can now define general substitution of terms in place of variables:

% \begin{definition}[$\alpha$-equivalence]
%     Let \keyword{$\alpha$-equivalence}, denoted $=_\alpha$, be the smallest equivalence relation on $\Lambda$ satisfying the following properties:
%     %
%     \begin{enumdef}
%         \item For $M \in \Lambda$, $x \in \varset$ and $y \in \varset \setminus \vars(M)$, $\lambda x.M =_\alpha \lambda y.M^{x \to y}$.
%         \item For $M,N,L \in \Lambda$ and $z \in \varset$, if $M =_\alpha N$ then $ML =_\alpha NL$, $LM =_\alpha LN$ and $\lambda z.M =_\alpha \lambda z.N$.
%     \end{enumdef}
%     %
%     If $M =_\alpha N$, then $M$ and $N$ are said to be \keyword{$\alpha$-equivalent}.
% \end{definition}
% %
% Notice that if $M \equiv N$, then in particular $M =_\alpha N$, since $=_\alpha$ is reflexive.

\begin{definition}[Substitution]
    Let $M,M_1,M_2,N \in \Lambda$ and $x,y,z \in \varset$. We define the \keyword{substitution} operation $M \mapsto M[x \defn N]$ on $\Lambda$ by
    %
    \begin{align*}
        x[x \defn N] &\syntaxdefn N, \\
        z[x \defn N] &\syntaxdefn z \quad \text{if $z \not\equiv x$}, \\
        (M_1 M_2)[x \defn N] &\syntaxdefn M_1[x \defn N] M_2[x \defn N],
        \intertext{and we further define}
        (\lambda y.M)[x \defn N] &\syntaxdefn \lambda z.(M^{y \to z}[x \defn N]),
    \end{align*}
    %
    if $\lambda z.M^{y \to z} =_\alpha \lambda y.M$ and $z \not\in \freevars(N)$.
\end{definition}
%
Note that the last definition is not strictly speaking well-defined, since different choices of $z$ lead to different expressions. This is of course no problem in practice (not least because we only consider terms up to $\alpha$-equivalence), but if we really wanted to, we could e.g. index the variables with the natural numbers (since there are countably many variables) and require that $z$ be the variable not occurring in either $\lambda y . M$ or $N$ with the lowest index. Notice also that these terms contain finitely many variables, so there is in fact such a $z$.


\section{$\beta$-reduction}

Until now the interpretation of $\lambda$-terms as functions has not been used to apply functions to arguments. To include this interpretation in the theory, we need some way of identifying the application of a function to an argument with the result of this application. We do this using the notion of reduction
%
\begin{equation*}
    \beta
        = \set[\big]{ \bigl( (\lambda x . M)N, M[x \defn N] \bigr) }{x \in \varset \text{ and } M,N \in \Lambda}.
\end{equation*}
%
That is, the one-step reduction $\to_\beta$ replaces every free occurrence of $x$ in $M$ with the argument $N$. Notice that by definition of substitution, it may happen that we rename bound variables in $M$ when applying $\lambda x . M$ to $N$. Hence it makes little sense to consider $\beta$-equivalence without also identifying terms that are $\alpha$-equivalent. We thus arrive at the usual notion of equivalence between $\lambda$-terms, namely $\alpha\beta$-equivalence (recalling that we write $\alpha\beta = \alpha \union \beta$). Instead of \textquote{$=_{\alpha\beta}$} for the resulting equivalence relation, we simply write \textquote{$=$}.

Not only is $\beta$-reduction a natural notion of reduction, it is also very well-behaved. The following theorem is fairly nontrivial, so we omit the proof:

\begin{theoremnoproof}
    $\beta$ is Church--Rosser.
\end{theoremnoproof}


\newpar

If we consider $\alpha$-equivalence as a way of identifying different terms, then this induces a notion of equality on terms that is \keyword{intensional}. That is, two terms are $\alpha$-equivalent if they somehow mean the same. This is intuitive enough, since all $\alpha$-equivalence does is allow us to rename bound variables.

But notice that there are terms that are not $\alpha$-equivalent, but that we usually would identify anyway. If the variable $x$ is not free in the term $M$, then we would usually identify $\lambda x . Mx$ and $M$: For the former is just the function that applies $M$ to its argument. Despite this, since we cannot obtain one by renaming bound variables in the other, they are not $\alpha$-equivalent. If we wish to identify terms not solely by their intension but also by their \keyword{extension}, i.e. the values that they take on different arguments, then we add the relation
%
\begin{equation*}
    \eta
        = \set{(\lambda x . Mx, M)}{M \in \Lambda, x \in \varset \setminus \freevars(M)}
\end{equation*}
%
and consider instead $\alpha\beta\eta$-equivalence. But this is not usually done.




% Since $\alpha$-equivalent subterms can usually be substituted for each other while preserving $\alpha$-equivalence of the whole term, it is usual to identify $\alpha$-equivalent terms. This is known as the \keyword{Barendregt convention}, though Barendregt himself attributes it to James Ottman.


% Hence instead of \textquote{$=_\alpha$} we simply write \textquote{$=$}. Notice that this is a form for \keyword{intensional} equality, since it identifies 


% %
% \begin{prooftree}
%     \AxiomC{}
%     \UnaryInfC{$M = M$}
% \end{prooftree}
% %
% \begin{prooftree}
%     \AxiomC{$M = N$}
%     \UnaryInfC{$N = M$}
% \end{prooftree}
% %
% \begin{prooftree}
%     \AxiomC{$M = N$}
%     \AxiomC{$N = L$}
%     \BinaryInfC{$M = L$}
% \end{prooftree}
% %
% Reflexivity in particular implies that if $M \equiv N$, then $M = N$. Next, equality should respect the structure of terms:
% %
% \begin{prooftree}
%     \AxiomC{$M = N$}
%     \UnaryInfC{$ML = NL$}
% \end{prooftree}
% %
% \begin{prooftree}
%     \AxiomC{$M = N$}
%     \UnaryInfC{$LM = LN$}
% \end{prooftree}
% %
% \begin{prooftree}
%     \AxiomC{$M = N$}
%     \RightLabel{$(\xi)$}
%     \UnaryInfC{$\lambda x.M = \lambda x.N$}
% \end{prooftree}
% %
% The inference rule $(\xi)$ is known as \keyword{weak extensionality}. This 

% \begin{prooftree}
%     \AxiomC{$y \not\in \vars(M)$}
%     \RightLabel{$(\alpha)$}
%     \UnaryInfC{$\lambda x.M = \lambda x.N$}
% \end{prooftree}

\setcounter{chapter}{1}

\chapter{Foundations}

\section{Absolute Values on a Field}

\begin{problem}[31]
    Let $\fieldK$ be a finite field. Show that the only absolute value on $\fieldK$ is the trivial absolute value
\end{problem}

\begin{solution}
    Let $x \in \fieldK \setminus \{0,1\}$. Then $x^n = 1$ for some $n \in \ints$, so $1 = \abs{x^n} = \abs{x}^n$. But then $\abs{x} = 1$.
\end{solution}


\begin{problem}[36]
    Let $A$ be an integral domain, and let $K$ be its field of fractions. Let $v \colon A \setminus \{0\} \to \reals$ be a valuation on $A$. Extend $v$ to $K \setminus \{0\}$ by setting $v(a/b) = v(a) - v(b)$. Show that the function $\abs{\,\cdot\,}_v \colon K \to \reals^+$ defined by
    %
    \begin{equation*}
        \abs{x}_v = \e^{-v(x)}
        \quad \text{for $x \neq 0$}
    \end{equation*}
    %
    and $\abs{0}_v = 0$ is a non-archimedean absolute value on $K$. Conversely, show that if $\abs{\,\cdot\,}$ is a non-archimedean absolute value, then $-\log\abs{\,\cdot\,}$ is a valuation.
\end{problem}

\begin{solution}
    Most of this is obvious. We show that the extension of $v$ to $K \setminus \{0\}$ is well-defined. If $a/b = c/d$ then $ad = bc$, and so
    %
    \begin{equation*}
        v(a) + v(d)
            = v(ad)
            = v(bc)
            = v(b) + v(c),
    \end{equation*}
    %
    which implies that $v(a) - v(b) = v(c) - v(d)$.
\end{solution}


\begin{problem}[37]
    Let $v \colon \fieldK^* \to \reals$ be a valuation. Show that the image of $v$ is an additive subgroup of $\reals$. This is sometimes called the \keyword{value group} of the valuation $v$. What is the value group of the $p$-adic valuation?
\end{problem}

\begin{solution}
    Since $v(xy) = v(x) + v(y)$ for all $x,y \in \fieldK^*$, $v$ is a group homomorphism from the multiplicative group $\fieldK^*$ to the additive group $(\reals,+)$. Hence its image is an additive subgroup of $\reals$.

    For the $p$-adic valuation $v_p$, $v_p(\ints \setminus \{0\}) = \naturals_0$, so $v_p(\rationals^*) = \ints$.
\end{solution}


\begin{problem}[49]
    Show that if $C = \sup \set{\abs{n}}{n \in \ints} < \infty$, then $\abs{\,\cdot\,}$ is non-archimedean, and $C = 1$.
\end{problem}

\begin{solution}
    We have $\abs{1} = 1$, so $C \geq 1$. If there is some $n \in \ints$ with $\abs{n} > 1$, then the powers of $n$ are unbounded. Hence we must have $\abs{n} \leq 1$. That $\abs{\,\cdot\,}$ is non-archimedean follows from Theorem~2.2.4.
\end{solution}


\addtocounter{section}{1}
\section{Topology}

\begin{problem}[56]
    Describe the closed ball of radius $1$ around the point $x = 0$ in $\rationals$ with respect to the $p$-adic absolute value. Describe the open ball of radius $1$ around $x = 3$; which integers belong to this ball?
\end{problem}

\begin{solution}
    We have $y \in \closure{B}(0,1)$ if and only if $p^{-v_p(y)} = \abs{y}_p \leq 1$. This is the case just when $v_p(y) \geq 0$, i.e. when $y = m/n$ (with $m$ and $n$ coprime) and $p \not\mid n$. For instance, $\closure{B}(0,1)$ contains all the integers.

    Similarly, we have $y \in B(3,1)$ if and only if $v_p(y-3) > 1$, i.e. when $y-3 = m/n$ (with $m$ and $n$ coprime) and $p \mid m$ and $p \not\mid n$. For instance, an integer $k$ lies in $B(3,1)$ just when $p \mid k-3$, i.e. when $k \equiv_p 3$.
\end{solution}


\begin{problem}[57]
    Let $\fieldK = \rationals$ and $\abs{\,\cdot\,} = \abs{\,\cdot\,}_p$. Show that the closed ball $\closure{B}(0,1)$ can be written as a disjoint union of open balls
    \begin{equation*}
        \closure{B}(0,1)
            = B(0,1) \union B(1,1) \union \cdots \union B(p-1,1).
    \end{equation*}
\end{problem}

\begin{solution}
    To show that the open balls are disjoint, it suffices by Proposition~2.3.7 to show that each ball has points not contained in the others. So let $k' \in \{0, 1, \ldots, p-1\}$. Then $k' \in B(k,1)$ just when $p \mid k-k'$. But then we must have $k = k'$.

    As in Problem~56, $\closure{B}(0,1)$ is the set of rationals $m/n$ with $p \not\mid n$. The numbers $0, n, 2n, \ldots, (p-1)n$ lie in distinct $p$-residue classes, so there exists a $k \in \{0, 1, \ldots, p-1\}$ such that $p \mid m - kn$. It follows that $m/n - k = (m-kn)/n \in B(0,1)$, so $m/n \in B(k,1)$.
\end{solution}


\begin{problem}[62]
    Show that in a field with a (non-trivial) non-archimedean absolute value every closed ball with radius $r > 0$ is disconnected. Is the same true for open balls?
\end{problem}

\begin{solution}
    First a lemma:
    %
    \begin{displaytheorem}
        If $\abs{\,\cdot\,}$ is a nontrivial absolute value on $\fieldK$, then for every $\epsilon > 0$ there is an $x \in \fieldK$ such that $\abs{x} < \epsilon$.
    \end{displaytheorem}
    %
    Let $y \in \fieldK$ be such that $\abs{y} < 1$ (if $\abs{y} > 1$, then replace $y$ with $1/y$). Then $\abs{y^n} = \abs{y}^n \to 0$ as $n \to \infty$, proving the claim.

    Next, since the metric on $\fieldK$ is invariant, this implies that for every $y \in \fieldK$ there is an $x \in \fieldK \setminus \{y\}$ with $\abs{x-y} < \epsilon$. In particular, every open ball contains infinitely many points.
    
    Now let $B$ be a (closed or open) ball with radius $r > 0$, centered at $x$, and let $y \in B \setminus \{x\}$. Let $s = \abs{x - y} \leq r$, and consider the open ball $B(x,s)$. This is both open and closed by Proposition~2.3.7(iii) (here we use that the absolute value is non-archimedean), and it is a proper subset of $B$. Then $B(x,s)$ and $B(x,s)^c$ disconnect $B$.
\end{solution}


\begin{problem}[63]
    Show that if a set in a field with a non-archimedean absolute value contains two distinct points then it is disconnected.
\end{problem}

\begin{solution}
    Let the set $A$ contain distinct points $x$ and $y$, and let $r = \abs{x-y} > 0$. The open ball $B(x,r)$ contains $x$ but not $y$, and it is both open and closed by Proposition~2.3.7(iii). Hence it and $B(x,r)^c$ disconnect $A$.
\end{solution}


\section{Algebra}

\begin{problem}[67]
    Prove that $\ints_{(p)} / p\ints_{(p)} = \field_p$.
\end{problem}

\begin{solution}
    Consider the composition $\ints \hookrightarrow \ints_{(p)} \twoheadrightarrow \ints_{(p)} / p\ints_{(p)}$, and notice that its kernel is $\ints \intersect p\ints_{(p)} = p\ints$. The first isomorphism theorem thus implies the claim.
\end{solution}


\begin{problem}[69]
    Consider $\rationals$ with a $p$-adic absolute value, and let $a \in \ints$. Describe the open ball $B(a,1)$ in terms of the algebraic structure. Use your description to interpret algebraically the fact (Problem~57) that the closed ball $\closure{B}(0,1)$ is the disjoint union of finitely many open balls of radius $1$.
\end{problem}

\begin{solution}
    Recall that $\closure{B}(0,1) = \ints_{(p)}$, and that $B(0,1) = p\ints_{(p)}$. Next notice that since the metric in $\rationals$ is invariant, we have $B(x,1) = x + B(0,1)$ for all $x \in \rationals$. And the additive group $\ints_{(p)}$ is a union of its $p\ints_{(p)}$-cosets, which by Problem~67 are represented by the integers $0,1,\ldots,p-1$. Hence
    %
    \begin{equation*}
        \ints_{(p)}
            = \bigunion_{k=0}^{p-1} \bigl( k + p\ints_{(p)} \bigr).
    \end{equation*}
    %
    Rewriting this in terms of balls we get the statement in Problem~57.
\end{solution}

\newcommand{\frakP}{\mathfrak{P}}

\begin{problem}[71]
    Let $\fieldK$ be a field, and let $\abs{\,\cdot\,}$ be a non-archimedean absolute value on $\fieldK$. Define a valuation $v$ on $\fieldK$ by $v(x) = -\log\abs{x}$ for $x \neq 0$ and $v(0) = \infty$.
    %
    \begin{enumerate}
        \item If $\abs{\,\cdot\,}$ is the $p$-adic absolute value, how does $v$ relate to the $p$-adic valuation $v_p$? What is the image of $v$ in this case?

        \item Show that the valuation ideal of $\abs{\,\cdot\,}$ is a principal ideal if and only if the image of $v$ is a discrete additive subgroup of $\reals$.
        
        \item Show that if the image of $v$ is a discrete subgroup of $\reals$ then the valuation ring $\calO$ is a principal ideal domain whose only prime ideals are $0$ and $\frakP$. (For example, check that this happens for the $p$-adic absolute values.)
    \end{enumerate}
\end{problem}

\begin{solution}
    (i) We have
    %
    \begin{equation*}
        v(x)
            = -\log \abs{x}_p
            = -\log p^{-v_p(x)}
            = v_p(x) \log p.
    \end{equation*}
    %
    In particular, $v(\rationals^*) = v_p(\rationals^*) \log p = \ints \log p$.

    (ii) Assume that $\frakP$ is principal, e.g. generated by $x$. Then $x \in \frakP$, so $\abs{x} < 1$. Since every element in $\frakP$ is on the form $xy$ for some $y \in \calO$, we have $\abs{xy} = \abs{x} \, \abs{y} \leq \abs{x}$. Hence there do not exist elements in $\frakP$ whose absolute value come arbitrarily close to $1$. This implies that there do not exist elements in $\frakP$ whose valuation come arbitrarily close to $0$. In other words, $v(\frakP)$ does not have $0$ as a limit point. Next notice that $\fieldK \setminus \calO$ also does not contain elements whose absolute value come arbitrarily close to $1$, since then $\frakP$ would also contain such elements (by taking the reciprocal). Of course, elements in $\calO \setminus \frakP$ are mapped by $v$ to $0$. This shows that $\im v$ is discrete.

    Conversely, assume that the value group is discrete, and let $x \in \frakP$ be an element such that $v(x)$ is minimal and positive. Then $\abs{x}$ is maximal among elements in $\frakP$. Thus if $y \in \frakP$, then $\abs{y/x} \leq 1$, and so $y/x \in \calO$. But $x(y/x) = y$, so $x$ generates $\frakP$.

    (iii) TODO
\end{solution}


\chapter{The $p$-adic Numbers}

\section{Absolute Values on $\rationals$}

\begin{problem}[76]
    Show that if $p$ and $q$ are two different primes, the $p$-adic and the $q$-adic absolute values are not equivalent.
\end{problem}

\begin{solution}
    Notice that $\abs{p}_p = 1/p$ and $\abs{q}_q = 1/q$, but $\abs{q}_p = \abs{p}_q = 1$. So there is no number $\alpha$ as in Proposition~3.1.3(iv).
\end{solution}


\begin{problem}[77]
    Show that in general a non-archimedean absolute value cannot be equivalent to an archimedean absolute value.
\end{problem}

\begin{solution}
    Let $\abs{\,\cdot\,}_1$ be non-archimedean, and let $\abs{\,\cdot\,}_2$ be equivalent to $\abs{\,\cdot\,}_1$. Let $\alpha$ be as in Proposition~3.1.3(iv). For any $x,y$ we then have
    %
    \begin{equation*}
        \abs{x+y}_2^\alpha
            = \abs{x+y}_1
            \leq \abs{x}_1 \join \abs{y}_1
            = \abs{x}_2^\alpha \join \abs{y}_2^\alpha
            = \bigl( \abs{x}_2 \join \abs{y}_2 \bigr)^\alpha.
    \end{equation*}
    %
    Taking the $\alpha$th root shows that $\abs{\,\cdot\,}_2$ is also non-archimedean.
\end{solution}


\section{Completions}

Step 1: Embedding a metric space as a dense subset of a complete pseudometric space.

Let $(S,\rho)$ be a metric space. We first consider the set $\calC_S$ of Cauchy sequences in $S$. For $x = (x_n)$ and $y = (y_n)$ in $\calC_S$ we define
%
\begin{equation*}
    \rho_\calC(x,y)
        = \lim_{n \to \infty} \rho(x_n,y_n).
\end{equation*}
%
This limit exists since $(\rho(x_n,y_n))$ is a Cauchy sequence and $\reals$ is complete.\footnote{Notice that since we assume completeness of $\reals$, we cannot use this procedure to construct $\reals$. But this is not an issue, since we wouldn't even be able to \emph{define} either absolute values or metrics without the existence of $\reals$. And from the existence of $\reals$ we of course also get completeness.} One easily show that $\rho_\calC$ is a pseudometric on $\calC_S$.

For completeness, let $(x^n)_{n \in \naturals}$ be a Cauchy sequence in $\calC_S$, and write $x^n = (x^n_k)_{k \in \naturals}$. Now let $N_1 = 1$, and assume that we have chosen a sequence of strictly increasing integers $N_1, \ldots, N_{k-1}$ such that for all $i \in \{1,\ldots,k-1\}$ and for $m,n \geq N_i$ we have $\rho(x^i_m,x^i_n) < 1/i$. Since $x^k$ is a Cauchy sequence in $S$, there is a $N_k \in \naturals$ such that $\rho(x^k_m,x^k_n) < 1/k$ when $m,n \geq N_k$. Choose this integer such that furthermore $N_k > N_{k-1}$. This yields a sequence $(N_k)_{k \in \naturals}$ with the above property.

Now define a sequence $y = (y_n)_{n \in \naturals}$ by $y_n = x^n_{N_n}$. We claim that this is a Cauchy sequence in $S$, such that it lies in $\calC_S$. Notice that for any $k \in \naturals$ we have
%
\begin{equation*}
    \rho(y_m,y_n)
        = \rho(x^m_{N_m}, x^n_{N_n})
        \leq \rho(x^m_{N_m}, x^m_k) + \rho(x^m_k, x^n_k) + \rho(x^m_k, x^n_{N_n}).
\end{equation*}
%
Let $\epsilon > 0$. Since $(x^n)$ is a Cauchy sequence, for large enough $m$ and $n$ we have $\rho_\calC(x^m,x_n) < \epsilon$. In particular, the middle term above eventually becomes less than $\epsilon$ as $k \to \infty$. Taking the $\limsup$ as $k \to \infty$ we thus get
%
\begin{equation*}
    \rho(y_m,y_n)
        \leq \tfrac{1}{m} + \epsilon + \tfrac{1}{n}.
\end{equation*}
%
Given any $\epsilon$ we can find large enough $m$ and $n$ such that this holds, so $y$ is a Cauchy sequence.

We next show that $x^n \to y$ in $\calC_S$. Let $\epsilon > 0$ and notice that
%
\begin{equation*}
    \rho_\calC(x^n,y)
        = \lim_{k \to \infty} \rho(x^n_k, y_k)
        \leq \limsup_{k \to \infty} \rho(x^n_k, x^n_{N_n})
             + \limsup_{k \to \infty} \rho(y_n, y_k),
\end{equation*}
%
since $y_n = x^n_{N_n}$. Since $y$ is a Cauchy sequence, there exists a $N \in \naturals$ such that $\rho(y_m,y_n) < \epsilon$ when $m,n \geq N$. Also choose $N$ such that $1/N < \epsilon$. By definition of $N_n$, we get for $n \geq N$ that
%
\begin{equation*}
    \rho_\calC(x^n,y)
        \leq \tfrac{1}{n} + \epsilon
        \leq \tfrac{1}{N} + \epsilon
        \leq 2\epsilon.
\end{equation*}
%
Hence $x^n \to y$.

Finally notice that the map $\iota \colon S \to \calC_S$ that sends an element $x \in S$ to the constant sequence $(x,x,\ldots)$ is an isometry, hence is injective. We claim that $\iota(S)$ is dense in $\calC_S$. If $y = (y_k)_{k \in \naturals}$ is an element of $\calC_S$, i.e. a Cauchy sequence in $S$, then we have
%
\begin{equation*}
    \rho_\calC(\iota(y_n),y)
        = \lim_{k \to \infty} \rho(y_n, y_k).
\end{equation*}
%
Since $y$ is a Cauchy sequence, choosing $n$ large enough the above can become arbitrarily small. Hence $\iota(y_n) \to y$.

Step 2: The metric identification of a complete space is complete.

Next let $(S,\rho)$ be a complete pseudometric space. We quotient out by the \keyword{metric identification} $\sim$, i.e. the equivalence relation on $S$ given by $x \sim y$ if $\rho(x,y) = 0$. Then define a metric $\tilde{\rho}$ on $\tilde{S} = S/{~}$ by $\tilde{\rho}([x],[y]) = \rho(x,y)$. It is easy to show that this is well-defined. Hence the quotient map $q \colon S \to \tilde{S}$ given by $x \mapsto [x]$ is an isometry. Now let $(\tilde{x}_n)_{n \in \naturals}$ be a Cauchy sequence in $\tilde{S}$, and choose for each equivalence class $\tilde{x}_n$ a representative $x_n$. Then $(x_n)$ is a Cauchy sequence in $S$ and hence converges to some $x \in S$. It follows that
%
\begin{equation*}
    \tilde{\rho}(\tilde{x}_n, [x])
        = \rho(x_n,x)
        \to 0,
\end{equation*}
%
as $n \to \infty$. Hence $\tilde{S}$ is also complete.

We note for future reference that the metric identification coincides with the $T_0$-identification, which in particular implies that the quotient map $q$ is closed.

Step 3: Dense embedding of metric space in completion.

Let $(S,\rho)$ be a metric space, let $\calC_S$ be its space of Cauchy sequences, and denote by $(\overline{S},\overline{\rho})$ the metric identification of $\calC_S$. Step 1 shows that the image $\iota(S)$ is dense in $\calC_S$. And since the quotient map $q$ is continuous and closed, we have
%
\begin{equation*} % TODO closure
    \overline{S}
        = q(\calC_S)
        = q\bigl( \overline{\iota(S)} \bigr)
        = \overline{(q \circ \iota)(S)}.
\end{equation*}
%
Next, it is clear that $q \circ \iota$ is injective, since $q$ only identifies elements if the distance between them is $0$, and $S$ is a metric space. Hence $q \circ \iota$ embeds $S$ as a dense subset of a complete metric space.

Step 4: Completion of fields.

Let $\fieldK$ be a field equipped with an absolute value $\abs{\,\cdot\,}$. Notice that the space $\calC_\fieldK$ of Cauchy sequences has a natural ring structure, where addition and multiplication are defined elementwise. This makes $\calC_\fieldK$ into a commutative ring with identity, and the subring $\iota(\fieldK)$ is a field. Furthermore, notice that since the metric $\rho(x,y) = \abs{x-y}$ on $\fieldK$ is invariant, so is the metric $\rho_\calC$.

The absolute value on $\fieldK$ of course induces an absolute value on $\iota(\fieldK)$, and we may extend this to a map on $\calC_\fieldK$ as follows: If $x = (x_n)$ is an element of $\calC_\field$, define
%
\begin{equation*}
    \abs{x}
        = \lim_{n \to \infty} \abs{x_n}.
\end{equation*}
%
This is well-defined since $(\abs{x_n})$ is a Cauchy sequence in $\reals$ (by the inverse triangle inequality), hence is convergent. Notice that we also have
%
\begin{equation*}
    \lim_{n \to \infty} \abs{x_n}
        = \lim_{n \to \infty} \rho(x_n,0)
        = \rho_\calC(x,\iota(0)),
\end{equation*}
%
so $\abs{x} = \rho_\calC(x,\iota(0))$, and in particular
%
\begin{equation*}
    \rho_\calC(x,y)
        = \rho_\calC(x-y,\iota(0))
        = \abs{x-y},
\end{equation*}
%
for $y \in \calC_\fieldK$. Next, the map $\abs{\,\cdot\,}$ on $\calC_\fieldK$ inherits many of the properties of the absolute value on $\fieldK$. For instance,
%
\begin{equation*}
    \abs{xy}
        = \lim_{n \to \infty} \abs{x_n y_n}
        = \lim_{n \to \infty} \abs{x_n} \, \abs{y_n}
        = \lim_{n \to \infty} \abs{x_n} \lim_{n \to \infty} \abs{y_n}
        = \abs{x} \, \abs{y}.
\end{equation*}
%
We also get the triangle inequality by a similar calculation. If the absolute value on $\fieldK$ is non-archimedean, we also get
%
\begin{equation*}
    \abs{x+y}
        = \lim_{n \to \infty} \abs{x_n + y_n}
        \leq \limsup_{n \to \infty} \bigl( \abs{x_n} \join \abs{y_n} \bigr)
        \leq \limsup_{n \to \infty} \abs{x_n} \join \limsup_{n \to \infty} \abs{y_n}
        = \abs{x} \join \abs{y}.
\end{equation*}
%
In particular, the ring operations are continuous. Hence the additive group $(\calC_\fieldK,+)$ is in particular a topological group.

Now recall that the metric identification which constructs $\overline{\field}$ from $\calC_\fieldK$ coincides with the $T_0$-identification. Furthermore, the $T_0$-identification on a topological group is also given by its quotient by the normal subgroup $\overline{\{e\}}$, where $e$ is the identity. Let $\calN$ denote the corresponding subgroup of $\calC_\fieldK$ so that $\overline{\field} = \calC_\field / \calN$, which makes $\overline{\field}$ into a group. In fact it is a ring, since $\calN$ is an ideal: Notice that $\calN$ is precisely the set of elements $x$ with $\abs{x} = 0$. Hence if $y \in \calC_\fieldK$, then
%
\begin{equation*}
    \abs{xy}
        = \abs{x} \, \abs{y}
        = 0,
\end{equation*}
%
so $xy \in \calN$. Thus $\overline{\field}$ is a ring. Furthermore, the map $\abs{\,\cdot\,}$ on $\calC_\fieldK$ descends to $\overline{\fieldK}$ by letting $\abs{[x]} = \abs{x}$. The triangle inequality on $\calC_\fieldK$ implies that this is well-defined. Thus the map $\abs{\,\cdot\,}$ on $\overline{\fieldK}$ inherits all the relevant properties of the same map on $\calC_\fieldK$. Furthermore, we have
%
\begin{equation*}
    \overline{\rho}([x],[y])
        = \rho(x,y)
        = \abs{x - y}
        = \abs[\big]{[x] - [y]},
\end{equation*}
%
as expected. It follows that the ring operations on $\overline{\field}$ are continuous.

Finally we may show that $\calN$ is a maximal ideal, making $\overline{\field}$ into a field: Let $x = (x_n) \in \calC_\fieldK \setminus \calN$ so that $\abs{x} > 0$, and denote by $\calI$ the ideal generated by $x$ and $\calN$. There exist $R > 0$ and $N \in \naturals$ such that $\abs{x_n} \geq R > 0$ when $n \geq N$. Now define a sequence $y = (y_n)$ by
%
\begin{equation*}
    y_n =
    \begin{cases}
        0, & n < N, \\
        \tfrac{1}{x_n}, & n \geq N.
    \end{cases}
\end{equation*}
%
For $m,n \geq N$ we thus have
%
\begin{equation*}
    \abs{y_m - y_n}
        = \abs[\bigg]{\frac{1}{x_m} - \frac{1}{x_n}}
        = \frac{ \abs{x_m - x_n} }{ \abs{x_m x_n} }
        \leq \frac{ \abs{x_m - x_n} }{ R^2 }.
\end{equation*}
%
Hence $y$ is a Cauchy sequence since $x$ is. Now notice that $xy$ is eventually constant and equal to $1$. But then $\iota(1) - xy \in \calN$, so $\iota(1) \in \calI$, and it follows that $\calI = \calC_\fieldK$. Thus $\calN$ is maximal.


\chapter{Exploring $\rationals_p$}

\addtocounter{section}{1}
\section{$p$-adic Integers}

\begin{problem}[100]
    Show that the sets $p^n \ints_p$, $n \in \ints$ form a fundamental system of neighborhoods of $0 \in \rationals_p$ which covers all of $\rationals_p$.
\end{problem}

\begin{solution}
    Recall that $\ints_p = \overline{B}(0,1)$ is open, so that $p^n \ints_p = \overline{B}(0,p^{-n})$ is also open. This is clearly a neighbourhood basis at $0$ (since $p^{-n}$ can become arbitrarily small), and it clearly covers $\rationals_p$ (since $p^{-n}$ can become arbitrarily large). For the last point we could also have used the first part of Corollary~4.2.4, but this is not necessary.
\end{solution}

TODO 102

\begin{problem}[103]
    For any $n \geq 1$, show that there is a homomorphism $\phi_n \colon \ints_p \to \ints / p^n \ints$ such that the sequence
    %
    \begin{equation*}
        \begin{tikzcd}
            0
                \ar[r]
            & \ints_p
                \ar[r, "p^n"]
            & \ints_p
                \ar[r, "\phi_n"]
            & \ints/p^n \ints
                \ar[r]
            & 0
        \end{tikzcd}
    \end{equation*}
    %
    of abelian groups is exact, and that the maps are continuous when $\ints / p^n \ints$ is equipped with the discrete topology.
\end{problem}

\begin{solution}
    Note that this must be a sequence of groups and not of rings, since the map $x \mapsto p^n x$ is not a ring homomorphism.

    We construct the map $\phi_n$ as follows: Given $x \in \ints_p$, Proposition~4.2.2(ii) furnishes a unique $\alpha \in \ints$ such that $0 \leq \alpha < p^n$ and $\abs{x - \alpha}_p \leq p^{-n}$. We let $\phi_n(x) = [\alpha]$, which is well-defined by uniqueness of $\alpha$. We next show that this is a group homomorphism. If $y \in \ints_p$ and $\beta \in \ints$ with $0 \leq \beta < p^n$ and $\phi_n(y) = [\beta]$, then choose $\gamma \in \ints$ with $0 \leq \gamma < p^n$ and $\gamma \equiv_{p^n} \alpha + \beta$. Then we have
    %
    \begin{align*}
        \abs{x+y-\gamma}_p
            &\leq \abs{x+y-(\alpha+\beta)}_p \join \abs{\alpha+\beta-\gamma}_p \\
            &\leq \abs{x-\alpha}_p \join \abs{y-\beta} \join \abs{\alpha+\beta-\gamma}_p \\
            &\leq p^{-n}.
    \end{align*}
    %
    Here we have used the definitions of $\alpha$ and $\beta$, and the fact that $\abs{\alpha+\beta-\gamma}_p \leq p^{-n}$ since $\gamma \equiv_{p^n} \alpha+\beta$. By the uniqueness part of Proposition~4.2.2(ii) we have
    %
    \begin{equation*}
        \phi_n(x+y)
            = [\gamma]
            = [\alpha + \beta]
            = [\alpha] + [\beta]
            = \phi_n(x) + \phi_n(y),
    \end{equation*}
    %
    so $\phi_n$ is a group homomorphism.
    
    Furthermore, $x \in \ker\phi_n$ if and only if $\alpha = 0$, which is the case just when $\abs{x}_p \leq p^{-n}$. But $\overline{B}(0,p^{-n}) = p^n \ints_p$, so $\ker\phi_n = p^n \ints_p$. This is clearly also the image of the map $x \mapsto p^n x$. Next, $x \mapsto p^n x$ is clearly injective (since its codomain lies in a field where we can perform division), and $\phi_n$ is clearly surjective, so the sequence is exact.

    Finally, to show that $\phi_n$ is continuous, notice that the preimage under $\phi_n$ of $[\alpha]$ with $0 \leq \alpha < p^n$ are all the $x \in \ints_p$ with $\abs{x-\alpha}_p \leq p^{-n}$. That is, the preimage is the closed ball $\overline{B}(\alpha,p^{-n})$, which is an open set.

    The first isomorphism theorem then implies that $\ints_p / p^n \ints_p$ and $\ints / p^n \ints$ are isomorphic as groups. However, they are in fact isomorphic as rings, since $\phi_n$ is a ring homomorphism: Clearly $\phi_n(1) = [1]$. Next, define $\gamma$ as above, except that $\gamma \equiv_{p^n} \alpha\beta$, and show that $\abs{xy - \gamma}_p \leq p^{-n}$. In this calculation we use that
    %
    \begin{align*}
        \abs{xy - \alpha\beta}_p
            &= \abs{(xy - x\beta) + (x\beta - \alpha\beta)}_p \\
            &\leq \abs{x}_p \, \abs{y - \beta}_p \join \abs{x - \alpha}_p \, \abs{\beta}_p \\
            &\leq p^{-n}.
    \end{align*}
    %
    Here we use the fact that $x,\beta \in \ints_p$, so $\abs{x}_p, \abs{\beta}_p \leq 1$. This shows that $\phi_n(xy) = [\gamma] = [\alpha][\beta] = \phi_n(x)\phi_n(y)$.
\end{solution}


\begin{problem}[106]
    If $\fieldK$ is a field with an absolute value, show that $\fieldK$ is locally compact if and only if there exists a neighborhood of zero that is compact.
\end{problem}

\begin{solution}
    This is obvious since the topology on $\fieldK$ is homogeneous.
\end{solution}


\begin{problem}[108]
    Let $\fieldK$ be a field, $\abs{\,\cdot\,}$ a non-archimedean absolute value on $\fieldK$, $\calO \subseteq \fieldK$ the valuation ring, and $\frakP$ the valuation ideal. Suppose that $\fieldK$ is complete and that $\frakP$ is principal. Show that $\fieldK$ is locally compact if and only if the residue field $\calO/\frakP$ is finite.
\end{problem}

\begin{solution}
    First assume that $\fieldK$ is locally compact. Then $\calO$ is compact, but this is covered by its $\frakP$-cosets which are open, so finitely many of them cover $\calO$. But the cosets are also disjoint, so there must be finitely many of them.

    Conversely assume that $\calO/\frakP$ is finite. We then show that $\calO/\frakP^n$ is also finite for all $n \in \naturals$, where $\frakP^n$ is the $n$-fold product of $\frakP$ with itself. We prove this by induction. The case $n = 1$ is true by assumption, so assume that $n > 1$ and that $\calO/\frakP^{n-1}$ is finite. Since $\frakP^n \subseteq \frakP$ there is a natural map $\calO/\frakP^n \to \calO/\frakP$ whose kernel is $\frakP/\frakP^n$. We prove that this is finite. To this end, let $x \in \frakP$ be a generator of $\frakP$, so that it is on the form $x\calO$. Then $\frakP^n = x^n \calO$. Consider the group homomorphism
    %
    \begin{equation*}
        \calO \to \frac{x \calO}{x^n \calO}
    \end{equation*}
    %
    given by $y \mapsto xy + x^n \calO$. The kernel of this homomorphism consists precisely of those $y \in \calO$ that are on the form $x^{n-1}z$ for some $z \in \calO$, i.e. $x^{n-1} \calO$. Thus we have
    %
    \begin{equation*}
        \frac{\calO}{\frakP^{n-1}}
            = \frac{\calO}{x^{n-1} \calO}
            \cong \frac{x \calO}{x^n \calO}
            = \frac{\frakP}{\frakP^n}
    \end{equation*}
    %
    But $\calO / \frakP^{n-1}$ is finite by induction, so $\frakP / \frakP^n$ is finite. It thus follows that
    %
    \begin{equation*}
        [\calO : \frakP^n]
            = [\calO : \frakP] [\frakP : \frakP^n] < \infty.
    \end{equation*}
    %
    Hence $\calO$ is the union of finitely many translates of the ball $\frakP^n = B(0,\abs{x}^n)$, and these can become arbitrarily small since $x \in \frakP$, so $\abs{x} < 1$. Thus $\calO$ is compact, and hence $\fieldK$ is locally compact.
\end{solution}


\section{The elements of $\rationals_p$}

\begin{problem}[112]
    Prove that the inclusion
    %
    \begin{equation*}
        \phi \colon \ints_p \hookrightarrow \bigprod_{n \in \naturals} \ints / p^n \ints
    \end{equation*}
    %
    is an embedding.
\end{problem}

\begin{solution}
    Notice that each factor $\phi_n$ of $\phi$ is continuous by Problem~103, so $\phi$ is also continuous. It is injective, so for it to be an embedding it suffices that it is open or closed. But $\ints_p$ is compact and the product is Hausdorff (since each factor is), so $\phi$ is closed.
\end{solution}

TODO 113


\chapter{Elementary Analysis in $\rationals_p$}

\section{Sequences and Series}

\begin{problem}[142]
    Let $(a_n)$ be a convergent sequence in $\rationals_p$. Show that either $\lim_{n \to \infty} \abs{a_n} = 0$ or there exists an integer $M$ such that $\abs{a_n}_p = \abs{a_M}_p$ for every $n \geq M$.
\end{problem}

\begin{solution}
    Recall that the image of $\rationals_p$ under $\abs{\,\cdot\,}_p$ is $\set{p^n}{n \in \ints} \union \{0\}$, and $0$ is the only limit point of this set. Furthermore, since $(a_n)$ is convergent so is $(\abs{a_n}_p)$, and the first sequence converges to $0 \in \rationals_p$ if and only if the second converges to $0 \in \reals$. And if the second does not converge to $0$, then since $\set{p^n}{n \in \ints}$ is discrete it must be eventually constant.
\end{solution}


\begin{problem}[143]
    Show that absolute convergence implies convergence in $\rationals_p$.
\end{problem}

\begin{solution}
    Let $\sum_{n=1}^\infty a_n$ be a series with terms in $\rationals_p$ such that the series $\sum_{n=1}^\infty \abs{a_n}_p$ converges in $\reals$. Letting $s_n = \sum_{i=1}^n a_i$ we get, for $m \leq n$, that
    %
    \begin{equation*}
        \abs{s_n - s_m}_p
            = \abs[\bigg]{ \sum_{i=m+1}^n a_i }_p
            \leq \sum_{i=m+1}^n \abs{a_i}_p
            \xrightarrow[m,n \to \infty]{} 0.
    \end{equation*}
    %
    So $(s_n)$ is a Cauchy sequence, hence convergent.

    Notice that this proof is precisely the same as the proof for series with terms in $\reals$. That is, the strong triangle inequality is not necessary.
\end{solution}

TODO (149) 150

\section{Functions, Continuity, Derivatives}

\begin{problem}[153]
    xx
\end{problem}

\begin{solution}
    xx
\end{solution}


\addtocounter{section}{1}
\section{Power Series}

TODO (156)


\section{Functions Defined by Power Series}

TODO 163


\chapter{Vector Spaces and Field Extensions}

\section{Normed Vector Spaces over Complete Valued Fields}

\begin{problem}[212]
    Show that two norms on $V$ are equivalent if and only if they define the same topology on $V$.
\end{problem}

\begin{solution}
    It is clear that equivalent norms induce the same topology. Conversely, let $\norm{\,\cdot\,}_1$ and $\norm{\,\cdot\,}_2$ be norms on $V$ that induce the same topology. Hence the identity function $\id_V \colon (V, \norm{\,\cdot\,}_1) \to (V, \norm{\,\cdot\,}_2)$ is continuous and hence bounded (by Problem~217). It follows that $\norm{v}_2 \leq K \norm{v}_1$ for all $v \in V$ for some $K > 0$. By symmetry the other inequality also holds.
\end{solution}


-----------------------------------------------------

Cohn exercises

\begin{problem}[1.1.7]
    Let $\calS$ be a collection of subsets of the set $X$. Show that for each $A$ in $\sigma(\calS)$, there is a countable subfamily $\calC_A$ of $\calS$ such that $A \in \sigma(\calC_A)$.
\end{problem}

\begin{solution}
    Let $\calA$ be the union the $\sigma$-algebras $\sigma(\calC)$, where $\calC$ ranges over the countable subfamilies of $\calS$. Then $\calS \subseteq \calA \subseteq \sigma(\calS)$, and $\calA$ is a $\sigma$-algebra, so in fact $\calA = \sigma(\calS)$. The only thing that is not obvious is that $\calA$ is closed under countable unions. But if $(A_n)_{n \in \naturals}$ is a sequence in $\calA$, then $A_n \in \sigma(\calC_n)$ for some countable $\calC_n$, and so
    %
    \begin{equation*}
        \bigunion_{n \in \naturals} A_n
            \in \bigunion_{n \in \naturals} \sigma(\calC_n)
            \subseteq \sigma \Bigl( \bigunion_{n \in \naturals} \calC_n \Bigr)
            \subseteq \calA,
    \end{equation*}
    %
    since the collection $\bigunion_{n \in \naturals} \calC_n$ is a countable subfamily of $\calS$.

    If $A \in \sigma(\calS) = \calA$, then there is a $\calC_A$ such that $A \in \sigma(\calC_A)$, as desired.
\end{solution}

\newlist{solutionsec}{enumerate}{1}
\setlist[solutionsec]{leftmargin=0pt, parsep=0pt, listparindent=\parindent, label=(\alph*), labelsep=0pt, labelwidth=20pt, itemindent=20pt, align=left, itemsep=.5\baselineskip}

\begin{problem}[1.2.6]
    Let $(X,\calA)$ be a measurable space.
    %
    \begin{enumerate}
        \item Show that if $(\mu_n)$ is an increasing sequence of measures on $(X,\calA)$, then the formula $\mu(A) = \lim_{n \to \infty} \mu_n(A)$ defines a measure on $(X,\calA)$.
        
        \item Show that if $(\mu_n)$ is an arbitrary sequence of measures on $(X,\calA)$, then the formula $\mu(A) = \sum_{n=1}^\infty \mu_n(A)$ defines a measure on $(X,\calA)$.
    \end{enumerate}
\end{problem}

\begin{solution}
\begin{solutionsec}
    \item Clearly $\mu(\emptyset) = 0$, so let $(A_j)$ be a sequence of disjoint sets from $\calA$. Then
    %
    \begin{equation*}
        \mu \Bigl( \bigunion_{j \in \naturals} A_j \Bigr)
            = \lim_{n \to \infty} \mu_n \Bigl( \bigunion_{j \in \naturals} A_j \Bigr)
            = \lim_{n \to \infty} \lim_{m \to \infty} \sum_{j=1}^m \mu_n(A_j).
    \end{equation*}
    %
    Notice that if $(a_{mn})_{(m,n) \in \naturals \prod \naturals}$ is any collection of real numbers, then
    %
    \begin{equation*}
        \sup_{m \in \naturals} \sup_{n \in \naturals} a_{mn}
            \leq \sup_{m,n \in \naturals} a_{mn}.
    \end{equation*}
    %
    The opposite inequality is obvious if the left-hand side is infinite, so assume that it is finite and let $\epsilon > 0$. Then there are $m,n \in \naturals$ such that
    %
    \begin{equation*}
        \sup_{m,n \in \naturals} a_{mn} - \epsilon
            \leq a_{mn}
            \leq \sup_{m \in \naturals} \sup_{n \in \naturals} a_{mn}.
    \end{equation*}
    %
    Since $\epsilon$ was arbitrary, this implies that
    %
    \begin{equation*}
        \sup_{m \in \naturals} \sup_{n \in \naturals} a_{mn}
            = \sup_{m,n \in \naturals} a_{mn}
            = \sup_{n \in \naturals} \sup_{m \in \naturals} a_{mn}.
    \end{equation*}
    %
    Letting $a_{mn} = \sum_{j=1}^m \mu_n(A_j)$, the double sequence $(a_{mn})$ is increasing in the product ordering on $\naturals \prod \naturals$ (i.e., increasing in $m$ and $n$ separately), so exchanging limits for suprema we get
    %
    \begin{align*}
        \mu \Bigl( \bigunion_{j \in \naturals} A_j \Bigr)
            &= \sup_{n \in \naturals} \sup_{m \in \naturals} \sum_{j=1}^m \mu_n(A_j)
             = \sup_{m \in \naturals} \sup_{n \in \naturals} \sum_{j=1}^m \mu_n(A_j) \\
            &= \lim_{m \to \infty} \sum_{j=1}^m \lim_{n \to \infty} \mu_n(A_j)
             = \sum_{j=1}^\infty \mu(A_j).
    \end{align*}

    \item Use part (a) on the increasing sequence of partial sums.
\end{solutionsec}
\end{solution}


\begin{problem}[1.5.1]
    Let $(X,\calA,\mu)$ be a measure space. Show that $(\calA_\mu)_{\overline{\mu}} = \calA_\mu$ and $\overline{\overline{\mu}} = \overline{\mu}$.
\end{problem}

\begin{solution}
    If $\calN_\mu$ denotes the set of (not necessarily measurable) $\mu$-null sets, then it is easy to show that
    %
    \begin{equation*}
        \calA_\mu
            = \set{A \union N}{A \in \calA, N \in \calN_\mu}.
    \end{equation*}
    %
    In particular, $\calA_\mu = \calA$ if and only if $\calN_\mu \subseteq \calA$, which is the case just when $\mu$ is complete. The claim follows since $\overline{\mu}$ is complete.
\end{solution}


\begin{problem}[1.5.3(b)]
    Let $\mu$ and $\nu$ be finite measures on a measurable space $(X,\calA)$. Prove or disprove: $\calA_\mu = \calA_\nu$ if and only if $\mu$ and $\nu$ have exactly the same sets of measure zero.
\end{problem}

\begin{solution}
    This is true, since in this case $\calN_\mu = \calN_\nu$.
\end{solution}


\begin{problem}[4.1.3]
    Let $\mu_1$ and $\mu_2$ be finite signed measures on the measurable space $(X,\calA)$. Define signed measures $\mu_1 \join \mu_2$ and $\mu_1 \meet \mu_2$ on $(X,\calA)$ by $\mu_1 \join \mu_2 = \mu_1 + (\mu_2 - \mu_1)^+$ and $\mu_1 \meet \mu_2 = \mu_1 - (\mu_1 - \mu_2)^+$.
    %
    \begin{enumerate}
        \item Show that $\mu_1 \join \mu_2$ is the smallest of those finite signed measures $\nu$ that satisfy
        $\nu(A) \geq \mu_1(A)$ and $\nu(A) \geq \mu_2(A)$ for all $A$ in $\calA$.
        
        \item Find and prove an analogous characterization of $\mu_1 \meet \mu_2$.
    \end{enumerate}
\end{problem}

\begin{solution}
\begin{solutionsec}
    \item Since $(\mu_2 - \mu_1)^+$ is a positive measure, we clearly have $\mu_1 + (\mu_2 - \mu_1)^+ \geq \mu_1$. Since $(\mu_2 - \mu_1)^+ \geq \mu_2 - \mu_1$, we also have
    %
    \begin{equation*}
        \mu_1 + (\mu_2 - \mu_1)^+
            \geq \mu_1 + \mu_2 - \mu_1
            = \mu_2.
    \end{equation*}
    %
    Next let $\nu$ be a finite signed measure with $\mu_1,\mu_2 \leq \nu$. Then
    %
    \begin{equation*}
        \mu_2 - \mu_1
            \leq \nu - \mu_1,
    \end{equation*}
    %
    and $\nu - \mu_1$ is positive so $(\mu_2 - \mu_1)^+ \leq \nu - \mu_1$, proving that $\mu_1 + (\mu_2 - \mu_1)^+$ is minimal.

    \item Similarly, $\mu_1 \meet \mu_2$ is the largest finite signed measure smaller than both $\mu_1$ and $\mu_2$.
\end{solutionsec}

In particular, $M(\calA,\reals)$ is a lattice when equipped with the product order from $\reals^\calA$. We also have $\mu_1 \join \mu_2 = \mu_2 \join \mu_1$, and similary for meets.
\end{solution}


\begin{problem}[4.1.5]
    Let $\mu$ be a signed or complex measure on $(X,\calA)$, and let $\nu$ be a positive measure on $(X,\calA)$ such that $\abs{\mu(A)} \leq \nu(A)$ holds for each $A$ in $\calA$. Show that $\abs{\mu}(A) \leq \nu(A)$ holds for each $A$ in $\calA$. Hence $\abs{\mu}$ is the smallest positive measure greater than $\mu$, i.e. $\abs{\mu} = \mu \join 0$.
\end{problem}

\begin{solution}
    If $(A_j)$ is a partition of $A$, then
    %
    \begin{equation*}
        \sum_{j=1}^n \abs{\mu(A_j)}
            \leq \sum_{j=1}^n \nu(A_j)
            = \nu(A).
    \end{equation*}
    %
    Taking the supremum on the left-hand side yields the claim.
\end{solution}


\begin{problem}[4.1.8]
    Let $\mu$ and $\mu_1, \mu_2, \ldots$ be finite signed or complex measures on $(X,\calA)$. Show that $\lim_{n \to \infty} \norm{\mu_n - \mu} = 0$ holds if and only if $\mu_n(A)$ converges to $\mu(A)$ uniformly in $A$ as $n$ approaches infinity.

    In particular, the norms $\norm{\,\cdot\,}$ and $\norm{\,\cdot\,}_{\sup}$ are equivalent and thus have the same Cauchy sequences.\footnote{Recall that topological and Lipschitz equivalence are equivalent for metrics induced by a norm.}
    
    Furthermore, $M(\calA,\fieldK)$ is complete.
\end{problem}

\begin{solution}
    Notice that the set $M(\calA,\fieldK)$ is a subspace of the space of bounded functions $\calA \to \fieldK$. In particular, it inherits the supremum norm $\norm{\,\cdot\,}_{\sup}$, and uniform convergence is just convergence with respect to this norm.

    For one implication, notice that for $A \in \calA$ we have
    %
    \begin{equation*}
        \abs{\mu(A)}
            \leq \abs{\mu}(A)
            \leq \norm{\mu},
    \end{equation*}
    %
    which implies that $\norm{\mu}_{\sup} \leq \norm{\mu}$. For the other implication, notice that it suffices to show that if $\norm{\mu_n}_{\sup} \to 0$, then also $\norm{\mu} \to 0$. If $\epsilon > 0$, then there is a partition $(A_j)_{j=1}^n$ of $X$ such that
    %
    \begin{equation*}
        \norm{\mu_n} - \epsilon
            \leq \sum_{j=1}^k \abs{\mu_n(A_j)}
            \leq k \norm{\mu_n}_{\sup}.
    \end{equation*}
    %
    It follows that
    %
    \begin{equation*}
        \limsup_{n \to \infty} \norm{\mu_n}
            = 0,
    \end{equation*}
    %
    proving the claim.

    In particular, the norms $\norm{\,\cdot\,}$ and $\norm{\,\cdot\,}_{\sup}$ are topologically equivalent (since $M(\calA,\fieldK)$ is a metric space and hence is a sequential space).

    To show that $M(\calA,\fieldK)$ is complete, it suffices to show that it is closed as a subset of the space of bounded functions $\calA \to \fieldK$, so let $(\mu_n)_{n \in \naturals}$ be a sequence in $M(\calA,\fieldK)$ that converges to some $\mu$. It is then clear that $\mu$ is finitely additive, and to show that it is countably additive, let $(A_k)_{k \in \naturals}$ be a decreasing sequence in $\calA$ with $\bigintersect_{k \in \naturals} A_k = \emptyset$. Let $\epsilon > 0$ and choose $N \in \naturals$ such that $\norm{\mu_N - \mu}_{\sup} < \epsilon$. Since $\mu_N$ is countably additive, there is a $K \in \naturals$ such that $k \geq K$ implies $\abs{\mu_N(A_k)} < \epsilon$. Hence
    %
    \begin{equation*}
        \abs{\mu(A_k)}
            \leq \abs{\mu(A_k) - \mu_N(A_k)} + \abs{\mu_N(A_k)}
            < 2\epsilon.
    \end{equation*}
    %
    (Notice the similarity with the proof that a uniform limit of continuous functions is continuous. Indeed, countable additivity of measures is a kind of continuity.)
\end{solution}


\begin{problem}[4.2.5]
    Let $(X,\calA)$ be a measurable space, let $\mu$ be a positive measure on $(X,\calA)$, and let $\nu_1$ and $\nu_2$ be finite signed measures on $(X,\calA)$ that are absolutely continuous with respect to $\mu$.
    %
    \begin{enumerate}
        \item Show that $\nu_1 \join \nu_2 \ll \mu$ and $\nu_1 \meet \nu_2 \ll \mu$.
        
        \item Show that if $\nu_1$ and $\nu_2$ have densities with respect to $\mu$, then so do $\nu_1 \join \nu_2$ and $\nu_1 \meet \nu_2$, and find them.
    \end{enumerate}
\end{problem}

\begin{solution}
\begin{solutionsec}
    \item Both of these follow since $\mu^\ll \defn \set{\nu \in M(\calA,\reals)}{\nu \ll \mu}$ is a subspace.

    \item Denote by $\calD_\mu$ the set of those measures in $M(\calA,\reals)$ that have a density with respect to $\mu$. Notice that such a density can be chosen to lie in $\calL^1(\mu)$. If $\nu_1 = h_1 \mu$, $\nu_2 = h_2 \mu$ and $\alpha \in \reals$, then
    %
    \begin{equation*}
        (\alpha \nu_1 + \nu_2)(A)
            = \alpha \nu_1(A) + \nu_2(A)
            = \alpha \int_A h_1 \dif\mu + \int_A h_2 \dif\mu
            = \int_A (\alpha h_1 + h_2) \dif\mu.
    \end{equation*}
    %
    In particular, $\calD_\mu$ is a subspace. Furthermore, if $\nu = h \mu$ then $\nu = h^+ \mu - h^- \mu$. But we clearly have $h^+ \mu \perp h^- \mu$, so by uniqueness of Jordan decompositions we have $\nu^+ = h^+ \mu$ and $\nu^- = h^- \mu$. In particular, $\nu^+, \nu^- \in \calD_\mu$. We finally find that
    %
    \begin{equation*}
        \nu_1 \join \nu_2
            = \nu_1 + (\nu_2 - \nu_1)^+
            = \bigl( h_1 + (h_2 - h_1)^+) \bigr) \mu
            = (h_1 \join h_2) \mu,
    \end{equation*}
    %
    and we similarly have $\nu_1 \meet \nu_2 = (h_1 \meet h_2) \mu$. In particular, $\calD_\mu$ is a sublattice of $M(\calA,\reals)$.
\end{solutionsec}
\end{solution}


\begin{problem}[4.2.7]
    Let $\mu \in M(\calA,\fieldK)$.
    %
    \begin{enumerate}
        \item Show that
        %
        \begin{equation*}
            \mu^\ll
                \defn \set{\nu \in M(\calA,\fieldK)}{\nu \ll \mu}
        \end{equation*}
        %
        is a closed linear subspace of the normed linear space $M(\calA,\fieldK)$.

        \item Assume that $\mu$ is $\sigma$-finite. Find an isometric isomorphism of $L^1(\mu,\fieldK)$ onto $\mu^\ll$.
    \end{enumerate}
\end{problem}

\begin{solution}
\begin{solutionsec}
    \item Clearly $\mu^\ll$ is a subspace of $M(\calA,\fieldK)$. To show that it is closed, let $(\nu_n)$ be a sequence in $\mu^\ll$ that converges to some $\nu \in M(\calA,\fieldK)$. By Exercise~4.1.8 we have $\abs{\nu_n(A) - \nu(A)} \to 0$, so if $A$ is a $\mu$-null set then $\nu_n(A) = 0$, and so $\nu(A) \to 0$. But this just means that $\nu(A) = 0$, so $\nu \ll \mu$.
    
    \item First consider the map
    %
    \begin{align*}
        \calL^1(\mu,\fieldK) &\to \mu^\ll, \\
        h &\mapsto h \mu.
    \end{align*}
    %
    This is clearly well-defined and linear. Furthermore, by Proposition~4.2.5 we have
    %
    \begin{equation*}
        \norm{h \mu}
            = \abs{h \mu}(X)
            = \int_X \abs{h} \dif\mu
            = \norm{h}_1,
    \end{equation*}
    %
    so the map is an isometry. Descending to the normed space $L^1(\mu,\fieldK)$ we thus obtain an injection. If $\mu$ is $\sigma$-finite, then the Radon--Nikodym theorem yields surjectivity.

    Also notice that if we know that $L^1(\mu,\fieldK)$ is complete, then (a) follows.
\end{solutionsec}
\end{solution}


\begin{problem}[4.2.9]
    Let $\mu$ and $\nu$ be $\sigma$-finite positive measures on $(X,\calA)$. Show that the conditions
    %
    \begin{enumerate}
        \item $\nu \ll \mu$ and $\mu \ll \nu$,
        \item $\mu$ and $\nu$ have exactly the same sets of measure zero, and
        \item there is an $\calA$-measurable function $g$ that satisfies $0 < g(x) < \infty$ at each $x$ in $X$ and is such that $\nu(A) = \int_A g \dif\mu$ holds for each $A$ in $\calA$
    \end{enumerate}
    %
    are equivalent.
\end{problem}

\begin{solution}
    Clearly (i) and (ii) are equivalent. Assuming (i) and (ii), $\nu$ has a density $g$ with respect to $\mu$. Let $(A_n)$ be a sequence of $\nu$-finite sets that increases to $X$. Notice that
    %
    \begin{equation*}
        \nu(\{g = \infty\} \intersect A_n)
            = \int_{\{g = \infty\} \intersect A_n} g \dif\mu
            = \infty \mu(\{g = \infty\} \intersect A_n).
    \end{equation*}
    %
    Since $\{g = \infty\} \intersect A_n$ is $\nu$-finite, it must be $\mu$-null. But then
    %
    \begin{equation*}
        \{g = \infty\}
            = \{g = \infty\} \intersect \bigunion_{n \in \naturals} A_n
            = \bigunion_{n \in \naturals} \bigl( \{g = \infty\} \intersect A_n \bigr)
    \end{equation*}
    %
    is also a $\mu$-null set. Hence we may replace $g$ with $g \indicator{\{g < \infty\}}$ so that $g$ is finite. Next notice that
    %
    \begin{equation*}
        \nu(\{g=0\})
            = \int_{\{g=0\}} g \dif\mu
            = 0,
    \end{equation*}
    %
    so also $\mu(\{g=0\}) = 0$. Hence
    %
    \begin{equation*}
        \nu(A)
            = \int_A g \dif\mu + \mu(A \intersect \{g=0\})
            = \int_A (g + \indicator{\{g=0\}}) \dif\mu
    \end{equation*}
    %
    for all $A \in \calA$, so replacing $g$ with $g + \indicator{\{g=0\}}$ we may also assume that $g > 0$ everywhere. In total $0 < g < \infty$ everywhere.

    Next assume (iii). Then we clearly have $\nu \ll \mu$. Furthermore, if $\nu(A) = 0$ then
    %
    \begin{equation*}
        \int_X g \indicator{A} \dif\mu
            = \int_A g \dif\mu
            = \nu(A)
            = 0,
    \end{equation*}
    %
    so $g \indicator{A} = 0$ $\mu$-a.e. since it is positive. But since $g > 0$, $A$ must be $\mu$-null. Thus we also have $\mu \ll \nu$.
\end{solution}


\begin{problem}[4.2.10]
    Show that if $\mu$ is a $\sigma$-finite measure on $(X,\calA)$, then there is a finite measure $\nu$ on $(X,\calA)$ such that $\nu \ll \mu$ and $\mu \ll \nu$.
\end{problem}

\begin{solution}
    If $\mu$ is trivial then choose $\nu = \mu$. Otherwise let $(A_n)$ be a sequence of disjoint sets from $\calA$ with $0 < \mu(A_n) < \infty$ and whose union is $X$, and define a function $g \colon X \to \reals$ by
    %
    \begin{equation*}
        g
            = \sum_{n=1}^\infty \frac{1}{2^n \mu(A_n)} \indicator{A_n}.
    \end{equation*}
    %
    Then $g \in \calL^1(\mu)$ and $0 < g < \infty$ everywhere, so the claim follows from Exercise~4.2.9.
\end{solution}


\begin{problem}[4.3.2]
    Let $\mu \in M(\calA,\fieldK)$. Show that
    %
    \begin{equation*}
        \mu^\perp
            \defn \set{\nu \in M(\calA,\fieldK)}{\nu \perp \mu}
    \end{equation*}
    %
    is a closed subspace.
\end{problem}

\begin{solution}
    Clearly $\mu^\perp$ is closed under scalar multiplication. If $\nu_1 \perp \mu$ and $\nu_2 \perp \mu$ with $\nu_j$ concentrated on $A_j$, $\mu$ concentrated on $B_j$, and with $A_j \intersect B_j = \emptyset$ and $A_j \union B_j = X$. Then $A_j$ are $\mu$-null, so $A_1 \union A_2$ is also $\mu$-null. Hence $\mu$ is concentrated on $B_1 \intersect B_2$ and $\nu_1 + \nu_2$ is concentrated on $A_1 \union A_2$. Thus $\mu^\perp$ is a subspace.

    To show that it is closed, let $(\nu_n)$ be a sequence in $\mu^\perp$ converging to a $\nu \in M(\calA,\fieldK)$, and let $A_n$ and $B_n$ be as above. Then $A = \bigunion_{n \in \naturals} A_n$ is also $\mu$-null and $B = \bigintersect_{n \in \naturals} B_n$ is $\nu_n$-null. If $B' \subseteq B$, then
    %
    \begin{equation*}
        \abs{\nu(B')}
            = \abs{\nu_n(B') - \nu(B')}
            \xrightarrow[n \to \infty]{} 0.
    \end{equation*}
    %
    So $B$ is also $\nu$-null, so $\nu \perp \mu$.
\end{solution}


\begin{problem}[4.3.3]
    Show that if $\nu_1$ and $\nu_2$ are singular signed or complex measures such that $\nu_1 + \nu_2$ is well-defined. Then
    %
    \begin{equation*}
        \abs{\nu_1 + \nu_2}(A)
            = \abs{\nu_1}(A) + \abs{\nu_2}(A)
    \end{equation*}
    %
    for all $A \in \calA$. In particular,
    %
    \begin{equation*}
        \norm{\nu_1 + \nu_2}
            = \norm{\nu_1} + \norm{\nu_2}.
    \end{equation*}
    %
    Furthermore, let $\mu$ be a positive measure on $(X,\calA)$, let $\nu$ be a signed or complex measure on $(X,\calA)$, and let $\nu = \nu_a + \nu_s$ be the Lebesgue decomposition of $\nu$. Then $\norm{\nu} = \norm{\nu_a} + \norm{\nu_s}$.
\end{problem}

\begin{solution}
    Assume that $\nu_1 \perp \nu_2$, with $\nu_1$ concentrated on $E$ and $\nu_2$ on $E^c$. Let $(A_i)_{i=1}^m$ and $(B_j)_{j=1}^n$ be partitions of a set $A$, and consider their common refinement $(C_l)_{l=1}^k$. By further refining $(C_j)$ we may assume that either $C_l \subseteq E$ or $C_l \subseteq E^c$ for each $C_l$. Notice that
    %
    \begin{equation*}
        \abs{\nu_1(C_l)} + \abs{\nu_2(C_l)}
            = \abs{\nu_1(C_l) + \nu_2(C_l)},
    \end{equation*}
    %
    since either $\nu_1(C_l)$ or $\nu_2(C_l)$ is zero. Hence we get
    %
    \begin{align*}
        \sum_{i=1}^m \abs{\nu_1(A_i)} + \sum_{j=1}^n \abs{\nu_2(B_j)}
            &\leq \sum_{l=1}^k \abs{\nu_1(C_l)} + \sum_{l=1}^k \abs{\nu_2(C_l)} \\
            &= \sum_{l=1}^k \abs{\nu_1(C_l) + \nu_2(C_l)} \\
            &\leq \abs{\nu_1 + \nu_2}(A).
    \end{align*}
    %
    Taking suprema on the left-hand side we get $\abs{\nu_1}(A) + \abs{\nu_2}(A) \leq \abs{\nu_1 + \nu_2}(A)$. For the opposite inequality, we simply have
    %
    \begin{equation*}
        \sum_{i=1}^m \abs{\nu_1(A_i) + \nu_2(A_i)}
            \leq \sum_{i=1}^m \abs{\nu_1(A_i)} + \sum_{i=1}^m \abs{\nu_2(A_i)}
            \leq \abs{\nu_1}(A) + \abs{\nu_2}(A).
    \end{equation*}
    %
    Again taking the supremum on the left-hand side we get $\abs{\nu_1 + \nu_2}(A) \leq \abs{\nu_1}(A) + \abs{\nu_2}(A)$. Letting $A = X$ we get $\norm{\nu_1 + \nu_2} = \norm{\nu_1} + \norm{\nu_2}$.

    If $\nu = \nu_a + \nu_s$ is a Lebesgue decomposition, we have $\nu_a \ll \mu$ and $\nu_s \perp \mu$, then it follows that $\nu_a \perp \nu_s$. The above then implies the claim.
\end{solution}


\begin{problem}[4.3.7]
    Let $\mu, \nu \in M(\calA,\reals)$. Then
    %
    \begin{equation}
        \label{eq:measure-join-meet-sum}
        \mu \join \nu + \mu \meet \nu
            = \mu + \nu.
    \end{equation}
    %
    In particular, if $\mu$ and $\nu$ are positive, then the conditions
    %
    \begin{enumerate}
        \item $\mu \perp \nu$,
        \item $\mu \meet \nu = 0$, and
        \item $\mu \join \nu = \mu + \nu$
    \end{enumerate}
    %
    are equivalent.
\end{problem}
%
Compare the identity \cref{eq:measure-join-meet-sum} with the identity
%
\newcommand{\lcm}{\operatorname{lcm}}
\begin{equation*}
    \lcm(a,b) \gcd(a,b)
        = ab
\end{equation*}
%
for $a,b \in \ints$, and the identity
%
\begin{equation*}
    \frac{HK}{H}
        \cong \frac{K}{H \intersect K}
\end{equation*}
%
for subgroups $H,K$ of an abelian group. Note also that if $\mu \meet \nu = 0$, then $\mu$ and $\nu$ are automatically positive.

\begin{solution}
    Simply notice that
    %
    \begin{equation*}
        \mu \join \nu + \mu \meet \nu
            = \mu + (\nu - \mu)^+ + \nu - (\nu - \mu)^+
            = \mu + \nu.
    \end{equation*}
    %
    If $\mu$ and $\nu$ are positive, then the second and third conditions are clearly equivalent. If $\mu \perp \nu$ and $\mu$ is concentrated on $E$ and $\nu$ on $E^c$, then
    %
    \begin{equation*}
        (\mu \meet \nu)(A)
            = (\mu \meet \nu)(A \intersect E) + (\mu \meet \nu)(A \intersect E^c)
            \leq \nu(A \intersect E) + \mu(A \intersect E^c)
            = 0.
    \end{equation*}
    %
    For the converse, assume that $\mu \meet \nu = 0$, and let $\mu$ have Hahn decomposition $(E_1,F_1)$ and $\nu$ have Hahn decomposition $(F_2,E_2)$. We claim that $(E,F)$ with $E = E_1 \union E_1$ and $F = F_1 \intersect F_2$ is a Hahn decomposition for $\mu$ and $(F,E)$ for $\nu$. But $\mu \meet \nu = 0$ implies that $\mu = (\mu - \nu)^+ = (\mu - \nu) \join 0$, so if $\mu(A) > 0$ then $\nu(A) = 0$. And if $\nu(A) > 0$ then $\mu(A) = 0$. The claim easily follows from these considerations.
\end{solution}





\nocite{*}
\chapter*{\bibname}
\markboth{\bibname}{\bibname}
\addcontentsline{toc}{chapter}{\bibname}
\printbibliography[heading=none]

\end{document}