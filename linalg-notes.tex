% Document setup
\documentclass[article, a4paper, 11pt, oneside]{memoir}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[UKenglish]{babel}

% Document info
\newcommand\doctitle{Notes on linear algebra}
\newcommand\docauthor{Danny Nyg√•rd Hansen}

% Formatting and layout
\usepackage[autostyle]{csquotes}
\renewcommand{\mktextelp}{(\textellipsis\unkern)}
\usepackage[final]{microtype}
\usepackage{xcolor}
\frenchspacing
\usepackage{latex-sty/articlepagestyle}
\usepackage{latex-sty/articlesectionstyle}

% Fonts
\usepackage{amssymb}
\usepackage[largesmallcaps,partialup]{kpfonts}
\DeclareSymbolFontAlphabet{\mathrm}{operators} % https://tex.stackexchange.com/questions/40874/kpfonts-siunitx-and-math-alphabets
\linespread{1.06}
% \let\mathfrak\undefined
% \usepackage{eufrak}
\DeclareMathAlphabet\mathfrak{U}{euf}{m}{n}
\SetMathAlphabet\mathfrak{bold}{U}{euf}{b}{n}
% https://tex.stackexchange.com/questions/13815/kpfonts-with-eufrak
\usepackage{inconsolata}

% Hyperlinks
\usepackage{hyperref}
\definecolor{linkcolor}{HTML}{4f4fa3}
\hypersetup{%
	pdftitle=\doctitle,
	pdfauthor=\docauthor,
	colorlinks,
	linkcolor=linkcolor,
	citecolor=linkcolor,
	urlcolor=linkcolor,
	bookmarksnumbered=true
}

% Equation numbering
\numberwithin{equation}{chapter}

% Footnotes
\footmarkstyle{\textsuperscript{#1}\hspace{0.25em}}

% Mathematics
\usepackage{latex-sty/basicmathcommands}
\usepackage{latex-sty/framedtheorems}
\usepackage{tikz-cd}
\tikzcdset{arrow style=math font} % https://tex.stackexchange.com/questions/300352/equalities-look-broken-with-tikz-cd-and-math-font
\usetikzlibrary{babel}

% Lists
\usepackage{enumitem}
\setenumerate[0]{label=\normalfont(\arabic*)}

% Bibliography
\usepackage[backend=biber, style=authoryear, maxcitenames=2, useprefix]{biblatex}
\addbibresource{references.bib}

% Title
\title{\doctitle}
\author{\docauthor}

\newcommand{\calM}{\mathcal{M}}
\newcommand{\calL}{\mathcal{L}}
\newcommand{\calV}{\mathcal{V}}
\newcommand{\calW}{\mathcal{W}}
\newcommand{\calU}{\mathcal{U}}
\newcommand{\calE}{\mathcal{E}}
\DeclareMathOperator{\sign}{sgn}
\DeclareMathOperator{\adj}{adj}
\DeclareMathOperator{\spec}{Spec}
\DeclareMathOperator{\trace}{tr}


\usepackage{listofitems}
\setsepchar{,}

\makeatletter

\newcommand{\mat@dims}[1]{%
    \readlist*\@dims{#1}%
    \ifnum \@dimslen=1
        \def\@dimsout{\@dims[1]}%
    \else
        \def\@dimsout{\@dims[1], \@dims[2]}%
    \fi
    \@dimsout
}

\newcommand{\matgroup}[3]{\mathrm{#1}_{#2}(#3)}
\newcommand{\matGL}[2]{\matgroup{GL}{#1}{#2}}
\newcommand{\trans}{^{\top}}
% \newcommand{\mat}[2]{\calM_{\mat@dims{#1}}(#2)}
\newcommand{\mat}[2]{\mathrm{Mat}_{\mat@dims{#1}}(#2)}
\newcommand{\matO}[1]{\mathrm{O}(#1)}
\newcommand{\matSO}[1]{\mathrm{SO}(#1)}
\newcommand{\field}{\mathbb{F}}
\DeclareMathOperator{\chr}{char}

% Break
\usepackage{adforn}
\newcommand\fleuronbreak{\fancybreak{\textcolor{linkcolor}{\adfhangingflatleafleft}}}


%%%%%%%%%%%%%%%% Gray codes TODO
\DeclarePairedDelimiter{\floor}{\lfloor}{\rfloor}

\makeatother


\begin{document}

\maketitle

\chapter{Linear equations and matrices}

\section{Linear equations}

Throughout we let $\field$ denote an arbitrary field and $R$ a commutative ring. Let $m$ and $n$ be positive integers. A \emph{linear equation in $n$ unknowns} is an equation on the form
%
\begin{equation*}
    l \colon a_1 x_1 + \cdots + a_n x_n = b,
\end{equation*}
%
where $a_1, \ldots, a_n, b \in \field$. A \emph{solution} to $l$ is an element $v = (v_1, \ldots, v_n) \in \field^n$ such that
%
\begin{equation*}
    a_1 v_1 + \cdots + a_n v_n = b.
\end{equation*}
%
A \emph{system of linear equations in $n$ unknowns} is a tuple $L = (l_1, \ldots, l_m)$, where each $l_i$ is a linear equation in $n$ unknowns. An element $v \in \field^n$ is a \emph{solution} to $L$ if it is a solution to each linear equation $l_1, \ldots, l_m$.

Let $L$ and $L'$ be systems of linear equations in $n$ unknowns. We say that $L$ and $L'$ are \emph{solution equivalent} if they have the same solutions. Furthermore, we say that they are \emph{combination equivalent} if each equation in $L'$ is a linear combination of the equations in $L$, and vice versa. Clearly, if $L$ and $L'$ are combination equivalent they are also solution equivalent, but the converse does not hold.


\section{Matrices}

It is well-known that a system of linear equations is equivalent to a matrix equation on the form $Ax = b$, where $A \in \mat{m,n}{\field}$, $x \in \field^n$ and $b \in \field^m$. Recall the \emph{elementary row operations} on $A$:
%
\begin{enumerate}
    \item multiplication of one row of $A$ by a nonzero scalar,
    \item addition to one row of $A$ a scalar multiple of another (different) row, and
    \item interchange of two rows of $A$.
\end{enumerate}
%
If $e$ is an elementary row operation, we write $e(A)$ for the matrix obtained when applying $e$ to $A$. Clearly each elementary row operation $e$ has an \enquote{inverse}, i.e. an elementary row operation $e'$ such that $e'(e(A)) = e(e'(A)) = A$. Two matrices $A,B \in \mat{m,n}{\field}$ are called \emph{row-equivalent} if $A$ is obtained by applying a finite sequence of elementary row operations to $B$ (and vice versa, though this need not be assumed since each elementary row operation has an inverse).

Clearly, if $A, B \in \mat{m,n}{\field}$ are row-equivalent, then the systems of equations $Ax = 0$ and $Bx = 0$ are combination equivalent, hence have the same solutions.

\begin{definition}
    A matrix $H \in \mat{m,n}{\field}$ is called \emph{row-reduced} if
    %
    \begin{enumdef}
        \item the first nonzero entry of each nonzero row in $H$ is $1$, and
        \item each column of $H$ containing the leading nonzero entry of some row has all its other entries equal $0$.
    \end{enumdef}
    %
    If $H$ is row-reduced, it is called a \emph{row-reduced echelon matrix} if it also has the following properties:
    %
    \begin{enumdef}[resume]
        \item Every row of $H$ only containing zeroes occur below every row which has a nonzero entry, and
        \item if rows $1, \ldots, r$ are the nonzero rows of $H$, and if the leading nonzero entry of row $i$ occurs in column $k_i$, then $k_1 < \cdots < k_r$.
    \end{enumdef}
\end{definition}

An \emph{elementary matrix} is a matrix obtained by applying a single elementary row operation to the identity matrix $I$. It is easy to show that if $e$ is an elementary row operation and $E = e(I) \in \mat{m}{\field}$, then $e(A) = EA$ for $A \in \mat{m,n}{\field}$. If $B \in \mat{m,n}{\field}$, then $A$ and $B$ are row-equivalent if and only if $A = PB$, where $P \in \mat{m}{\field}$ is a product of elementary matrices.

\begin{proposition}
    Every matrix in $\mat{m,n}{\field}$ is row-equivalent to a unique row-reduced echelon matrix.
\end{proposition}

\begin{proof}
    The usual Gauss--Jordan elimination algorithm proves existence. If $H, K \in \mat{m,n}{R}$ are row-equivalent row-reduced echelon matrices, we claim that $H = K$. We prove this by induction in $n$. If $n = 1$ then this is obvious, so assume that $n > 1$. Let $H_1$ and $K_1$ be the matrices obtained by deleting the $n$th column in $H$ and $K$ respectively. Then $H_1$ and $K_1$ are also row-equivalent\footnote{It should be obvious that deleting columns preserves row-equivalence, but we give a more precise argument: If $P \in \mat{m}{\field}$ is a product of elementary matrices and $a_1, \ldots, a_n \in \field^m$ are the columns in $A$, then the columns in $PA$ are $Pa_1, \ldots, Pa_m$. Thus elementary row operations are applied to each column independently of the other columns.} and row-reduced echelon matrices, so by induction $H_1 = K_1$. Thus if $H$ and $K$ differ, they must differ in the $n$th column.

    Let $H_2$ be the matrix obtained by deleting columns in $H$, only keeping those columns containing pivots, as well as keeping the $n$th column. Define $K_2$ similarly. Thus we have deleted the same columns in $H$ and $K$, so $H_2$ and $K_2$ are also row-equivalent. Say that the number of columns in $H_2$ and $K_2$ is $r+1$, and write the matrices on the form
    %
    \begin{equation*}
        H_2
            = \begin{pmatrix}
                I_r & h \\
                0   & h'
            \end{pmatrix}
        \quad \text{and} \quad
        K_2
            = \begin{pmatrix}
                I_r & k \\
                0   & k'
            \end{pmatrix},
    \end{equation*}
    %
    where $h,k \in \field^r$ and $h',k' \in \field^{m-r}$ are column vectors. Since $H_2$ and $K_2$ are row-equivalent, the systems $H_2 x = 0$ and $K_2 x = 0$ are solution equivalent. If $h' = 0$, then $H_2 x = 0$ has the solution $(-h,1)$. But this is also a solution to $K_2 x = 0$, so $h = k$ and $k' = 0$. If $h' \neq 0$, then $H_2 x = 0$ only has the trivial solution. But then $K_2 x = 0$ also only has the trivial solution, and hence $k' \neq 0$. But that must be because both $H_2$ and $K_2$ has a pivot in the rightmost column, so also in this case $H_2 = K_2$.
\end{proof}


\section{Invertible matrices}

Notice that elementary matrices are invertible, since elementary row operations are invertible.

\begin{lemma}
    If $A \in \mat{n}{\field}$, then the following are equivalent:
    %
    \begin{enumlem}
        \item \label{enum:lemma-A-invertible} $A$ is invertible,
        \item \label{enum:lemma-A-equivalent-to-I} $A$ is row-equivalent to $I_n$,
        \item \label{enum:lemma-A-elementary-matrix-product} $A$ is a product of elementary matrices, and
        \item \label{enum:lemma-only-trivial-solution} the system $Ax = 0$ has only the trivial solution $x = 0$.
    \end{enumlem}
\end{lemma}
% I don't think we will need (iii), but I guess I'll leave it.

\begin{proof}
\begin{proofsec}
    \item[\subcref{enum:lemma-A-invertible} $\Leftrightarrow$ \subcref{enum:lemma-A-equivalent-to-I}]
    Let $H \in \mat{n}{\field}$ be a row-reduced echelon matrix that is row-equivalent to $A$. Then $H = PA$, where $P \in \mat{n}{\field}$ is a product of elementary matrices. Then $A = P\inv H$, so $A$ is invertible if and only if $H$ is. But the only invertible row-reduced echelon matrix is the identity matrix, so \subcref{enum:lemma-A-invertible} and \subcref{enum:lemma-A-equivalent-to-I} are equivalent.
    
    \item[\subcref{enum:lemma-A-equivalent-to-I} $\implies$ \subcref{enum:lemma-A-elementary-matrix-product}]
    As above, there exists a product $P$ of elementary matrices such that $I_n = PA$, so $A = P\inv$.

    \item[\subcref{enum:lemma-A-elementary-matrix-product} $\implies$ \subcref{enum:lemma-A-invertible}]
    This is obvious since elementary matrices are invertible.

    \item[\subcref{enum:lemma-A-equivalent-to-I} $\Leftrightarrow$ \subcref{enum:lemma-only-trivial-solution}]
    If $A$ and $I_n$ are row-equivalent, then the systems $Ax = 0$ and $I_n x = 0$ have the same solutions. Conversely, assume that $Ax = 0$ only has the trivial solution. If $H \in \mat{m,n}{\field}$ is a row-reduced echelon matrix that is row-equivalent to $A$, then $Hx = 0$ has no nontrivial solution. Thus if $r$ is the number of nonzero rows in $H$, then $r \geq n$. But then $r = n$, so $H$ must be the identity matrix.
\end{proofsec}
\end{proof}


\begin{proposition}
    Let $A \in \mat{n}{\field}$. Then the following are equivalent:
    %
    \begin{enumprop}
        \item $A$ is invertible,
        \item $A$ has a left inverse, and
        \item $A$ has a right inverse.
    \end{enumprop}
\end{proposition}

\begin{proof}
    If $A$ has a left inverse, then $Ax = 0$ has no nontrivial solution, so $A$ is invertible. If $A$ has a right inverse $B \in \mat{n}{\field}$, i.e. $AB = I$, then $B$ has a left inverse and is thus invertible. But then $A$ is the inverse of $B$ and hence is itself invertible.
\end{proof}


\chapter{Coordinates}

\newcommand{\coordmap}[1]{\phi_{#1}}
\newcommand{\coordvec}[2]{[#1]_{#2}}
\newcommand{\basischange}[2]{\phi_{#1,#2}}
\newcommand{\mr}[3]{{}_{#1}[#2]_{#3}}
\newcommand{\basischangemat}[2]{\mr{#1}{\square}{#2}}
\newcommand{\lin}{\calL}
\newcommand{\smr}[1]{\calM(#1)} % standard matrix representation

\newcommand{\colvec}[1]{\begin{pmatrix}#1\end{pmatrix}}

For $A \in \mat{m,n}{\field}$ we define the map $M_A \colon \field^n \to \field^m$ by $M_A v = Av$.

\begin{proposition}
    \label{prop:smr-properties}
    Let $(e_1, \ldots, e_n)$ be the standard basis for $\field^n$. The map
    %
    \begin{align*}
        \calM \colon \lin(\field^n, \field^m) &\to \mat{m,n}{\field}, \\
        T &\mapsto \bigl( Te_1 \mid \cdots \mid Te_n \bigr),
    \end{align*}
    %
    is a linear isomorphism with inverse $A \mapsto M_A$. The matrix $\smr{T}$ is called the \emph{standard matrix representation} of $T$. If $T \colon \field^n \to \field^m$ and $S \colon \field^m \to \field^l$ are linear maps, then
    %
    \begin{enumprop}
        \item \label{enum:smr-vector-multiplication} $Tv = \smr{T}v$ for all $v \in \field^n$.
        
        \item \label{enum:smr-of-identity-map} $\smr{\id_{\field^n}} = I$.

        \item \label{enum:smr-multiplicative} $\smr{S \circ T} = \smr{S} \smr{T}$.

        \item \label{enum:smr-invertibility} $T$ is invertible if and only if $\smr{T}$ is invertible, in which case $\smr{T\inv} = \smr{T}\inv$.
    \end{enumprop}
\end{proposition}

\begin{proof}
    The map $A \mapsto M_A$ is clearly linear, so to prove the first point it suffices to show that this is the inverse of $\calM$. Let $T \in \lin(\field^n,\field^m)$. Then
    %
    \begin{equation*}
        M_{\smr{T}} \colvec{\alpha_1 \\ \vdots \\ \alpha_n}
            = \smr{T} \colvec{\alpha_1 \\ \vdots \\ \alpha_n}
            = \bigl( Te_1 \mid \cdots \mid Te_n \bigr) \colvec{\alpha_1 \\ \vdots \\ \alpha_n}
            = \sum_{i=1}^n \alpha_i Te_i
            = T \colvec{\alpha_1 \\ \vdots \\ \alpha_n}
    \end{equation*}
    %
    for $\alpha_1, \ldots, \alpha_n \in \field$. Conversely, for $A \in \mat{m,n}{\field}$ we have
    %
    \begin{equation*}
        \smr{M_A}
            = \bigl( M_A e_1 \mid \cdots \mid M_A e_n \bigr)
            = \bigl( A e_1 \mid \cdots \mid A e_n \bigr)
            = A,
    \end{equation*}
    %
    since $Ae_i$ is the $i$th column of $A$. We prove the remaining claims:
    %
    \begin{proofsec}
        \item[Proof of \subcref{enum:smr-vector-multiplication}]
        Simply notice that $Tv = M_{\smr{T}}v = \smr{T}v$.

        \item[Proof of \subcref{enum:smr-of-identity-map}]
        This is obvious from the definition of $\calM$.

        \item[Proof of \subcref{enum:smr-multiplicative}]
        Let $v \in \field^n$ and notice that
        %
        \begin{equation*}
            \smr{S \circ T}v
                = (S \circ T) v
                = S(Tv)
                = S(\smr{T}v)
                = \smr{S}\smr{T}v
        \end{equation*}
        %
        by \subcref{enum:smr-vector-multiplication}. Since this holds for all $v$, the claim follows.

        \item[Proof of \subcref{enum:smr-invertibility}]
        This follows easily from \subcref{enum:smr-of-identity-map} and \subcref{enum:smr-multiplicative}.
    \end{proofsec}
\end{proof}

Let $V$ be a finite-dimensional $\field$-vector space. If $\calV = (v_1, \ldots, v_n)$ is an ordered basis for $V$, then for every $v \in V$ there are unique $\alpha_1, \ldots, \alpha_n \in \field$ such that $v = \sum_{i=1}^n \alpha_i v_i$. Hence the map $\coordmap{\calV} \colon V \to \field^n$ given by $\coordmap{\calV}(v) = (\alpha_1, \ldots, \alpha_n)$ is well-defined. Furthermore, it is clearly linear, and since $\calV$ is a basis it is also bijective, hence a linear isomorphism. The map $\coordmap{\calV}$ is called the \emph{coordinate map} with respect to $\calV$, and the vector $\coordvec{v}{\calV} = \coordmap{\calV}(v)$ is called the \emph{coordinate vector} of $v$ with respect to $\calV$.

Now let $\calW$ be another ordered basis for $V$. The composition $\basischange{\calW}{\calV} = \coordmap{\calW} \circ \coordmap{\calV}\inv$ is called the \emph{change of basis operator} from $\calV$ to $\calW$, and this makes the diagram
%
\begin{equation}
    \label{eq:change-of-basis-diagram}
    \begin{tikzcd}[row sep=small]
        & \field^n
            \ar[dd, "\basischange{\calW}{\calV}"] \\
        V
            \ar[ru, "\coordmap{\calV}"]
            \ar[rd, "\coordmap{\calW}", swap] \\
        & \field^n
    \end{tikzcd}
\end{equation}
%
commute. Its standard matrix is denoted $\basischangemat{\calW}{\calV}$. This has the expected properties:

\begin{proposition}
    Let $\calV$, $\calW$ and $\calU$ be ordered bases for a finite-dimensional $\field$-vector space $V$. Then
    %
    \begin{enumprop}
        \item \label{enum:basis-change-coordvec} $\coordvec{v}{\calW} = \basischange{\calW}{\calV} (\coordvec{v}{\calV})$ for all $v \in V$. In particular, $\coordvec{v}{\calW} = \basischangemat{\calW}{\calV} \cdot \coordvec{v}{\calV}$.

        \item \label{enum:basis-change-identity-map} $\basischange{\calV}{\calV}$ is the identity map. In particular, $\basischangemat{\calV}{\calV}$ is the identity matrix.

        \item $\basischange{\calU}{\calW} \circ \basischange{\calW}{\calV} = \basischange{\calU}{\calV}$. In particular, $\basischangemat{\calU}{\calW} \cdot \basischangemat{\calW}{\calV} = \basischangemat{\calU}{\calV}$.

        \item $\basischange{\calW}{\calV}$ (resp. $\basischangemat{\calW}{\calV}$) is invertible with inverse $\basischange{\calV}{\calW}$ (resp. $\basischangemat{\calV}{\calW}$).
    \end{enumprop}
\end{proposition}

\begin{proof}
    All claims about change of basis matrices follow by \cref{prop:smr-properties} from the corresponding claims about change of basis operators.

    The claim \subcref{enum:basis-change-coordvec} follows by commutativity of the diagram \cref{eq:change-of-basis-diagram}, i.e.
    %
    \begin{equation*}
        \basischange{\calW}{\calV} (\coordvec{v}{\calV})
            = (\coordmap{\calW} \circ \coordmap{\calV}\inv) \circ \coordmap{\calV}(v)
            = \coordmap{\calW}(v)
            = \coordvec{v}{\calW}.
    \end{equation*}
    %
    Claim \subcref{enum:basis-change-identity-map} is an immediate consequence of the definition of $\basischange{\calV}{\calV}$. The remaining claims are proved similarly to \subcref{enum:basis-change-coordvec}.
\end{proof}

Next consider a linear map $T \colon V \to W$. If $\calV \in V^n$ and $\calW \in W^m$ are bases for $V$ and $W$ respectively, then the diagram
%
\begin{equation*}
    \begin{tikzcd}
        V
            \ar[d, "T", swap]
            \ar[r, "\coordmap{\calV}"]
        & \field^n
            \ar[d, "\coordmap{\calW} \circ T \circ \coordmap{\calV}\inv"] \\
        W
            \ar[r, "\coordmap{\calW}", swap]
        & \field^n
    \end{tikzcd}
\end{equation*}
%
commutes. The map $\coordmap{\calW} \circ T \circ \coordmap{\calV}\inv$ is the \emph{basis representation of $T$} with respect to the bases $\calV$ and $\calW$. This is a linear map $\field^n \to \field^m$, so it has a standard matrix which we denote $\mr{\calW}{T}{\calV}$. This is called the \emph{matrix representation} of $T$ with respect to the bases $\calV$ and $\calW$.

\begin{proposition}
    \label{prop:mr-properties}
    Let $V$ and $W$ be finite-dimensional $\field$-vector spaces with ordered bases $\calV \in V^n$ and $\calW \in W^m$, respectively. The map
    %
    \begin{align*}
        \mr{\calW}{\,\cdot\,}{\calV} \colon \lin(V,W) &\to \mat{m,n}{\field}, \\
        T &\mapsto \mr{\calW}{T}{\calV},
    \end{align*}
    %
    is a linear isomorphism. Let $T \colon V \to W$ and $S \colon W \to U$ be linear maps, and let $\calU \in U^l$ be an ordered basis for $U$. Then
    %
    \begin{enumprop}
        \item \label{enum:mr-vector-multiplication} $\coordvec{Tv}{\calW} = \mr{\calW}{T}{\calV} \cdot \coordvec{v}{\calV}$ for all $v \in V$.

        \item \label{enum:mr-of-identity-map} If $\calV'$ is another basis for $V$, then $\mr{\calV'}{\id_V}{\calV} = \basischangemat{\calV'}{\calV}$.

        \item \label{enum:mr-multiplicative} $\mr{\calU}{S \circ T}{\calV} = \mr{\calU}{S}{\calW} \cdot \mr{\calW}{T}{\calV}$.

        \item \label{enum:mr-invertibility} $T$ is invertible if and only if $\mr{\calW}{T}{\calV}$ is invertible, in which case $\mr{\calV}{T\inv}{\calW} = \mr{\calW}{T}{\calV}\inv$.
    \end{enumprop}
\end{proposition}

\begin{proof}
    For the first claim, notice that the map $T \mapsto \coordmap{\calW} \circ T \circ \coordmap{\calV}\inv$ is a linear isomorphism, since pre- and postcomposition with linear isomorphisms are themselves linear isomorphisms. Composing this map with $\calM$ yields $\mr{\calW}{\,\cdot\,}{\calV}$, so this is a linear isomorphism by \cref{prop:smr-properties}.
    %
    \begin{proofsec}
        \item[Proof of \subcref{enum:mr-vector-multiplication}]
        Notice that
        %
        \begin{align*}
            \coordvec{Tv}{\calW}
                &= (\coordmap{\calW} \circ T)(v) \\
                &= (\coordmap{\calW} \circ T \circ \coordmap{\calV}\inv) \circ \coordmap{\calV}(v) \\
                &= (\coordmap{\calW} \circ T \circ \coordmap{\calV}\inv)(\coordvec{v}{\calV}) \\
                &= \mr{\calW}{T}{\calV} \cdot \coordvec{v}{\calV}.
        \end{align*}
        %
        where the last equality follows from \cref{enum:smr-vector-multiplication}.

        \item[Proof of \subcref{enum:mr-of-identity-map}]
        This is obvious from the definitions of $\mr{\calV'}{\id_V}{\calV}$ and $\basischangemat{\calV'}{\calV}$.

        \item[Proof of \subcref{enum:mr-multiplicative}]
        Notice that
        %
        \begin{equation*}
            \coordmap{\calU} \circ (S \circ T) \circ \coordmap{\calV}\inv
                = (\coordmap{\calU} \circ S \circ \coordmap{\calW}\inv) \circ (\coordmap{\calW} \circ T \circ \coordmap{\calV}\inv)
        \end{equation*}
        %
        The claim then follows from \cref{enum:smr-multiplicative}.

        \item[Proof of \subcref{enum:mr-invertibility}]
        This is an immediate consequence of either \subcref{enum:mr-multiplicative} or of \cref{enum:smr-invertibility}.
    \end{proofsec}
\end{proof}


\begin{proposition}
    Let $\calV = (v_1, \ldots, v_n)$ be an ordered basis for an $\field$-vector space $V$, and let $T \colon V \to V$ be a linear isomorphism. Let $\calW = (w_1, \ldots, w_n)$ where $w_i = Tv_i$. Then $\calW$ is an ordered basis for $V$ and
    %
    \begin{equation*}
        \basischange{\calW}{\calV}
            = \coordmap{\calV} \circ T\inv \circ \coordmap{\calV}\inv,
        \quad \text{or} \quad
        \basischangemat{\calW}{\calV}
            = \mr{\calV}{T\inv}{\calV}.
    \end{equation*}
    %
    In particular, if $V = \field^n$ and $\calV$ is the standard basis $\calE$, then
    %
    \begin{equation*}
        \basischange{\calW}{\calE}
            = T\inv,
        \quad \text{or} \quad
        \basischangemat{\calW}{\calE}
            = \smr{T\inv}.
    \end{equation*}
\end{proposition}
%
We think of this result as follows: If we change basis by applying an invertible linear transformation $T$, we obtain the coordinate vectors corresponding to the transformed basis by applying $T\inv$ (in the old basis). This says that if we perform a \emph{passive transformation}, i.e. a change of basis while keeping vectors themselves fixed, the coordinates change by the inverse of said transformation.

\begin{proof}
    Let $v \in V$ and write $v = \sum_{i=1}^n \alpha_i v_i$. Then
    %
    \begin{equation*}
        Tv
            = \sum_{i=1}^n \alpha_i Tv_i
            = \sum_{i=1}^n \alpha_i w_i
            = \coordmap{\calW}\inv \colvec{\alpha_1 \\ \vdots \\ \alpha_n}
            = \coordmap{\calW}\inv \circ \coordmap{\calV}(v),
    \end{equation*}
    %
    implying that
    %
    \begin{equation*}
        \basischange{\calW}{\calV}
            = \coordmap{\calW} \circ \coordmap{\calV}\inv
            = (T \circ \coordmap{\calV}\inv)\inv \circ \coordmap{\calV}\inv
            = \coordmap{\calV} \circ T\inv \circ \coordmap{\calV}\inv
    \end{equation*}
    %
    as claimed.
\end{proof}

[TODO] Recall that two matrices $A,B \in \mat{n}{\field}$ are \emph{similar} if there exists an invertible matrix $P \in \mat{n}{\field}$ such that $A = PBP\inv$. 


\chapter{Determinants}

\section{Existence of determinants}

If $M_1, \ldots, M_n, N$ are modules over a commutative ring $R$, a map
%
\begin{equation*}
    \phi \colon M_1 \prod \cdots \prod M_n \to N
\end{equation*}
%
is called \emph{$n$-linear} if, for all $i$, the maps $m_i \mapsto \phi(m_1, \ldots, m_n)$ are linear for all choices of $m_j \in M_j$ where $j \neq i$. Since there is a natural isomorphism $\mat{m,n}{R} \cong (R^n)^m$, a map $\phi \colon \mat{m,n}{R} \to N$ that is linear in each row is also called $n$-linear.

In the case $M_1 = \cdots = M_n$, we call $\phi$ \emph{alternating} if $\phi(m_1, \ldots, m_n) = 0$ whenever $m_i = m_j$ for some $i \neq j$. Furthermore, $\phi$ is called \emph{skew-symmetric} if
%
\begin{multline*}
    \phi(m_1, \ldots, m_{i-1}, m_i, m_{i+1}, \ldots, m_{j-1}, m_j, m_{j+1}, \ldots, m_n) \\
        = -\phi(m_1, \ldots, m_{i-1}, m_j, m_{i+1}, \ldots, m_{j-1}, m_i, m_{j+1}, \ldots, m_n)
\end{multline*}
%
for all $i < j$.

\begin{lemma}
    Let $M$ and $N$ be $R$-modules, and let $\phi \colon M^n \to N$ be an $n$-linear map.
    %
    \begin{enumlem}
        \item \label{enum:alternating-implies-skew-symmetric} If $\phi$ is alternating, then $\phi$ is skew-symmetric. If $\chr R \neq 2$ then the converse also holds.
        \item \label{enum:alternating-adjacent-rows} If $\phi(m_1, \ldots, m_n) = 0$ whenever $m_i = m_{i+1}$ for some $i = 1, \ldots, n-1$, then $\phi$ is alternating.
    \end{enumlem}
\end{lemma}
%
We shall not use the converse direction of \cref{enum:alternating-implies-skew-symmetric} but we include it for completeness.

\begin{proof}
\begin{proofsec}
    \item[Proof of \subcref{enum:alternating-implies-skew-symmetric}]
    Consider $m_1, \ldots, m_n \in M$, and let $1 \leq i < j \leq n$. Define a map $\psi \colon M \prod M \to N$ by
    %
    \begin{equation*}
        \psi(a, b)
            = \phi(m_1, \ldots, m_{i-1}, a, m_{i+1}, \ldots, m_{j-1}, b, m_{j+1}, \ldots, m_n),
    \end{equation*}
    %
    and notice that it suffices to show that $\psi(m_i,m_j) = -\psi(m_j,m_i)$. But $\psi$ is $2$-linear and alternating, so for $a,b \in M$ we have
    %
    \begin{equation*}
        \psi(a+b, a+b)
            = \psi(a,a) + \psi(a,b) + \psi(b,a) + \psi(b,b)
            = \psi(a,b) + \psi(b,a).
    \end{equation*}
    %
    Thus $\psi(m_i,m_j) = -\psi(m_j,m_i)$, so $\phi$ is skew-symmetric as claimed.

    Conversely, if $\chr R \neq 2$ and $\psi$ is skew-symmetric, then since $\psi(a,b) = -\psi(b,a)$, letting $a = b$ we have $2\psi(a,a) = 0$, so $\psi(a,a) = 0$.

    \item[Proof of \subcref{enum:alternating-adjacent-rows}] 
    The argument above shows that, in particular, if $A, B \in M^n$, and $B$ is obtained from $A$ by interchanging two adjacent elements, then $\phi(B) = -\phi(A)$. Assuming now that $B$ is obtained from $A$ by interchanging the $i$th and $j$th elements in $A$, with $i < j$, we claim that we may obtain $B$ by successively interchanging adjacent elements of $A$. Writing $A = (m_1, \ldots, m_n)$, we first perform $j - i$ such interchanges and arrive that the tuple
    %
    \begin{equation*}
        (m_1, \ldots, m_{i-1}, m_{i+1}, \ldots, m_{j-1}, m_j, m_i, m_{j+1}, \ldots, m_n),
    \end{equation*}
    %
    moving $m_i$ to the right $j - i$ places. Next we perform another $j-i-1$ interchanges, moving $m_j$ to the left until we reach
    %
    \begin{equation*}
        B = (m_1, \ldots, m_{i-1}, m_j, m_{i+1}, \ldots, m_{j-1}, m_i, m_{j+1}, \ldots, m_n).
    \end{equation*}
    %
    Since each interchange results in a sign change, we have
    %
    \begin{equation*}
        \phi(B) = (-1)^{2(j-i) - 1} \phi(A) = -\phi(A).
    \end{equation*}
    %
    If $m_i = m_j$ for $i < j$, then we claim that $\phi(A) = 0$. For let $B$ be obtained from $A$ by interchanging $m_{i+1}$ and $m_j$. Then $\phi(B) = 0$, so $\phi(A) = -\phi(B) = 0$ by the above argument, and hence $\phi$ is alternating as claimed.
    \end{proofsec}
\end{proof}

\begin{definition}[Determinant functions]
    If $n$ be a positive integer, a \emph{determinant function} is a map $\phi \colon \mat{n}{R} \to R$ that is $n$-linear, alternating, and which satisfies $\phi(I_n) = 1$.
\end{definition}
%
If $A \in \mat{n}{R}$ with $n > 1$ and $1 \leq i,j \leq n$, denote by $M(A)_{i,j}$ the matrix in $\mat{n-1}{R}$ obtained by removing the the $i$th row and the $j$th column of $A$. This is called the \emph{$(i,j)$-th minor} of $A$. If $\phi \colon \mat{n-1}{R} \to R$ is an $(n-1)$-linear function and $A \in \mat{n}{R}$, then we write $\phi_{i,j}(A) = \phi(M(A)_{i,j})$. Then $\phi_{i,j} \colon \mat{n}{R} \to R$ is clearly linear in all rows except row $i$, and is independent of row $i$.

\begin{theorem}[Construction of determinants]
    \label{thm:determinant-recursive-definition}
    Let $n > 1$, and let $\phi \colon \mat{n-1}{R} \to R$ be alternating and $(n-1)$-linear. For $j = 1, \ldots, n$ define a map $\psi_j \colon \mat{n}{R} \to R$ by
    %
    \begin{equation*}
        \psi_j(A)
            = \sum_{i=1}^n (-1)^{i+j} a_{ij} \phi_{i,j}(A),
    \end{equation*}
    %
    for $A = (a_{ij}) \in \mat{n}{R}$. Then $\psi_j$ is alternating and $n$-linear. If $\phi$ is a determinant function, then so is $\psi_j$.
\end{theorem}

\begin{proof}
    Let $A = (a_{ij}) \in \mat{n}{R}$. Then $A \mapsto a_{ij}$ is independent of all rows except row $i$, and $\phi_{i,j}$ is linear in all rows except row $i$. Thus $A \mapsto a_{ij} \phi_{i,j}(A)$ is linear in all rows except row $i$. Conversely, $A \mapsto a_{ij}$ is linear in row $i$, and $\phi_{i,j}$ is independent of row $i$, so $A \mapsto a_{ij} \phi_{i,j}(A)$ is also linear in row $i$. Since $\psi_j$ is a linear combination of $n$-linear maps, is it itself $n$-linear.

    Now assume that $A$ has two equal adjacent rows, say $a_k, a_{k+1} \in R^n$. If $i \neq k$ and $i \neq k+1$, then $M(A)_{i,j}$ has two equal rows, so $\phi_{i,j}(A) = 0$. Thus
    %
    \begin{equation*}
        \psi_j(A)
            = (-1)^{k+j} a_{kj} \phi_{k,j}(A)
              + (-1)^{k+1+j} a_{(k+1)j} \phi_{k+1,j}(A).
    \end{equation*}
    %
    Since $a_k = a_{k+1}$ we also have $a_{kj} = a_{(k+1)j}$ and $M(A)_{k,j} = M(A)_{k+1,j}$. Thus $\psi_j(A) = 0$, so \cref{enum:alternating-adjacent-rows} implies that $\psi_j$ is alternating.

    Finally suppose that $\phi$ is a determinant function. Then $M(I_n)_{j,j} = I_{n-1}$ and we have
    %
    \begin{equation*}
        \psi_j(I_n)
            = (-1)^{j+j} \phi_{j,j}(I_n)
            = \phi(I_{n-1})
            = 1,
    \end{equation*}
    %
    so $\psi_j$ is also a determinant function.
\end{proof}


\begin{corollary}[Existence of determinants]
    For every positive integer $n$, there exists a determinant function $\mat{n}{R} \to R$.
\end{corollary}

\begin{proof}
    The identity map on $\mat{1}{R} \cong R$ is a determinant function for $n = 1$, and \cref{thm:determinant-recursive-definition} allows us to recursively construct a determinant for each $n > 1$.
\end{proof}


\section{Uniqueness of determinants}

\begin{theorem}[Uniqueness of determinants]
    \label{thm:determinant-uniqueness}
    Let $n$ be a positive integer. There is precisely one determinant function on $\mat{n}{R}$, namely the function $\det \colon \mat{n}{R} \to R$ given by
    %
    \begin{equation*}
        \det A
            = \sum_{\sigma \in S_n} (\sign\sigma) a_{1 \sigma(1)} \cdots a_{n \sigma(n)}
    \end{equation*}
    %
    for $A = (a_{ij}) \in \mat{n}{R}$. If $\phi \colon \mat{n}{R} \to R$ is any alternating $n$-linear function, then
    %
    \begin{equation*}
        \phi(A)
            = (\det A) \phi(I_n).
    \end{equation*}
\end{theorem}
%
We use the notation $\det$ for the unique determinant on $\mat{n}{R}$ for all $n$.

\begin{proof}
    Let $e_1, \ldots, e_n$ denote the rows of $I_n$, and denote the rows of a matrix $A = (a_{ij}) \in \mat{n}{R}$ by $a_1, \ldots, a_n$. Then $a_i = \sum_{j=1}^n a_{ij} e_j$, so
    %
    \begin{equation*}
        \phi(A)
            = \sum_{k_1, \ldots, k_n} a_{1k_1} \cdots a_{nk_n} \phi(e_{k_1}, \ldots, e_{k_n}),
    \end{equation*}
    %
    where the sum is taken over all $k_i = 1, \ldots, n$. Since $\phi$ is alternating we have $\phi(e_{k_1}, \ldots, e_{k_n}) = 0$ if two of the indices $k_1, \ldots, k_n$ are equal. Thus it suffices to sum over those sequences $(k_1, \ldots, k_n)$ that are permutations of $(1, \ldots, n)$, and so
    %
    \begin{equation*}
        \phi(A)
            = \sum_{\sigma \in S_n} a_{1 \sigma(1)} \cdots a_{n \sigma(n)} \phi(e_{\sigma(1)}, \ldots, e_{\sigma(n)}).
    \end{equation*}
    %
    Next notice that, since $\phi$ is also skew-symmetric by \cref{enum:alternating-implies-skew-symmetric}, we have $\phi(e_{\sigma(1)}, \ldots, e_{\sigma(n)}) = (-1)^m \phi(e_1, \ldots, e_n)$, where $m$ is the number of transpositions of $(1, \ldots, n)$ it takes to obtain the permutation $(\sigma(1), \ldots, \sigma(n))$. But then $(-1)^m$ is just the sign of $\sigma$, so
    %
    \begin{equation*}
        \phi(A)
            = \sum_{\sigma \in S_n} (\sign\sigma) a_{1 \sigma(1)} \cdots a_{n \sigma(n)} \phi(I_n).
    \end{equation*}
    %
    Finally, if $\phi$ is a determinant function, then $\phi(I_n) = 1$, so we must have $\phi = \det$. The rest of the theorem follows directly from this.
\end{proof}


\section{Properties of determinants}

\begin{theorem}
    \label{thm:determinant-multiplicative}
    Let $A,B \in \mat{n}{R}$. Then
    %
    \begin{equation*}
        \det AB
            = (\det A) (\det B).
    \end{equation*}
    %
    In particular, $\det \colon \matGL{n}{R} \to R^*$ is a group homomorphism.
\end{theorem}

\begin{proof}
    The map $\phi \colon \mat{n}{R} \to R$ given by $\phi(A) = \det AB$ is clearly $n$-linear and alternating. Hence $\phi(A) = (\det A) \phi(I)$, and $\phi(I) = \det B$.

    Furthermore, if $A$ is invertible, then $1 = \det I = (\det A) (\det A\inv)$. Thus $\det A \in R^*$, so $\det$ is a group homomorphism as claimed.
\end{proof}


\begin{corollary}
    \label{cor:determinant-similar-matrices}
    If $A,B \in \mat{n}{\field}$ are similar matrices, then $\det A = \det B$.
\end{corollary}

\begin{proof}
    Let $P \in \mat{n}{\field}$ be such that $A = PBP\inv$. \Cref{thm:determinant-multiplicative} then implies that
    %
    \begin{equation*}
        \det A
            = (\det P)(\det B)(\det P\inv)
            = (\det B)(\det PP\inv)
            = \det B.
    \end{equation*}
\end{proof}

\Cref{cor:determinant-similar-matrices} allows us to define the determinant of a general linear operator $T \colon V \to V$ on a finite-dimensional $\field$-vector space. If $\calV$ and $\calW$ are bases for $V$, then the matrix representations $\mr{\calV}{T}{\calV}$ and $\mr{\calW}{T}{\calW}$ are similar. This allows us to define the determinant $\det T$ of $T$ as the matrix representation $\mr{\calV}{T}{\calV}$ for any basis $\calV$.


\begin{proposition}
    Let $A_{11}, \ldots, A_{nn}$ be square matrices with entries in $R$ and consider the block matrix
    %
    \begin{equation*}
        M
            = \begin{pmatrix}
                A_{11} & A_{12} & \cdots & A_{1n} \\
                0      & A_{22} & \cdots & A_{2n} \\
                \vdots & \ddots & \ddots & \vdots \\
                0      & \cdots & 0      & A_{nn}
            \end{pmatrix},
    \end{equation*}
    %
    where the remaining $A_{ij}$ are matrices of appropriate dimensions. Then $\det M = \bigprod_{i=1}^n \det A_{ii}$.
\end{proposition}

\begin{proof}
    By induction it suffices to consider the case where $M$ has the block form
    %
    \begin{equation*}
        M
            = \begin{pmatrix}
                A & C \\
                0 & B
            \end{pmatrix},
    \end{equation*}
    %
    where $A \in \mat{r}{R}$, $B \in \mat{s}{R}$ and $C \in \mat{r,s}{R}$ for appropriate integers $r,s$. Notice that if we define the matrices
    %
    \begin{equation*}
        M_1
            = \begin{pmatrix}
                I_r & 0 \\
                0   & B
            \end{pmatrix}
        \quad \text{and} \quad
        M_2
            = \begin{pmatrix}
                A & C   \\
                0 & I_s
            \end{pmatrix},
    \end{equation*}
    %
    then $M = M_1 M_2$. But using \cref{thm:determinant-recursive-definition} we easily see that $\det M_1 = \det B$ and $\det M_2 = \det A$, so it follows that
    %
    \begin{equation*}
        \det M
            = (\det M_1) (\det M_2)
            = (\det A) (\det B)
    \end{equation*}
    %
    as desired.
\end{proof}

% \begin{proof}
%     By induction it suffices to consider the case where $A$ has the block form
%     %
%     \begin{equation*}
%         A
%             = \begin{pmatrix}
%                 B & C \\
%                 0 & D
%             \end{pmatrix}.
%     \end{equation*}
%     %
%     Say that $B \in \mat{r}{R}$ and $D \in \mat{s}{R}$, and put $\phi(B,C,D) = \det A$. Then $D \mapsto \phi(B,C,D)$ is clearly $s$-linear and alternating, so \cref{thm:determinant-recursive-definition} implies that
%     %
%     \begin{equation*}
%         \phi(B,C,D)
%             = (\det D) \phi(B,C,I_s).
%     \end{equation*}
%     %
%     By subtracting multiples of the rows of $I_s$ from $C$ we obtain $\phi(B,C,I_s) = \phi(B,0,I_s)$. Next, $B \mapsto \phi(B,0,I_s)$ is also $r$-linear and alternating, so
%     %
%     \begin{equation*}
%         \phi(B,0,I_s)
%             = (\det B) \phi(I_r,0,I_s).
%     \end{equation*}
%     %
%     But $\phi(I_r,0,I_s) = 1$, so summarising we have
%     %
%     \begin{equation*}
%         \phi(B,C,D)
%             = (\det D) \phi(B,C,I_s)
%             = (\det D) \phi(B,0,I_s)
%             = (\det D) (\det B),
%     \end{equation*}
%     %
%     as desired.
% \end{proof}


\begin{proposition}
    Let $A \in \mat{n}{R}$. Then $\det A = \det A\trans$.
\end{proposition}

\begin{proof}
    Writing $A = (a_{ij})$, first notice that
    %
    \begin{equation*}
        \det A\trans
            = \sum_{\sigma \in S_n} (\sign \sigma\inv) a_{\sigma(1)1} \cdots a_{\sigma(n)n},
    \end{equation*}
    %
    since $\sign \sigma = \sign \sigma\inv$. Next notice that, if $j = \sigma(i)$, then $a_{\sigma(i)i} = a_{j \sigma\inv(j)}$. Since $R$ is commutative, it follows that
    %
    \begin{equation*}
        \det A\trans
            = \sum_{\sigma \in S_n} (\sign \sigma\inv) a_{1\sigma\inv(1)} \cdots a_{n\sigma\inv(n)},
    \end{equation*}
    %
    and since $\sigma \mapsto \sigma\inv$ is a bijection on $S_n$, it follows that $\det A\trans = \det A$ as desired.
\end{proof}

\newcommand{\cof}{\operatorname{cof}}

Let $A \in \mat{n}{R}$. For $1 \leq i,j \leq n$, the \emph{$(i,j)$-th cofactor} of $A$ is the number $A_{i,j} = (-1)^{i+j} \det M(A)_{i,j}$, where we recall that $M(A)_{i,j}$ is the $(i,j)$-th minor of $A$. The \emph{cofactor matrix} of $A$ is the matrix $\cof A \in \mat{n}{R}$ whose $(i,j)$-th entry is the cofactor $A_{i,j}$. Note that
%
\begin{equation*}
    (A\trans)_{i,j}
        = (-1)^{i+j} \det M(A\trans)_{i,j}
        = (-1)^{j+i} \det M(A)_{j,i}
        = A_{j,i},
\end{equation*}
%
so $\cof A\trans = (\cof A)\trans$. Of greater importance than the cofactor matrix is the \emph{adjoint matrix} of $A$, written $\adj A$, which is just the transpose of $\cof A$. That is, the $(i,j)$-th entry of $\adj A$ is the cofactor $A_{j,i}$. Similar to the cofactor matrix we have
%
\begin{equation*}
    \adj A\trans
        = (\cof A\trans)\trans
        = \cof A
        = (\adj A)\trans.
\end{equation*}
%
We have the following:

\begin{proposition}
    \label{thm:adjoint-matrix-product}
    Let $A \in \mat{n}{R}$. Then
    %
    \begin{equation*}
        (\adj A) A
            = (\det A) I
            = A (\adj A).
    \end{equation*}
\end{proposition}

\begin{proof}
    Writing $A = (a_{ij})$ and fixing some $j \in \{1, \ldots, n\}$, \cref{thm:determinant-recursive-definition} implies that
    %
    \begin{equation*}
        \det A
            = \sum_{i=1}^n (-1)^{i+j} a_{ij} \det M(A)_{i,j}
            = \sum_{i=1}^n a_{ij} A_{i,j},
    \end{equation*}
    %
    which is just the $(j,j)$-th entry in the product $(\adj A)A$.

    Next we claim that if $k \neq j$, then $\sum_{i=1}^n a_{ik} A_{i,j} = 0$. Let $B = (b_{ij}) \in \mat{n}{R}$ be the matrix obtained from $A$ by replacing the $j$th column of $A$ by its $k$th column. Then $B$ has two equal columns, so $\det B = 0$. Also, $b_{ij} = a_{ik}$ and $M(B)_{i,j} = M(A)_{i,j}$, so it follows that
    %
    \begin{align*}
        0
            &= \det B
             = \sum_{i=1}^n (-1)^{i+j} b_{ij} \det M(B)_{i,j} \\
            &= \sum_{i=1}^n (-1)^{i+j} a_{ik} \det M(A)_{i,j}
             = \sum_{i=1}^n a_{ik} A_{i,j}.
    \end{align*}
    %
    That is, the $(j,k)$-th entry of the product $(\adj A)A$ is zero, so the off-diagonal entries of $(\adj A)A$ are zero. In total we thus have $(\adj A)A = (\det A) I$.

    Finally we prove the equality $A(\adj A) = (\det A) I$, Applying the first equality to $A\trans$ yields
    %
    \begin{equation*}
        (\adj A\trans) A\trans
            = (\det A\trans)I
            = (\det A)I,
    \end{equation*}
    %
    and transposing we get
    %
    \begin{equation*}
        A (\adj A)
            = A (\adj A\trans)\trans
            = (\det A) I
    \end{equation*}
    %
    as desired.
\end{proof}


\begin{corollary}
    Let $A \in \mat{n}{R}$. The following are equivalent:
    %
    \begin{enumcor}
        \item $A$ is a (two-sided) unit in $\mat{n}{R}$.
        \item $A$ is a left- or right-unit in $\mat{n}{R}$.
        \item $\det A$ is a unit in $R$.
    \end{enumcor}
\end{corollary}

\begin{proof}
    If $A$ is e.g. a left-unit, then \cref{thm:determinant-multiplicative} implies that
    %
    \begin{equation*}
        1
            = \det I_n
            = (\det A)(\det A\inv),
    \end{equation*}
    %
    so $\det A$ is a unit in $R$. Conversely, if $\det A$ is a unit then \cref{thm:adjoint-matrix-product} implies that $(\det A)\inv(\adj A)$ is a two-sided inverse of $A$.
\end{proof}

Notice that this gives us a second proof of the fact that a matrix is invertible just when it has either a left- or right-inverse. In fact, we see that this holds for matrices with entries in any commutative ring.


\section{Determinants and eigenvalues}

Let $V$ be a vector space of dimension $n < \infty$. If $T \in \calL(V)$, then recall that an \emph{eigenvalue} of $T$ is an element $\lambda \in \field$ such that there is a nonzero vector $v \in V$ with $Tv = \lambda v$. The set of eigenvalues of $T$ is called the \emph{spectrum} of $T$ and is denoted $\spec T$. Clearly $\lambda \in \spec T$ if and only if $\lambda I - T$ is not injective, i.e. if $\det(\lambda I - T) = 0$. This motivates the definition of the \emph{characteristic polynomial} $p_T(t) \in \field[t]$ of $T$, given by $p_T(t) = \det(tI - T)$. The eigenvalues of $T$ are then precisely the roots of $p_T(t)$.

\begin{proposition}
    Let $T \in \calL(V)$.
    %
    \begin{enumprop}
        \item $p_T(t)$ is a monic polynomial of degree $n$.
        \item The constant term of $p_T(t)$ equals $(-1)^n \det T$.
        \item The coefficient of $t^{n-1}$ in $p_T(t)$ equals $-\trace T$.
    \end{enumprop}
    %
    Assume further that $p_T(t)$ splits over $\field$. Then:
    %
    \begin{enumprop}[resume]
        \item $T$ has an eigenvalue.
        \item $\det T$ is the product of the eigenvalues of $T$.
        \item $\trace T$ is the sum of the eigenvalues of $T$.
    \end{enumprop}
\end{proposition}
%
The condition that $p_T(t)$ splits over $\field$ means that $p_T(t)$ decomposes into a product of linear factors on the form $t - a \in \field[t]$ (up to multiplication by a constant). This is in particular the case if $\field$ is algebraically closed.

\begin{proof}
\begin{proofsec}
    \item[(i)]
    Let $A = (a_{ij}) \in \mat{n}{\field}$ be a matrix representation of $T$. The $(i,j)$-th entry of $tI - A$ is then $t\delta_{ij} - a_{ij}$, so
    %
    \begin{equation}
        \label{eq:characteristic-polynomial-Leibniz}
        \det(tI - T)
            = \sum_{\sigma \in S_n} (\sign\sigma) (t\delta_{1\sigma(1)} - a_{1\sigma(1)}) \cdots (t\delta_{n\sigma(n)} - a_{n \sigma(n)})
    \end{equation}
    %
    by \cref{thm:determinant-uniqueness}. Thus $p_T(t)$ is a polynomial in $t$. Furthermore, the only entries in $tI - A$ containing $t$ are the diagonal entries, and the largest number of such entries occurring in a single term of \cref{eq:characteristic-polynomial-Leibniz} is $n$, so $\deg p_T(t) \leq n$. But notice that there is only one term in which $t$ appears $n$ times, namely the term corresponding to the identity permutation in $S_n$, giving the product of the diagonal entries in $tI-A$. This term equals
    %
    \begin{equation}
        \label{eq:characteristic-polynomial-diagonal-product}
        (t-a_{11})(t-a_{22}) \cdots (t-a_{nn}),
    \end{equation}
    %
    and multiplying out we see that the only resulting term containing $t^n$ is $t^n$ itself. Hence $p_T(t)$ is monic and of degree $n$. Thus we may write $p_T(t) = \sum_{i=0}^n c_i t^i$ for appropriate $c_0, \ldots, c_n \in \field$.

    \item[(ii)]
    Simply notice that
    %
    \begin{equation*}
        (-1)^n \det T
            = \det(-T)
            = p_T(0)
            = c_0
    \end{equation*}
    %
    by $n$-linearity of $\det$ and the definition of $p_T(t)$.

    \item[(iii)]
    The only way for one of the terms in \cref{eq:characteristic-polynomial-Leibniz} to contain the factor $t^{n-1}$ is for at least $n-1$ of the $b_{ij}$ to be a diagonal element. But in choosing $n-1$ elements along the diagonal we are forced to also choose the final diagonal element, since otherwise $\sigma$ would not be a permutation. Hence the factor $t^n$ can only appear in the product \cref{eq:characteristic-polynomial-diagonal-product}. It is then clear that
    %
    \begin{equation*}
        c_{n-1}
            = - (a_{11} + \cdots + a_{nn})
            = - \trace T
    \end{equation*}
    %
    as claimed.

    \item[(iv)]
    Now assume that $p_T(t)$ splits over $\field$. Then some linear factor $t-\lambda \in \field[t]$ divides $p_T(t)$, which implies that $\lambda \in \field$ is an eigenvalue of $T$.
    
    \item[(v)]
    Since $p_T(t)$ is monic we have
    %
    \begin{equation*}
        p_T(t)
            = (t - \lambda_1) (t - \lambda_2) \cdots (t - \lambda_n)
    \end{equation*}
    %
    for appropriate $\lambda_1, \ldots, \lambda_n \in \field$. These are then the (not necessarily distinct) eigenvalues of $T$. Thus $p_T(0) = (-1)^n \lambda_1 \cdots \lambda_n$, and the claim follows from (ii).

    \item[(vi)]
    We similarly find that $c_{n-1} = -(\lambda_1 + \cdots + \lambda_n)$, so the final claim follows from (iii).
\end{proofsec}
\end{proof}


\section{Proofs without determinants}

\subsection{Existence of eigenvalues}

% \begin{theorem}[The spectral mapping theorem]
%     Let $F$ be an algebraically closed field, let $A \in \mat{n}{F}$ and let $p(t) \in F[t]$. Then
%     %
%     \begin{equation*}
%         \spec p(A) = p(\spec A).
%     \end{equation*}
% \end{theorem}

% \begin{proof}
%     First let $\lambda \in \spec A$, and choose a corresponding eigenvector $v \in F^n$. If we write $p(t) = \sum_{i=0}^n a_i t^i$, then
%     %
%     \begin{equation*}
%         p(A)v
%             = \biggl( \sum_{i=0}^n a_i A^i \biggr) v
%             = \sum_{i=0}^n a_i A^i v
%             = \sum_{i=0}^n a_i \lambda^i v
%             = \biggl( \sum_{i=0}^n a_i \lambda^i \biggr) v
%             = p(\lambda) v.
%     \end{equation*}
%     %
%     Thus $p(\lambda) \in \spec p(A)$, and so $p(\spec A) \subseteq \spec p(A)$.

%     For the opposite inclusion, let $\lambda \in \spec p(A)$, i.e. $(p(A) - \lambda I) v = 0$ for some nonzero $v \in F^n$.
% \end{proof}

\newcommand{\ev}{\mathrm{ev}}

Assume that $\field$ is algebraically closed, and consider $T \in \calL(V)$. For $d \in \naturals$, let $\field[t]_d$ denote the vector space of polynomials in $\field[t]$ with degree strictly less than $d$, such that $\dim \field[t]_d = d$. Consider the map $\ev_T \colon \field[t]_{n^2+1} \to \calL(V)$ given by $\ev_T(p) = p(T)$. This cannot be injective, so there is some nonzero $p(t) \in \field[t]_{n^2+1}$ such that $p(T) = 0$. Note that $p(t)$ cannot be constant.

Since $\field$ is algebraically closed, there exist $c, \lambda_1, \ldots, \lambda_m \in \field$ such that $p(t) = c \bigprod_{i=1}^m (t - \lambda_i)$. But then
%
\begin{equation*}
    0
        = p(T)
        = c \bigprod_{i=1}^m (T - \lambda_i I),
\end{equation*}
%
so at least one $T - \lambda_i I$ is not injective. Hence $\lambda_i$ is an eigenvalue of $T$.


\subsection{Trace is sum of eigenvalues}

\newcommand{\Span}{\operatorname{span}}


\begin{corollary}
    Let $\field$ be algebraically closed, and let $T \in \calL(V)$. Then the sum of the eigenvalues of $T$ is $\trace T$.
\end{corollary}

\begin{proof}
    Let $A \in \mat{n}{\field}$ be an upper triangular matrix [TODO reference to later, perhaps move things around.] for $T$. The diagonal elements of $A$ are the eigenvalues, and the trace of $T$ is just the sum of these elements.
\end{proof}


\section{Cross products}

\begin{definition}[Cross products]
    Let $v = (\alpha_1, \alpha_2, \alpha_3)$ and $w = (\beta_1, \beta_2, \beta_3)$ be vectors in $\reals^3$. The \emph{cross product} of $v$ and $w$ is the vector
    %
    \begin{equation*}
        v \crossp w =
        \begin{pmatrix}
            \alpha_2 \beta_3 - \alpha_3 \beta_2 \\
            \alpha_3 \beta_1 - \alpha_1 \beta_3 \\
            \alpha_1 \beta_2 - \alpha_2 \beta_1
        \end{pmatrix}.
    \end{equation*}
\end{definition}
%
Denote the standard basis on $\reals^3$ by $\calE = (e_1, e_2, e_3)$. We easily see that $e_i \crossp e_j = e_k$ when $(i,j,k)$ is a cyclic permutation of $(1,2,3)$.


\begin{lemma}
    \label{lem:cross-product-determinant}
    Let $v,w,u \in \reals^3$. Then
    %
    \begin{equation*}
        \inner{u}{v \crossp w}
            = \det(u,v,w).
    \end{equation*}
\end{lemma}

\begin{proof}
    By multilinearity of the inner product and of determinants, it suffices to prove the lemma when $u$ is a basis vector. But it is clear that
    %
    \begin{equation*}
        \inner{e_i}{v \crossp w}
            = \det(e_i,v,w),
    \end{equation*}
    %
    as desired.
\end{proof}


The product $\inner{u}{v \crossp w}$ is called the \emph{(scalar) triple product} of $u$, $v$ and $w$, and is denoted $[u,v,w]$. We call it the \emph{scalar} triple product to distinguish it from the \emph{vector} triple product $u \crossp (v \crossp w)$, whose properties we will examine in \cref{cor:vector-triple-product}. The scalar triple product has some very nice properties summarised in the following proposition:

\begin{proposition}
    Let $u,v,w \in \reals^3$.
    %
    \begin{enumprop}
        \item The cross product map $(v,w) \mapsto v \crossp w$ is bilinear.

        \item $v \crossp w = - w \crossp v$.

        \item The triple product $[u,v,w]$ is invariant under cyclic permutations, i.e.
        %
        \begin{equation*}
            [u,v,w]
                = [v,w,u]
                = [w,u,v]
        \end{equation*}
        %
        and invariant under interchange of inner product and cross product, i.e.
        %
        \begin{equation*}
            \inner{u}{v \crossp w}
                = [u,v,w]
                = \inner{u \crossp v}{w}.
        \end{equation*}

        \item $v \crossp w = 0$ if and only if $v$ and $w$ are linearly dependent.

        \item $v \crossp w$ is orthogonal to both $v$ and $w$.
    \end{enumprop}
\end{proposition}

\begin{proof}
    The first three claims follow from \cref{lem:cross-product-determinant} since the determinant is multilinear and alternating (hence skew-symmetric).
    
    For the fourth claim, if $v$ and $w$ are linearly dependent then $\det(u,v,w) = 0$ for all $u \in \reals^3$, so $v \crossp w = 0$. Conversely, if $v$ and $w$ are linearly independent, then extending to a basis $(u,v,w)$ for $\reals^3$ we have $\det(u,v,w) \neq 0$, implying that $v \crossp w \neq 0$.

    To prove the final claim, notice that
    %
    \begin{equation*}
        [v,v,w]
            = \det(v,v,w)
            = 0,
    \end{equation*}
    %
    and similarly for $w$.
\end{proof}


\begin{proposition}
    Let $a,b,v,w \in \reals^3$. Then
    %
    \begin{equation*}
        \inner{a \crossp b}{v \crossp w}
            = \det
            \begin{pmatrix}
                \inner{a}{v} & \inner{b}{v} \\
                \inner{a}{w} & \inner{b}{w}
            \end{pmatrix}.
    \end{equation*}
    %
    In particular,
    %
    \begin{equation*}
        \norm{v \crossp w}^2
            = \det
            \begin{pmatrix}
                \norm{v}^2 & \inner{v}{w} \\
                \inner{v}{w} & \norm{w}^2
            \end{pmatrix}.
    \end{equation*}
\end{proposition}
%
The latter identity is just Lagrange's identity in three dimensions. If $\theta$ is the angle between $v$ and $w$, then $\inner{v}{w} = \norm{v} \, \norm{w} \cos\theta$, so
%
\begin{equation*}
    \norm{v \crossp w}^2
        = \norm{v}^2 \norm{w}^2 - \inner{v}{w}^2
        = \norm{v}^2 \norm{w}^2 (1 - \cos^2 \theta)
        = \norm{v}^2 \norm{w}^2 \sin^2 \theta.
\end{equation*}
%
Hence $\norm{v \crossp w} = \norm{v} \, \norm{w} \, \abs{\sin \theta}$, which is the area of the parallelogram spanned by $v$ and $w$. If $u \in \reals^3$ is another vector and $\phi$ is the angle between $u$ and the normal of the plane spanned by $v$ and $w$ (e.g. $v \crossp w$), then
%
\begin{equation*}
    \abs[\big]{[u,v,w]}
        = \abs{\inner{u}{v \crossp w}}
        = \norm{u} \, \norm{v \crossp w} \, \abs{\cos\phi}
        = \norm{u} \, \norm{v} \, \norm{w} \, \abs{\sin\theta \cos\phi}.
\end{equation*}
%
But this is the volume of the parallelepiped spanned by $u$, $v$ and $w$. This gives a geometric interpretation (or \enquote{proof}) of the invariance of the scalar triple product.

\begin{proof}
    By linearity it suffices to prove the identity when the four vectors are basis vectors. If $a = b$ or $v = w$ then both sides are zero, so we may assume that $a = e_i$, $b = e_j$, $v = e_k$ and $v = e_l$ with $i \neq j$ and $k \neq l$. By potentially swapping $a$ and $b$ and/or $v$ and $w$ we may assume that $e_i \crossp e_j = e_p$ and $e_k \crossp e_l = e_q$ for some $p,q \in \{1,2,3\}$.

    If $p = q$ then $i = k$ and $j = l$, so both sides equal $1$. If instead $p \neq q$, then the two cross products on the left-hand side are orthogonal, so the inner product is zero. Furthermore, either $k$ or $l$ equals $p$, so one of the rows in the right-hand side matrix is zero, and hence the determinant is zero.
\end{proof}


\begin{corollary}
    \label{cor:vector-triple-product}
    Let $u,v,w \in \reals^3$. Then
    %
    \begin{equation}
        \label{eq:bac-cab}
        u \crossp (v \crossp w)
            = v\inner{u}{w} - w\inner{u}{v}.
    \end{equation}
    %
    In particular, the cross product satisfies the \emph{Jacobi identity}
    %
    \begin{equation}
        \label{eq:cross-product-Jacobi}
        u \crossp (v \crossp w)
            + v \crossp (w \crossp u)
            + w \crossp (u \crossp v)
            = 0.
    \end{equation}
\end{corollary}
%
The identity \cref{eq:bac-cab} is sometimes called the \enquote{bac-cab rule}, a name that would have been self-explanatory had we used the names $a$, $b$ and $c$ instead of $u$, $v$ and $w$. Note that to conform to this rule we need to write the vectors before the scalars.

\begin{proof}
    For $x \in \reals^3$ we have
    %
    \begin{align*}
        \inner{x}{u \crossp (v \crossp w)}
            &= [x, u, v \crossp w] \\
            &= \inner{x \crossp u}{v \crossp w} \\
            &= \det \begin{pmatrix}
                \inner{x}{v} & \inner{u}{v} \\
                \inner{x}{w} & \inner{u}{w}
            \end{pmatrix} \\
            &= \inner{x}{v} \inner{u}{w} - \inner{u}{v} \inner{x}{w} \\
            &= \inner[\big]{x}{ v\inner{u}{w} - w\inner{u}{v} }.
    \end{align*}
    %
    The claim then follows since $x$ was arbitrary.
\end{proof}


\begin{lemma}
    \label{lem:approximate-singular-matrix-with-invertible}
    Let $A \in \mat{d}{\reals}$. Every neighbourhood of $A$ contains an invertible matrix different from $A$. In particular, there exists a sequence $(A_n)_{n \in \naturals}$ of invertible matrices different from $A$ such that $A_n \to A$ for $n \to \infty$.
\end{lemma}
%
Since $\mat{d}{\reals}$ is a finite-dimensional vector space, it has a unique vector space topology. More concretely, all norms on $\mat{d}{\reals}$ are Lipschitz equivalent, so we may choose whatever norm we wish. We choose the Euclidean norm, identifying $\mat{d}{\reals}$ with $\reals^{d^2}$.

\begin{proof}
    Let $t \in \reals \setminus \{0\}$. Then $A - tI$ is invertible if and only if $\det(A - tI) = 0$, but $\det(A - tI)$ is a polynomial in $t$, so it has finitely many roots. Hence the nonzero roots of $\det(A - tI)$ are bounded away from zero, so since $A - tI \to A$ as $t \to 0$, the claim follows.
\end{proof}


\begin{proposition}[Transformation of cross products]
    Let $u,v,w \in \reals^3$, and let $A \in \mat{3}{\reals}$. Then we have the following:
    %
    \begin{enumprop}
        \item $[Au, Av, Aw] = (\det A) [u,v,w]$.
        
        \item $Av \crossp Aw = (\cof A)(v \crossp w) = (\adj A)\trans (v \crossp w)$.

        \item \label{enum:cross-product-orthogonal-transformation} If $A$ is orthogonal, then $A(v \crossp w) = (\det A)(Av \crossp Aw)$.
    \end{enumprop}
\end{proposition}
%
This gives a geometric interpretation of the determinant. If $[u,v,w]$ is the signed volume of the parallelepiped spanned by $u$, $v$ and $w$, and $[Au,Av,Aw]$ is the signed volume of the parallelepiped spanned by $Au$, $Av$ and $Aw$, then $\det A$ is the factor by which this volume increasing when applying $A$ to each of $u$, $v$ and $w$. In particular, this explains why the determinant of $A$ is zero if and only if $A$ is singular: This means that $A$ sends a basis of $\reals^3$ to a linearly dependent set, and the parallelepiped spanned by such a set has zero volume.

\begin{proof}
\begin{proofsec}
    \item[Proof of (i)]
    Simply notice that
    %
    \begin{equation*}
        [Au, Av, Aw]
            = \det(Au, Av, Aw)
            = (\det A) \det(u,v,w)
            = (\det A) \inner{u}{v \crossp w},
    \end{equation*}
    %
    where the second equality follows since $\det(Au, Av, Aw)$ is also the determinant of the matrix
    %
    \begin{equation*}
        \bigl( Au \mid Av \mid Aw \bigr)
            = A \bigl( u \mid v \mid w \bigr),
    \end{equation*}
    %
    and the determinant is multiplicative.

    \item[Proof of (ii)]
    First assume that $A$ is invertible. Then replacing $u$ with $A\inv u$ in (i) we obtain
    %
    \begin{align*}
        \inner{u}{Av \crossp Aw}
            &= (\det A) \inner{A\inv u}{v \crossp w} \\
            &= (\det A) \inner{u}{(A\inv)\trans (v \crossp w)} \\
            &=  \inner{u}{(\cof A)(v \crossp w)},
    \end{align*}
    %
    where the last equality follows from \cref{thm:adjoint-matrix-product}. Hence we obtain the desired identity when $A$ is invertible. Finally notice that both the maps $A \mapsto \cof A$ and $A \mapsto Av \crossp Aw$ are continuous. Hence the claim for general $A$ follows from \cref{lem:approximate-singular-matrix-with-invertible}.

    \item[Proof of (iii)]
    Notice that $A\inv = A\trans$, so this follows immediately from (ii).
\end{proofsec}
\end{proof}

\fleuronbreak

If $A$ is a proper rotation, i.e. if $A$ is orthogonal and $\det A = 1$, then \cref{enum:cross-product-orthogonal-transformation} implies that $A(v \crossp w) = Av \crossp Av$. This allows us to define a cross product on any three-dimensional inner product space, when this is equipped with an orientation.

\newcommand{\calO}{\mathcal{O}}

First, if $\calV$ and $\calW$ are ordered bases for any finite-dimensional real vector space $V$, then we say that $\calV$ and $\calW$ have the \emph{same orientation} if the change of basis operator $\basischange{\calW}{\calV}$ has positive determinant. It follows that orientation partitions the set of ordered bases for $V$ into two \emph{orientation classes}, each called an \emph{orientation} of $V$. If $V$ is equipped with an orientation $\calO$, then we call this class the \emph{positive orientation} of $V$, and the other class the \emph{negative orientation} of $V$. An ordered basis for $V$ is called \emph{positive} if it lies in $\calO$ and \emph{negative} if it does not.

Returning to the case where $V$ is three-dimensional and equipped with an orientation, let $\calV$ and $\calW$ be positive ordered orthonormal bases for $V$. For vectors $v,w \in V$ we can then consider the cross products of their coordinate vectors, i.e.
%
\begin{equation*}
    \coordvec{v}{\calV} \crossp \coordvec{w}{\calV}
    \quad \text{and} \quad
    \coordvec{v}{\calW} \crossp \coordvec{w}{\calW}.
\end{equation*}
%
Since $\basischangemat{\calW}{\calV}$ is orthogonal with determinant $1$, we have
%
\begin{equation*}
    \basischangemat{\calW}{\calV}(\coordvec{v}{\calV} \crossp \coordvec{w}{\calV})
        = \basischangemat{\calW}{\calV} \cdot \coordvec{v}{\calV} \crossp \basischangemat{\calW}{\calV} \cdot \coordvec{w}{\calV}
        = \coordvec{v}{\calW} \crossp \coordvec{w}{\calW}.
\end{equation*}
%
Hence we have
%
\begin{equation*}
    \coordmap{\calV}\inv(\coordvec{v}{\calV} \crossp \coordvec{w}{\calV})
        = \coordmap{\calW}\inv(\coordvec{v}{\calW} \crossp \coordvec{w}{\calW}),
\end{equation*}
%
so we may define the cross product of $v$ and $w$ as $v \crossp w = \coordmap{\calV}\inv(\coordvec{v}{\calV} \crossp \coordvec{w}{\calV})$ where $\calV$ is any positive ordered orthonormal basis for $V$. Notice that this means that $\coordvec{v \crossp w}{\calV} = \coordvec{v}{\calV} \crossp \coordvec{w}{\calV}$.

This allows us to generalise most of the above results to general vector spaces. For instance, using that the coordinate map $\coordmap{\calV}$ is an isometry, the scalar triple product of $u,v,w \in V$ is given by
%
\begin{equation*}
    [u,v,w]
        = \inner{u}{v \crossp w}
        = \inner{\coordvec{u}{\calV}}{\coordvec{v \crossp w}{\calV}}
        = \inner{\coordvec{u}{\calV}}{\coordvec{v}{\calV} \crossp \coordvec{w}{\calV}}
        = \bigl[ \coordvec{u}{\calV}, \coordvec{v}{\calV}, \coordvec{w}{\calV} \bigr],
\end{equation*}
%
and hence it has all the properties of the scalar triple product on $\reals^3$, such as invariance under cyclic permutations. Notice also that it is indeed a \emph{scalar} quantity, in the sense that it is invariant under a change of basis. Similarly, the \enquote{bac-cab rule} \cref{eq:bac-cab} becomes
%
\begin{align*}
    \coordvec{u \crossp (v \crossp w)}{\calV}
        &= \coordvec{u}{\calV} \crossp \coordvec{v \crossp w}{\calV} \\
        &= \coordvec{u}{\calV} \crossp (\coordvec{v}{\calV} \crossp \coordvec{w}{\calV}) \\
        &= \coordvec{v}{\calV} \inner{\coordvec{u}{\calV}}{\coordvec{w}{\calV}} - \coordvec{w}{\calV} \inner{\coordvec{u}{\calV}}{\coordvec{v}{\calV}} \\
        &= \coordvec{v}{\calV} \inner{u}{w} - \coordvec{w}{\calV} \inner{u}{v} \\
        &= \coordvec{ v \inner{u}{w} - w \inner{u}{v} }{\calV}.
\end{align*}
%
Hence $u \crossp (v \crossp w) = v \inner{u}{w} - w \inner{u}{v}$ since $\coordmap{\calV}$ is an isomorphism. In particular, the cross product on $V$ also satisfies the Jacobi identity \cref{eq:cross-product-Jacobi}, so $V$ becomes a Lie algebra whose Lie bracket is given by the cross product, i.e. $[v,w] = v \crossp w$.


\chapter{Complexification}

If $W$ is a complex vector space, then we may restrict the scalar multiplication $\complex \prod W \to W$ to a map $\reals \prod W \to W$. When we equip $W$ with this restricted scalar multiplication instead of the original one, we call the resulting space the \emph{real version of $W$} and denote it by $W_\reals$.

Conversely, if $V$ is a real vector space then we define the \emph{complexification of $V$} as the vector space $V^\complex$ whose underlying set is $V \prod V$, and which is equipped with componentwise addition and the complex scalar multiplication
%
\begin{equation*}
    (a + \iu b)(v,u)
        = (av - bu, au + bv),
\end{equation*}
%
for $a,b \in \reals$ and $v,u \in V$. We denote the vector $(v,u)$ by $v + \iu u$.

% \newcommand{\cpx}{\mathrm{cpx}}

% We define the \emph{complexification map} $\cpx \colon V \to V^\complex$ by $\cpx(v) = v + \iu 0$. This is clearly real-linear and injective, so it induces a real-linear isomorphism between $V$ and its image in $V^\complex$. More precisely, a linear isomorphism between $V$ and a subspace of the real version $(V^\complex)_\reals$ of $V^\complex$.

If $T \colon V \to W$ is a linear map between real vector spaces, then we define the complexification of $T$ by
%
\begin{align*}
    T^\complex \colon V^\complex &\to W^\complex, \\
    v + \iu u &\mapsto Tv + \iu Tu.
\end{align*}
%
That is, $T^\complex$ is just the product map $T \prod T$. This is easily seen to be complex-linear.

If $V$ is a real inner product space, then we define an inner product by
%
\begin{equation*}
    \inner{v + \iu u}{x + \iu y}
        = \inner{v}{x}
          + \inner{u}{y}
          + \iu (\inner{u}{x} - \inner{v}{y}).
\end{equation*}
%
Notice that this identity holds in any \emph{complex} inner product space, where the notation $v + \iu u$ instead means the sum of $v$ and the scalar product of $\iu$ and $u$ (in justifying this claim, the reader will recall that the inner product on a complex space is \emph{sesqui}linear).


\chapter{Operator adjoints}

\begin{definition}[Operator adjoints]
    Let $V$ and $W$ be $\field$-vector spaces, and let $T \colon V \to W$ be a linear map. The \emph{(operator) adjoint} of $T$ is the pullback
    %
    \begin{align*}
        T^* \colon W^* &\to V^*, \\
        \phi &\mapsto \phi \circ T.
    \end{align*}
\end{definition}
%
Note that this is just the action of the dual functor on maps in the category of $\field$-vector spaces. Hence it already satisfies $\id_V^* = \id_{V^*}$ and $(ST)^* = T^* S^*$, so that in particular $(T\inv)^* = (T^*)\inv$ when $T$ is invertible. Furthermore, it is easy to show that the map $T \mapsto T^*$ is linear. It is also injective, since if $Tv \neq Sv$ then there is a $\phi \in W^*$ such that $\phi(Tv) \neq \phi(Sv)$. If $V$ and $W$ are finite-dimensional, it is therefore a linear isomorphism.

\newcommand{\im}{\operatorname{im}}

\begin{proposition}
    Let $T \in \calL(V,W)$.
    %
    \begin{enumprop}
        \item $\ker T^* = (\im T)^0$.
        \item $\im T^* = (\ker T)^0$.
    \end{enumprop}
\end{proposition}

\begin{proof}
    \textcite[Theorem~3.19]{romanlinalg}.
\end{proof}

\newcommand{\rank}{\operatorname{rank}}

\begin{corollary}
    \label{cor:adjoint-rank}
    If $T \in \calL(V,W)$ with $V$ and $W$ finite-dimensional, then $\rank T^* = \rank T$.
\end{corollary}

\begin{proof}
    Recall that the dimension of $(\ker T)^0$ equals the codimension of $\ker T$, which is just $\dim V - \dim \ker T$ when $V$ is finite-dimensional (cf. \cite[Theorem~3.15]{romanlinalg}). We then have
    %
    \begin{equation*}
        \rank T^*
            = \dim \im T^*
            = \dim (\ker T)^0
            = \dim V - \dim \ker T
            = \dim \im T
            = \rank T,
    \end{equation*}
    %
    as desired.
\end{proof}

Note that if $\calV = (v_1, \ldots, v_n)$ is an ordered basis for $V$, $\calV^*$ the corresponding dual basis, and $\calV^{**}$ the double dual basis, then for $\phi = \phi_1 v_1^* + \cdots + \phi_n v_n^*$ we have
%
\begin{equation*}
    v_i^{**}(\phi)
        = \phi_i
        = \phi(v_i),
\end{equation*}
%
since both $v_i^*(v_j) = \delta_{ij}$ and $v_i^{**}(v_j^*) = \delta_{ij}$, by definition of the dual basis.

\begin{proposition}
    \label{prop:adjoint-mr}
    If $T \in \calL(V,W)$ is a linear map between finite-dimensional vector spaces, and $\calV$ and $\calW$ are ordered bases for $V$ and $W$ respectively, then
    %
    \begin{equation*}
        \mr{\calV^*}{T^*}{\calW^*}
            = (\mr{\calW}{T}{\calV})\trans.
    \end{equation*}
\end{proposition}

\begin{proof}
    Write $\calV = (v_1, \ldots, v_n)$ and $\calW = (w_1, \ldots, w_m)$. Then
    %
    \begin{equation*}
        (\mr{\calW}{T}{\calV})_{ij}
            = (\coordvec{Tv_j}{\calW})_i
            = w_i^*(Tv_j),
    \end{equation*}
    %
    and
    %
    \begin{equation*}
        (\mr{\calV^*}{T^*}{\calW^*})_{ij}
            = (\coordvec{T^* w_j^*}{\calV^*})_i
            = v_i^{**}(T^* w_j^*)
            = T^* w_j^*(v_i)
            = w_j^*(Tv_i).
    \end{equation*}
    %
    These expressions are the same, but with $i$ and $j$ switched.
\end{proof}


\begin{corollary}
    The row rank and the column rank of a matrix $A \in \mat{m,n}{\field}$ are equal.
\end{corollary}

\begin{proof}
    The matrix representation of the multiplication operator $M_A$ with respect to the standard bases on $\field^n$ and $\field^m$ is just $A$ itself, and \cref{prop:adjoint-mr} then implies that the matrix representation of $(M_A)^*$ with respect to the dual bases is $A\trans$. But the rank of an operator equals the rank of any matrix representation of that operator, so \cref{cor:adjoint-rank} implies that $A$ and $A\trans$ have the same (column) rank. Finally, the column rank of $A\trans$ is the row rank of $A$, proving the claim.
\end{proof}


If $V$ is a finite-dimensional inner product space, for $v \in V$ let $\phi_v$ denote the element in $V^*$ given by $\phi_v(w) = \inner{w}{v}$. Further, let $\Phi_V \colon V \to V^*$ denote the (conjugate-)linear isomorphism $v \mapsto \phi_v$.

\begin{theorem}
    Let $V$ and $W$ be finite-dimensional inner product spaces, and let $T \in \calL(V,W)$. Denoting the Hilbert space adjoint of $T$ by $T^\dagger \colon W \to V$ we have
    %
    \begin{equation*}
        T^*
            = \Phi_V \circ T^\dagger \circ \Phi_W\inv,
    \end{equation*}
    %
    i.e. the diagram
    %
    \begin{equation*}
        \begin{tikzcd}
            V
                \ar[r, "T", shift left]
                \ar[d, "\Phi_V", swap]
            & W
                \ar[l, "T^\dagger", shift left]
                \ar[d, "\Phi_W"] \\
            V^*
            & W^*
                \ar[l, "T^*", swap]
        \end{tikzcd}
    \end{equation*}
    %
    commutes. [TODO also commutes when $T$ is there?]
\end{theorem}

\begin{proof}
    Simply notice that, for $v \in V$ and $\phi \in W^*$, we have
    %
    \begin{equation*}
        T^*\phi(v)
            = \phi(Tv)
            = \inner{Tv}{\Phi_W\inv(\phi)}
            = \inner{v}{T^\dagger \Phi_W\inv(\phi)}
            = \Phi_V\bigl( T^\dagger \Phi_W\inv(\phi) \bigr)(v),
    \end{equation*}
    %
    which implies the claim.
\end{proof}


\chapter{Triangularisation and diagonalisation}

\section{Triangularisation}

Recall that a matrix $A = (a_{ij}) \in \mat{n}{R}$ is called \emph{upper triangular} if $a_{ij} = 0$ whenever $i > j$. If $V$ is an $n$-dimensional $\field$-vector space and $\calV$ is an ordered basis for $V$, then we say that the operator $T \in \calL(V)$ is \emph{upper triangular with respect to $\calV$} if the matrix representation $\mr{\calV}{T}{\calV}$ is upper triangular.

A subspace $U$ of a vector space $V$ is said to be \emph{invariant under $T \in \calL(T)$} if $T(U) \subseteq U$.

\begin{proposition}
    \label{prop:upper-triangular-criterion}
    Let $V$ be an $\field$-vector space with $n = \dim V < \infty$, and let $\calV = (v_1, \ldots, v_n)$ be an ordered basis for $V$. An operator $T \in \calL(V)$ is upper triangular with respect to $\calV$ if and only if $\Span(v_1, \ldots, v_i)$ is invariant under $T$ for all $i \in \{1, \ldots, n\}$.
\end{proposition}

\begin{proof}
    This is obvious.
\end{proof}


\begin{lemma}
    Let $V$ be an $\field$-vector space, and let $T \in \calL(V)$ be an isomorphism. If $U$ is a finite-dimensional subspace of $V$ that is invariant under $T$, then $U$ is also invariant under $T\inv$.
\end{lemma}

\begin{proof}
    Since $U$ is finite-dimensional and $T|_U \colon U \to U$ is injective, applying the rank--nullity theorem implies that $T|_U$ is also surjective. Hence if $u \in U$, then there exists a $v \in U$ such that $Tv = u$. It follows that
    %
    \begin{equation*}
        T\inv u
            = T\inv Tv
            = v
            \in U,
    \end{equation*}
    %
    so $U$ is invariant under $T\inv$.
\end{proof}


\begin{proposition}
    Let $V$ be a finite-dimensional $\field$-vector space, and let $\calV$ be an ordered basis for $V$. If $T \in \calL(V)$ is an isomorphism that is upper triangular with respect to $\calV$, then $T\inv$ is also upper triangular with respect to $\calV$.

    In particular, the subset of $\matGL{n}{\field}$ consisting of upper triangular matrices is a subgroup.
\end{proposition}

\begin{proof}
    This is an obvious consequence of the above two results.
\end{proof}


\begin{lemma}
    \label{lem:upper-triangular-invertible}
    Let $A \in \mat{n}{\field}$ be upper triangular. Then $A$ is invertible if and only if all its diagonal elements are nonzero.
\end{lemma}

\begin{proof}
    Denote the diagonal elements of $A$ by $\lambda_1, \ldots, \lambda_n$, and let $(e_1, \ldots, e_n)$ be the standard basis of $\field^n$. First assume that the diagonal elements are nonzero. Then notice that $e_1 \in R(A)$, and that
    %
    \begin{equation*}
        A e_i
            = a_1 e_1 + \cdots + a_{i-1} e_{i-1} + \lambda_i e_i
    \end{equation*}
    %
    for appropriate $a_1, \ldots, a_{i-1} \in \field$. By induction we then have $e_i \in R(A)$. Since $(e_1, \ldots, e_n)$ is a basis, this implies that $R(A) = \field^n$.

    Conversely, assume that some diagonal element $\lambda_i$ is zero. Then
    %
    \begin{equation*}
        A \Span(e_1, \ldots, e_i)
            \subseteq \Span(e_1, \ldots, e_{i-1}),
    \end{equation*}
    %
    so the null-space of $A$ is nontrivial, and hence $A$ is singular.
\end{proof}


\begin{lemma}
    Let $A \in \mat{n}{\field}$ be upper triangular. Then the eigenvalues of $A$ are its diagonal elements.
\end{lemma}

\begin{proof}
    Let $\lambda \in \field$, and denote the diagonal elements of $A$ by $\lambda_1, \ldots, \lambda_n$. By \cref{lem:upper-triangular-invertible}, the matrix $\lambda I - A$ is singular if and only if $\lambda - \lambda_i = 0$ for some $i$, and hence $\lambda_1, \ldots, \lambda_n$ are the eigenvalues of $A$.
\end{proof}


\begin{proposition}
    \label{prop:upper-triangular-basis-exists}
    Let $\field$ be algebraically closed, and let $V$ be a finite-dimensional $\field$-vector space. If $T \in \calL(V)$, then $V$ has an ordered basis with respect to which $T$ is upper triangular.
\end{proposition}

\begin{proof}
    This is obvious if $\dim V = 1$, so assume that $n = \dim V > 1$, and assume that the claim is true for $\field$-vector spaces of dimension $n-1$. Since $\field$ is algebraically closed, $T$ has an eigenvector $v_1 \in V$. Then $U = \Span(v_1)$ is invariant under $T$, so we may define a linear operator\footnote{The operator $\tilde T$ may arise as follows: Let $\pi \colon V \to V/U$ be the quotient map. Then $U \subseteq \ker (\pi \circ T)$ since $U$ is invariant under $T$, so $\pi \circ T$ descends to a linear map $\tilde T \colon V/U \to V/U$.} $\tilde T \in \calL(V/U)$ by $\tilde T(v + U) = Tv + U$. Since $\dim V/U = n-1$, by induction there is a basis $v_2 + U, \ldots, v_n + U$ of $V/U$ with respect to which the matrix of $\tilde T$ is upper triangular. It is easy to show that the collection $v_1, \ldots, v_n$ is linearly independent, hence a basis for $V$.

    Now notice that
    %
    \begin{equation*}
        Tv_i + U
            = \tilde T(v_i + U)
            \in \Span(v_2 + U, \ldots, v_i + U)
    \end{equation*}
    %
    for $i \in \{2, \ldots, n\}$. That is, there exist $a_2, \ldots, a_i \in \field$ such that
    %
    \begin{equation*}
        Tv_i + U
            = (a_2 v_2 + \cdots + a_i v_i) + U.
    \end{equation*}
    %
    But then $Tv_i \in \Span(v_1, \ldots, v_i)$ for all $i \in \{2, \ldots, n\}$, and since $U$ is invariant under $T$ this also holds for $i = 1$. Hence $T$ is upper triangular with respect to the basis $v_1, \ldots, v_n$ of $V$.
\end{proof}


\begin{theorem}[Schur's Theorem]
    Let $V$ be a finite-dimensional complex inner product space. If $T \in \calL(V)$, then $V$ has an ordered orthonormal basis with respect to which $T$ is upper triangular.
\end{theorem}

\begin{proof}
    By \cref{prop:upper-triangular-basis-exists} $V$ has an ordered basis $\calV = (v_1, \ldots, v_n)$ with respect to which $\mr{\calV}{T}{\calV}$ is upper triangular. Now apply the Gram--Schmidt procedure to $\calV$ and obtain an orthonormal basis $\calU = (u_1, \ldots, u_n)$ for $V$ such that
    %
    \begin{equation*}
        \Span(u_1, \ldots, u_i)
            = \Span(v_1, \ldots, v_i)
    \end{equation*}
    %
    for all $i \in \{1, \ldots, n\}$. Then \cref{prop:upper-triangular-criterion} shows that $\mr{\calU}{T}{\calU}$ is also upper triangular, proving the claim.
\end{proof}


\section{Orthonormal diagonalisation}

Let $V$ and $W$ be finite-dimensional inner product spaces, and let $T \in \calL(V,W)$. Recall that the \emph{adjoint of $T$} is the operator $T^* \in \calL(W,V)$ with the property that
%
\begin{equation*}
    % \label{eq:adjoint-defining-property}
    \inner{T^*w}{v}_V = \inner{w}{Tv}_W,
\end{equation*}
%
or by complex conjugation equivalently
%
\begin{equation*}
    \inner{Tv}{w}_W = \inner{v}{T^*w}_V,
\end{equation*}
%
for all $v \in V$ and $w \in W$. An operator with this property is unique if it exists, since if $S \in \calL(W,V)$ is another such operator, then $\inner{v}{Sw}_V = \inner{v}{T^*w}_V$ for all $v$ and $w$, so $S = T^*$.

For existence, for $w \in W$ define $\psi_w \in V^*$ by $\psi_w(v) = \inner{Tv}{w}$, and let $\Psi_W \colon W \to V^*$ be the map $\Psi_W(w) = \psi_w$. Then define $T^* = \Phi_V\inv \circ \Psi_W$. Both $\Phi_V$ and $\Psi_W$ are (conjugate-)linear, so $T^*$ is linear. Furthermore we have
%
\begin{equation*}
    \inner{v}{T^*w}_V
        = \inner{v}{\Phi_V\inv \circ \Psi_W(w)}_V
        = \psi_w(v)
        = \inner{Tv}{w}_W
\end{equation*}
%
as required.

\fleuronbreak

An operator $U \colon V \to W$ is an \emph{isometry} if
%
\begin{equation*}
    \inner{Uv}{Uu}_W
        = \inner{v}{u}_V
\end{equation*}
%
for all $v,u \in V$. Clearly $U$ is injective. If $U$ is also surjective (i.e. if $\dim V = \dim W < \infty$), then it is called \emph{unitary}. Notice that if $U$ is an isometry, then
%
\begin{equation*}
    \inner{U^*Uv}{u}_V
        = \inner{Uv}{Uu}_W
        = \inner{v}{u}_V,
\end{equation*}
%
implying that $U^*U = \id_V$, and the converse clearly also holds. If $U$ is also surjective, then it is an isomorphism and so also $UU^* = \id_W$ (an operator with this property is called a \emph{coisometry}). In this case $U^* = U\inv$.

In the case $W = V$ we say that $T$ is \emph{normal} if $TT^* = T^*T$, and that $T$ is \emph{self-adjoint} if $T^* = T$. Clearly both self-adjoint and unitary operators (with $V = W$) are normal.

% There first of all exist such $L^*w \in V$ since, if $(v_1, \ldots, v_n)$ is an orthonormal basis for $V$, then
% %
% \begin{align*}
%     \inner{w}{Tv}_W
%         &= \inner[\bigg]{w}{\sum_{i=1}^n \inner{v}{v_i}_V Tv_i}_W \\
%         &= \sum_{i=1}^n \inner{v_i}{v}_V \inner{w}{Tv_i}_W \\
%         &= \inner[\bigg]{ \sum_{i=1}^n \inner{w}{Tv_i}_W v_i }{v}_V.
% \end{align*}
% %
% Hence we may choose $T^*w = \sum_{i=1}^n \inner{w}{Tv_i}_W v_i$. Furthermore, this vector is unique since \cref{eq:adjoint-defining-property} implies that
% %
% \begin{equation*}
%     T^*w
%         = \sum_{i=1}^n \inner{T^*w}{v_i}_V v_i
%         = \sum_{i=1}^n \inner{w}{Tv_i}_W v_i.
% \end{equation*}
% %
% In particular, $T^*w$ does not depend on a choice of basis for $V$. Notice that taking complex conjugates in \cref{eq:adjoint-defining-property} we find that $T^{**} = T$.


\begin{lemma}
    \label{lem:coordinate-map-isometry}
    Let $V$ and $W$ be finite-dimensional inner product spaces, and let $\calV$ and $\calW$ be ordered orthonormal bases for $V$ and $W$.
    %
    \begin{enumlem}
        \item The coordinate map $\coordmap{\calV}$ is unitary, i.e.
        %
        \begin{equation}
            \label{eq:coordinate-map-isometry}
            \inner{\coordvec{v}{\calV}}{\coordvec{u}{\calV}}
                = \inner{v}{u}
        \end{equation}
        %
        for all $v,u \in V$.
    \end{enumlem}
    %
    Let further $T \colon V \to W$ be a linear map, and let $A \in \mat{m,n}{\mathbb{K}}$.
    %
    \begin{enumlem}[resume]
        \item $(M_A)^* = M_{A^*}$. In particular, if $V = \mathbb{K}^n$ and $W = \mathbb{K}^m$ then $\smr{T^*} = \smr{T}^*$.
        \item $(\mr{\calW}{T}{\calV})^* = \mr{\calV}{T^*}{\calW}$.
    \end{enumlem}
\end{lemma}

\begin{proof}
\begin{proofsec}
    \item[(i)]
    By bi- or sesquilinearity of the inner product it suffices to prove \cref{eq:coordinate-map-isometry} for a basis for $V$. And writing $\calV = (v_1, \ldots, v_n)$ we find that
    %
    \begin{equation*}
        \inner{\coordvec{v_i}{\calV}}{\coordvec{v_j}{\calV}}
            = \inner{e_i}{e_j}
            = \delta_{ij}
            = \inner{v_i}{v_j}
    \end{equation*}
    %
    for $1 \leq i,j \leq n$.

    \item[(ii)]
    Notice that
    %
    \begin{equation*}
        \inner{M_{A^*} w}{v}
            = \inner{A^* w}{v}
            = v^* (A^* w)
            = (Av)^* w
            = \inner{w}{Av}
            = \inner{w}{M_A v}
    \end{equation*}
    %
    for all $v \in \mathbb{K}^n$ and $w \in \mathbb{K}^m$. By uniqueness of the adjoint operator, it follows that $(M_A)^* = M_{A^*}$. Furthermore, we have
    %
    \begin{equation*}
        M_{\smr{T^*}}
            = T^*
            = (M_{\smr{T}})^*
            = M_{\smr{T}^*}.
    \end{equation*}
    %
    It follows that $\smr{T^*} = \smr{T}^*$.

    \item[(iii)]
    Notice that
    %
    \begin{equation*}
        (\coordmap{\calW} \circ T \circ \coordmap{\calV}\inv)^*
            = (\coordmap{\calW} \circ T \circ \coordmap{\calV}^*)^*
            = \coordmap{\calV} \circ T^* \circ \coordmap{\calW}^*
            = \coordmap{\calV} \circ T^* \circ \coordmap{\calW}\inv,
    \end{equation*}
    %
    and taking standard matrix representations, it follows from (iii) that $(\mr{\calW}{T}{\calV})^* = \mr{\calV}{T^*}{\calW}$.
\end{proofsec}
\end{proof}


\begin{proposition}
    Let $V$ be a finite-dimensional inner product space, and let $T \in \calL(V)$ and $\lambda \in \mathbb{K}$. Then $\lambda \id_V - T$ is invertible if and only if $\conj{\lambda} \id_V - T^*$ is invertible. In other words, $\lambda$ is an eigenvalue of $T$ if and only if $\conj{\lambda}$ is an eigenvalue of $T^*$.
\end{proposition}

\begin{proof}
    Since the map $T \mapsto T^*$ is idempotent it suffices to prove one implication, so assume that $\lambda \id_V - T$ is invertible. Then there exists an $S \in \calL(V)$ such that
    %
    \begin{equation*}
        S(\lambda \id_V - T)
            = (\lambda \id_V - T)S
            = \id_V,
    \end{equation*}
    %
    and taking adjoints we find that
    %
    \begin{equation*}
        (\conj{\lambda} \id_V - T^*)S^*
            = S^*(\conj{\lambda} \id_V - T^*)
            = \id_V.
    \end{equation*}
    %
    That is, $\conj{\lambda} \id_V - T^*$ is invertible as claimed.
\end{proof}

\begin{remark}
    Note that this does \emph{not} say that $v \in V$ is an eigenvector of $T^*$ if it is an eigenvector of $T$. A counterexample is given by the matrix
    %
    \begin{equation*}
        A =
        \begin{pmatrix}
            1 & 1 \\
            0 & 0
        \end{pmatrix},
    \end{equation*}
    %
    which has the eigenvector $(1,0)$ with eigenvalue $1$. However, while $1$ is also an eigenvalue of the transpose $A\trans$ (with eigenvector $(1,1)$), $(1,0)$ is not an eigenvector of $A\trans$.

    While this does not hold in general, in \cref{enum:normal-adjoint-eigenvalues} we will see that it holds for \emph{normal} operators.
\end{remark}



\begin{proposition}
    \label{prop:complexification-adjoint}
    Let $V$ and $W$ be real inner product spaces, and let $T \in \calL(V,W)$. Then we have
    %
    \begin{equation*}
        (T^\complex)^*
            = (T^*)^\complex,
    \end{equation*}
    %
    i.e., the adjoint of the complexification of $T$ is the complexification of the adjoint of $T$. In particular
    %
    \begin{enumprop}
        \item $T$ is normal if and only if $T^\complex$ is normal, and
        \item $T$ is self-adjoint if and only if $T^\complex$ is self-adjoint.
    \end{enumprop}
\end{proposition}

\begin{proof}
    For $v,u,x,y \in V$ we have
    %
    \begin{align*}
        \inner{(T^*)^\complex(x + \iu y)}{v + \iu u}
            &= \inner{ T^*x + \iu T^*y }{v + \iu u} \\
            &= \inner{T^*x}{v}
                + \inner{T^*y}{u}
                + \iu ( \inner{T^*y}{u} - \inner{T^*x}{v} ) \\
            &= \inner{x}{Tv}
                + \inner{y}{Tu}
                + \iu ( \inner{y}{Tu} - \inner{x}{Tv} ) \\
            &= \inner{x + \iu y}{Tv + \iu Tu} \\
            &= \inner{x + \iu y}{T^\complex(v + \iu u)}.
    \end{align*}
    %
    Uniqueness of adjoints thus yields the claim.

    Assume that $T$ is normal. Then
    %
    \begin{equation*}
        T^\complex (T^\complex)^*
            = T^\complex (T^*)^\complex
            = (TT^*)^\complex
            = (T^*T)^\complex
            = (T^*)^\complex T^\complex
            = (T^\complex)^* T^\complex,
    \end{equation*}
    %
    so $T^\complex$ is normal. The converse follows similarly. If $T$ is self-adjoint, then
    %
    \begin{equation*}
        (T^\complex)^*
            = (T^*)^\complex
            = T^\complex,
    \end{equation*}
    %
    and similarly if $T^\complex$ is self-adjoint.
\end{proof}


\begin{lemma}
    \label{lemma:mr-eigenvalues}
    Let $V$ be a finite-dimensional vector space, let $T \in \calL(V)$, and let $\calV$ be an ordered basis for $V$. Then $v \in V$ is an eigenvector for $T$ if and only if $\coordvec{v}{\calV}$ is an eigenvector for $\mr{\calV}{T}{\calV}$ with the same eigenvalue.
\end{lemma}

\begin{proof}
    Let $\lambda \in \field$ be the eigenvalue of $v$. Then
    %
    \begin{equation*}
        \mr{\calV}{T}{\calV} \cdot \coordvec{v}{\calV}
            = \coordvec{Tv}{\calV}
            = \coordvec{\lambda v}{\calV}
            = \lambda \coordvec{v}{\calV}.
    \end{equation*}
    %
    For the converse, a similar calculation shows that  $\coordvec{Tv}{\calV} = \coordvec{\lambda v}{\calV}$. Since $\coordmap{\calV}$ is an isomorphism, it follows that $Tv = \lambda v$ as desired.
\end{proof}


\begin{lemma}
    \label{lem:complexification-eigenvalue}
    Let $V$ be a real vector space, and let $T \in \calL(V)$. If $\lambda \in \reals$ is an eigenvalue of the complexification $T^\complex$ of $T$, then $\lambda$ is also an eigenvalue of $T$.
\end{lemma}

\begin{proof}
    Let $v + \iu u \in V^\complex$ be an eigenvector of $T^\complex$ corresponding to $\lambda$. Then
    %
    \begin{equation*}
        Tv + \iu Tu
            = T^\complex (v + \iu u)
            = \lambda(v + \iu u)
            = \lambda v + \iu \lambda u.
    \end{equation*}
    %
    It follows that $Tv = \lambda v$ as desired.
\end{proof}


\begin{proposition}
    Let $T \in \calL(V)$ be a normal operator.
    %
    \begin{enumprop}
        \item \label{enum:normal-adjoint-norm} $\norm{Tv} = \norm{T^*v}$ for all $v \in V$.
        
        \item \label{enum:normal-adjoint-eigenvalues} If $\lambda \in \mathbb{K}$ is an eigenvalue of $T$, then $\conj{\lambda}$ is an eigenvalue of $T^*$ with the same eigenvectors. In other words, $E_T(\lambda) = E_{T^*}(\conj{\lambda})$.

        \item \label{enum:normal-orthogonal-eigenspaces} If $\mu \in \mathbb{K}$ is another eigenvalue of $T$ distinct from $\lambda$, then $E_T(\lambda)$ and $E_T(\mu)$ are orthogonal.

        \item \label{enum:self-adjoint-eigenvalues-exists-and-real} If $T$ is self-adjoint, then it has an eigenvalue and all its eigenvalues are real.

        \item \label{enum:unitary-eigenvalues-unit-circle} If $T$ is unitary, then all its eigenvalues lie on the unit circle $S^1 \subseteq \complex$.
    \end{enumprop}
\end{proposition}
%
In \cref{cor:self-adjoint-unitary-eigenvalue-characterisation} we will prove the converses of \subcref{enum:self-adjoint-eigenvalues-exists-and-real} and \subcref{enum:unitary-eigenvalues-unit-circle} under the assumption that $T$ is normal, using the spectral theorem (cf. \cref{thm:spectral-theorem}). We will use \subcref{enum:self-adjoint-eigenvalues-exists-and-real} in the proof of the spectral theorem, and we have proved \subcref{enum:unitary-eigenvalues-unit-circle} already to make explicit that it does not depend on the spectral theorem.

\begin{proof}
\begin{proofsec}
    \item[Proof of \subcref{enum:normal-adjoint-norm}]
    Notice that
    %
    \begin{equation*}
        \norm{Tv}^2
            = \inner{Tv}{Tv}
            = \inner{T^*Tv}{v}
            = \inner{TT^*v}{v}
            = \inner{T^*v}{T^*v}
            = \norm{T^*v}^2.
    \end{equation*}

    \item[Proof of \subcref{enum:normal-adjoint-eigenvalues}]
    If $T$ is normal then so is $\lambda \id_V - T$, so \subcref{enum:normal-adjoint-norm} implies that
    %
    \begin{equation*}
        \norm{(\lambda \id_V - T)v}
            = \norm{(\conj{\lambda} \id_V - T^*)v},
    \end{equation*}
    %
    so $v \in V$ is an eigenvector for $T$ with eigenvalue $\lambda$ if and only if $v$ is an eigenvector for $T^*$ with eigenvalue $\conj{\lambda}$.

    \item[Proof of \subcref{enum:normal-orthogonal-eigenspaces}]
    Let $v \in E_T(\lambda)$ and $u \in E_T(\mu)$. Since $w$ is also an eigenvector for $T^*$ with eigenvalue $\conj{\mu}$, we have
    %
    \begin{equation*}
        \lambda \inner{v}{u}
            = \inner{Tv}{u}
            = \inner{v}{T^*u}
            = \mu \inner{v}{u}.
    \end{equation*}
    %
    Since $\lambda \neq \mu$ we must have $\inner{v}{u} = 0$ as claimed.

    \item[Proof of \subcref{enum:self-adjoint-eigenvalues-exists-and-real}]
    If $T$ is self-adjoint and $v \in V$ is an eigenvector for $T$ with $\lambda \in \mathbb{K}$, then
    %
    \begin{equation*}
        \lambda \inner{v}{v}
            = \inner{Tv}{v}
            = \inner{v}{Tv}
            = \conj{\lambda} \inner{v}{v},
    \end{equation*}
    %
    and since $v \neq 0$ we must have $\lambda = \conj{\lambda}$. Hence $\lambda$ is real.

    If $\mathbb{K} = \complex$ then $V$ has a complex eigenvalue, which is real by the above argument. Assume instead that $\mathbb{K} = \reals$ and consider the complexification $T^\complex$ of $T$. This is self-adjoint by \cref{prop:complexification-adjoint}, so it has a real eigenvalue by the above. But then \cref{lem:complexification-eigenvalue} implies that this also is an eigenvalue of $T$.
    
    % Alternatively, let $\calV$ be an orthonormal basis for $V$ and consider the matrix representation $\mr{\calV}{T}{\calV}$. This is a real symmetric matrix, in particular a matrix with \emph{complex} entries, i.e. an operator $\complex^n \to \complex^n$. Hence it has a complex eigenvalue $\lambda$, which is real by the above. This means that $\lambda I - \mr{\calV}{T}{\calV}$ is singular when considered as an operator $\complex^n \to \complex^n$. But then it is clearly singular as an operator $\reals^n \to \reals^n$ since an inverse in $\calL(\reals^n)$ would also be an inverse in $\calL(\complex^n)$, so $\lambda$ is an eigenvalue of $T$.

    \item[Proof of \subcref{enum:unitary-eigenvalues-unit-circle}]
    Let $\lambda \in \mathbb{K}$ be an eigenvalue of $T$ with eigenvector $v$. Then
    %
    \begin{equation*}
        \inner{v}{v}
            = \inner{Tv}{Tv}
            = \inner{\lambda v}{\lambda v}
            = \lambda \conj{\lambda} \inner{v}{v}
            = \abs{\lambda}^2 \inner{v}{v},
    \end{equation*}
    %
    so $\abs{\lambda} = 1$.
\end{proofsec}
\end{proof}


Let $T \colon V \to V$ is an operator on an $\field$-vector space $V$, and let $U$ be a subspace of $V$ that is invariant under $T$. If $W$ is a complement of $V$, i.e. $V = U \oplus W$, then $W$ is not necessarily invariant under $T$. However, we have the following:

\begin{lemma}
    \label{thm:adjoint-invariant-subspace}
    Let $T \in \calL(V)$ be an operator on a finite-dimensional inner product space $V$. If a subspace $U$ of $V$ is invariant under $T$, then $U^\perp$ is invariant under $T^*$.
\end{lemma}

\begin{proof}
    Let $v \in U^\perp$. For $u \in U$ we have $Tu \in U$, so
    %
    \begin{equation*}
        \inner{T^*v}{u}
            = \inner{v}{Tu}
            = 0.
    \end{equation*}
    %
    Since this holds for all $u \in U$, it follows that $T^*v \in U^\perp$ as desired.
\end{proof}

\begin{lemma}
    \label{lem:spectral-lemma}
    $V$ be a finite-dimensional inner product space over $\mathbb{K}$, and consider $T \in \calL(V)$. If either
    %
    \begin{enumthm}
        \item $\mathbb{K} = \reals$ and $T$ is self-adjoint, or
        \item $\mathbb{K} = \complex$ and $T$ is normal,
    \end{enumthm}
    %
    then $T$ is orthogonally diagonalisable.
\end{lemma}

\begin{proof}
    Assume that either $\mathbb{K} = \reals$ and $T$ is self-adjoint, or that $\mathbb{K} = \complex$ and $T$ is normal. We prove by induction in $n = \dim V$ that $T$ is orthogonally diagonalisable. If $n = 1$ then this follows since $T$ has an eigenvalue, so assume that the claim is proved for operators on spaces of dimension strictly less than $n$.

    Let $\lambda \in \spec T$, and consider the corresponding eigenspace $E_T(\lambda)$. If $d \defn \dim E_T(\lambda) = n$, then any orthonormal basis of $E_T(\lambda)$ will suffice. Assume therefore that $0 < d < n$.

    The space $E_T(\lambda) = E_{T^*}(\conj{\lambda})$ is clearly invariant under both $T$ and $T^*$. It follows from \cref{thm:adjoint-invariant-subspace} that $E_T(\lambda)^\perp$ is also invariant under both $T$ and $T^*$. We furthermore have $\dim E_T(\lambda)^\perp = n-d$ and $0 < n-d < n$. Let $T_\parallel \in \calL(E_T(\lambda))$ and $T_\perp \in \calL(E_T(\lambda)^\perp)$ denote the restrictions of $T$ to $E_T(\lambda)$ and $E_T(\lambda)^\perp$ respectively. Both $T_\parallel$ and $T_\perp$ are also self-adjoint or normal, depending on the hypothesis, so the induction hypothesis furnishes orthonormal bases $\calU$ and $\calW$ for $E_T(\lambda)$ and $E_T(\lambda)^\perp$ consisting of eigenvectors of $T$. But then $\calV = \calU \union \calW$ is an orthonormal basis for $V$ as desired.
\end{proof}


\begin{theorem}[The spectral theorem]
    \label{thm:spectral-theorem}
    Let $V$ be a finite-dimensional inner product space over $\mathbb{K}$, and let $T \in \calL(V)$. Then the following are equivalent:
    %
    \begin{enumthm}
        \item \label{enum:spectral-selfadjoint-normal} $\mathbb{K} = \reals$ and $T$ is self-adjoint, or $\mathbb{K} = \complex$ and $T$ is normal.
        
        \item \label{enum:spectral-orthogonally-diagonalisable} $T$ is orthogonally diagonalisable.

        \item \label{enum:spectral-operator-decomposition} $T$ has the orthogonal spectral resolution
        %
        \begin{equation*}
            T
                = \sum_{\lambda \in \spec T} \lambda P_\lambda,
        \end{equation*}
        %
        where $P_\lambda$ is the orthogonal projection onto the eigenspace $E_T(\lambda)$. In particular, $V$ is an orthogonal direct sum of the eigenspaces of $T$, i.e.
        %
        \begin{equation*}
            V
                = \bigodot_{\lambda \in \spec T} E_T(\lambda).
        \end{equation*}

        \item \label{enum:spectral-multiplication-operator} $T$ is unitarily (when $\mathbb{K} = \complex$) or orthogonally (when $\mathbb{K} = \reals$) equivalent to a multiplication operator $M_A \in \calL(\mathbb{K}^n)$ where $A$ is a diagonal matrix, and the diagonal of $A$ contains the eigenvalues of $T$ with multiplicity. If $\calV$ is an ordered orthonormal basis for $V$ consisting of eigenvectors for $T$, then we may choose $A = \mr{\calV}{T}{\calV}$ and
        %
        \begin{equation*}
            T
                = \coordmap{\calV}\inv \circ M_A \circ \coordmap{\calV},
        \end{equation*}
        %
        with $\coordmap{\calV}$ unitary.
    \end{enumthm}
\end{theorem}
%
Note that the first part of property \subcref{enum:spectral-operator-decomposition} means that
%
\begin{equation*}
    \id_V
        = \sum_{\lambda \in \spec T} P_\lambda
\end{equation*}
%
is a resolution of the identity, i.e. that $P_\lambda P_\mu = 0$ for $\lambda \neq \mu$, and that this is composed of orthogonal projections.

\begin{proof}
\begin{proofsec}
    \item[\subcref{enum:spectral-selfadjoint-normal} $\implies$ \subcref{enum:spectral-orthogonally-diagonalisable}]
    This is just \cref{lem:spectral-lemma}.

    \item[\subcref{enum:spectral-selfadjoint-normal} \& \subcref{enum:spectral-orthogonally-diagonalisable} $\implies$ \subcref{enum:spectral-operator-decomposition}]
    The first claim says that distinct eigenspaces are orthogonal, which is just a restatement of \cref{enum:normal-orthogonal-eigenspaces}. To prove the second, let $\calV = (v_1, \ldots, v_n)$ be an orthonormal basis for $V$ consisting of eigenvectors for $T$, and let $\lambda_1, \ldots, \lambda_n$ be the corresponding eigenvalues. Then for any $v = \alpha_1 v_1 + \cdots + \alpha_n v_n$ we have $P_{\lambda_i} v = \alpha_i v_i$, so
    %
    \begin{equation*}
        \biggl( \sum_{\lambda \in \spec T} P_\lambda \biggr) v
            = \sum_{\lambda \in \spec T} P_\lambda v
            = \sum_{i=1}^n \alpha_i v_i
            = v.
    \end{equation*}
    %
    For the third claim, notice that
    %
    \begin{equation*}
        \biggl( \sum_{\lambda \in \spec T} \lambda P_\lambda \biggr) v
            = \sum_{\lambda \in \spec T} \lambda P_\lambda v
            = \sum_{i=1}^n \lambda_i \alpha_i v_i
            = \sum_{i=1}^n \alpha_i Tv_i
            = Tv.
    \end{equation*}
    %
    The final claim follows from the first two.

    \item[\subcref{enum:spectral-operator-decomposition} $\implies$ \subcref{enum:spectral-orthogonally-diagonalisable}]
    This follows from the decomposition of $V$ into an orthogonal sum of eigenspaces, by constructing an orthonormal basis for each eigenspace.

    \item[\subcref{enum:spectral-orthogonally-diagonalisable} $\implies$ \subcref{enum:spectral-multiplication-operator}]
    Let $\calV = (v_1, \ldots, v_n)$ be an ordered orthonormal basis for $\calV$ consisting of eigenvectors for $T$ with corresponding eigenvalues $\lambda_1, \ldots, \lambda_n$, and consider the matrix representation $\mr{\calV}{T}{\calV}$. If $(e_1, \ldots, e_n)$ is the standard basis on $\mathbb{K}^n$, then \cref{lemma:mr-eigenvalues} implies that the vectors $\coordvec{v_i}{\calV} = e_i$ are eigenvectors for $\mr{\calV}{T}{\calV}$. Hence $\mr{\calV}{T}{\calV}$ is diagonal, so the basis representation $\coordmap{\calV} \circ T \circ \coordmap{\calV}\inv$ is multiplication by a diagonal matrix. Next notice that
    %
    \begin{equation*}
        T
            = \coordmap{\calV}\inv \circ (\coordmap{\calV} \circ T \circ \coordmap{\calV}\inv) \circ \coordmap{\calV},
    \end{equation*}
    %
    so it suffices to show that $\coordmap{\calV}$ is unitary (orthogonal). But this follows by \cref{lem:coordinate-map-isometry}.

    \item[\subcref{enum:spectral-multiplication-operator} $\implies$ \subcref{enum:spectral-selfadjoint-normal}]
    First assume that $\mathbb{K} = \complex$. Since $\coordmap{\calV}$ is unitary we have $\coordmap{\calV}\inv = \coordmap{\calV}^*$, so
    %
    \begin{equation*}
        T^*
            = (\coordmap{\calV}^* \circ M_A \circ \coordmap{\calV})^*
            = \coordmap{\calV}^* \circ M_A^* \circ \coordmap{\calV}
            = \coordmap{\calV}\inv \circ M_{A^*} \circ \coordmap{\calV}.
    \end{equation*}
    %
    Since $A$ is diagonal, $T$ clearly commutes with $T^*$, hence is normal.

    If instead $\mathbb{K} = \reals$, the same argument shows that $T^* = \coordmap{\calV}\inv \circ M_{A\trans} \circ \coordmap{\calV}$, but since $A$ is diagonal this is just $T$, so $T$ is self-adjoint.
\end{proofsec}
\end{proof}


\begin{corollary}
    \label{cor:self-adjoint-unitary-eigenvalue-characterisation}
    Let $T \in \calL(V)$ be a normal operator.
    %
    \begin{enumcor}
        \item \label{enum:self-adjoint-eigenvalue-characterisation} $T$ is self-adjoint if and only if $\spec T \subseteq \reals$.
        \item \label{enum:unitary-eigenvalue-characterisation} $T$ is unitary if and only if $\spec T \subseteq S^1$.
    \end{enumcor}
\end{corollary}

\begin{proof}
\begin{proofsec}
    \item[Proof of \subcref{enum:self-adjoint-eigenvalue-characterisation}]
    The \enquote{only if} part follows from \cref{enum:self-adjoint-eigenvalues-exists-and-real}, so assume that $\spec T \subseteq \reals$ and notice that
    %
    \begin{equation*}
        T^*
            = \biggl( \sum_{\lambda \in \spec T} \lambda P_\lambda \biggr)^*
            = \sum_{\lambda \in \spec T} \conj{\lambda} P_\lambda^*
            = \sum_{\lambda \in \spec T} \lambda P_\lambda,
    \end{equation*}
    %
    since each $\lambda \in \reals$, and each $P_\lambda$ is an orthogonal projection, hence self-adjoint.
    
    Alternatively, choose a diagonal matrix $A \in \mat{n}{\mathbb{K}}$ in accordance with \cref{enum:spectral-multiplication-operator}. Since the diagonal of $A$ contains the eigenvalues of $T$, we have $A^* = A$, and so it follows that $T^* = T$.

    \item[Proof of \subcref{enum:unitary-eigenvalue-characterisation}]
    Similarly, the \enquote{only if} part is just \cref{enum:unitary-eigenvalues-unit-circle}. Assume that $\spec T \subseteq S^1$ and notice that
    %
    \begin{equation*}
        T^*
            = \sum_{\lambda \in \spec T} \conj{\lambda} P_\lambda.
    \end{equation*}
    %
    Since the projections $P_\lambda$ are pairwise orthogonal, we have
    %
    \begin{equation*}
        T^*T
            = \sum_{\lambda \in \spec T} \conj{\lambda} \lambda P_\lambda
            = \sum_{\lambda \in \spec T} \abs{\lambda}^2 P_\lambda
            = \sum_{\lambda \in \spec T} P_\lambda
            = \id_V,
    \end{equation*}
    %
    so $U$ is unitary.
    
    Alternatively, let $A$ be as above. Then all diagonal elements in $A$ are nonzero, so $A$ is invertible, and we clearly have $A^* A = I_n$. Hence also $T^* T = \id_V$, so $T$ is unitary.
\end{proofsec}
\end{proof}


\nocite{*}

\printbibliography

\end{document}