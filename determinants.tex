\chapter{Determinants}

\section{Existence and uniqueness}\label{sec:determinant-existence-uniqueness}

We begin by establishing some terminology and some basic properties of maps between modules. If $M_1, \ldots, M_n, N$ are modules over a commutative ring $R$, a map
%
\begin{equation*}
    \phi \colon M_1 \prod \cdots \prod M_n \to N
\end{equation*}
%
is called \keyword{$n$-linear} if, for all $i$, the maps $m_i \mapsto \phi(m_1, \ldots, m_n)$ are linear for all choices of $m_j \in M_j$ where $j \neq i$. Since there is a natural isomorphism $\mat{m,n}{R} \cong (R^n)^m$, a map $\phi \colon \mat{m,n}{R} \to N$ that is linear in each row is also called $n$-linear.

In the case $M_1 = \cdots = M_n$, we call $\phi$ \keyword{alternating} if $\phi(m_1, \ldots, m_n) = 0$ whenever $m_i = m_j$ for some $i \neq j$. Furthermore, $\phi$ is called \keyword{skew-symmetric} if
%
\begin{multline*}
    \phi(m_1, \ldots, m_{i-1}, m_i, m_{i+1}, \ldots, m_{j-1}, m_j, m_{j+1}, \ldots, m_n) \\
        = -\phi(m_1, \ldots, m_{i-1}, m_j, m_{i+1}, \ldots, m_{j-1}, m_i, m_{j+1}, \ldots, m_n)
\end{multline*}
%
for all $i < j$.

With this terminology at hand, we can now define determinants:

\begin{definition}[Determinant functions]
    If $n$ be a positive integer, a \keyword{determinant function} is a map $\phi \colon \mat{n}{R} \to R$ that is $n$-linear, alternating, and which satisfies $\phi(I_n) = 1$.
\end{definition}


Before proceeding with proving the existence of determinants, we need the following lemma:

\begin{lemma}
    Let $M$ and $N$ be $R$-modules, and let $\phi \colon M^n \to N$ be an $n$-linear map.
    %
    \begin{enumlemma}
        \item \label{enum:alternating-implies-skew-symmetric} If $\phi$ is alternating, then $\phi$ is skew-symmetric. If $\chr R \neq 2$ then the converse also holds.
        \item \label{enum:alternating-adjacent-rows} If $\phi(m_1, \ldots, m_n) = 0$ whenever $m_i = m_{i+1}$ for some $i = 1, \ldots, n-1$, then $\phi$ is alternating.
    \end{enumlemma}
\end{lemma}
%
We shall not use the converse direction of \cref{enum:alternating-implies-skew-symmetric} but we include it for completeness.

\begin{proof}
\begin{proofsec*}
    \item[\itemref{enum:alternating-implies-skew-symmetric}]
    Consider $m_1, \ldots, m_n \in M$, and let $1 \leq i < j \leq n$. Define a map $\psi \colon M \prod M \to N$ by
    %
    \begin{equation*}
        \psi(a, b)
            = \phi(m_1, \ldots, m_{i-1}, a, m_{i+1}, \ldots, m_{j-1}, b, m_{j+1}, \ldots, m_n),
    \end{equation*}
    %
    and notice that it suffices to show that $\psi(m_i,m_j) = -\psi(m_j,m_i)$. But $\psi$ is $2$-linear and alternating, so for $a,b \in M$ we have
    %
    \begin{equation*}
        \psi(a+b, a+b)
            = \psi(a,a) + \psi(a,b) + \psi(b,a) + \psi(b,b)
            = \psi(a,b) + \psi(b,a).
    \end{equation*}
    %
    Thus $\psi(m_i,m_j) = -\psi(m_j,m_i)$, so $\phi$ is skew-symmetric as claimed.

    Conversely, if $\chr R \neq 2$ and $\psi$ is skew-symmetric, then since $\psi(a,b) = -\psi(b,a)$, letting $a = b$ we have $2\psi(a,a) = 0$, so $\psi(a,a) = 0$.

    \item[\itemref{enum:alternating-adjacent-rows}] 
    The argument above shows that, in particular, if $A, B \in M^n$, and $B$ is obtained from $A$ by interchanging two adjacent elements, then $\phi(B) = -\phi(A)$. Assuming now that $B$ is obtained from $A$ by interchanging the $i$th and $j$th elements in $A$, with $i < j$, we claim that we may obtain $B$ by successively interchanging adjacent elements of $A$. Writing $A = (m_1, \ldots, m_n)$, we first perform $j - i$ such interchanges and arrive that the tuple
    %
    \begin{equation*}
        (m_1, \ldots, m_{i-1}, m_{i+1}, \ldots, m_{j-1}, m_j, m_i, m_{j+1}, \ldots, m_n),
    \end{equation*}
    %
    moving $m_i$ to the right $j - i$ places. Next we perform another $j-i-1$ interchanges, moving $m_j$ to the left until we reach
    %
    \begin{equation*}
        B = (m_1, \ldots, m_{i-1}, m_j, m_{i+1}, \ldots, m_{j-1}, m_i, m_{j+1}, \ldots, m_n).
    \end{equation*}
    %
    Since each interchange results in a sign change, we have
    %
    \begin{equation*}
        \phi(B) = (-1)^{2(j-i) - 1} \phi(A) = -\phi(A).
    \end{equation*}
    %
    If $m_i = m_j$ for $i < j$, then we claim that $\phi(A) = 0$. For let $B$ be obtained from $A$ by interchanging $m_{i+1}$ and $m_j$. Then $\phi(B) = 0$, so $\phi(A) = -\phi(B) = 0$ by the above argument, and hence $\phi$ is alternating as claimed.
    \end{proofsec*}
\end{proof}

We now proceed with constructing determinants. If $A \in \mat{n}{R}$ with $n > 1$ and $1 \leq i,j \leq n$, denote by $M(A)_{i,j}$ the matrix in $\mat{n-1}{R}$ obtained by removing the the $i$th row and the $j$th column of $A$. This is called the \keyword{$(i,j)$-th minor} of $A$. If $\phi \colon \mat{n-1}{R} \to R$ is an $(n-1)$-linear function and $A \in \mat{n}{R}$, then we write $\phi_{i,j}(A) = \phi(M(A)_{i,j})$. Then $\phi_{i,j} \colon \mat{n}{R} \to R$ is clearly linear in all rows except row $i$, and is independent of row $i$.

We construct determinants recursively, using the Laplace expansion:

\begin{theorem}[Construction of determinants]
    \label{thm:determinant-recursive-definition}
    Let $n > 1$, and let $\phi \colon \mat{n-1}{R} \to R$ be alternating and $(n-1)$-linear. For $j = 1, \ldots, n$ define a map $\psi_j \colon \mat{n}{R} \to R$ by
    %
    \begin{equation*}
        \psi_j(A)
            = \sum_{i=1}^n (-1)^{i+j} a_{ij} \phi_{i,j}(A),
    \end{equation*}
    %
    for $A = (a_{ij}) \in \mat{n}{R}$. Then $\psi_j$ is alternating and $n$-linear. If $\phi$ is a determinant function, then so is $\psi_j$.
\end{theorem}

\begin{proof}
    Let $A = (a_{ij}) \in \mat{n}{R}$. Then $A \mapsto a_{ij}$ is independent of all rows except row $i$, and $\phi_{i,j}$ is linear in all rows except row $i$. Thus $A \mapsto a_{ij} \phi_{i,j}(A)$ is linear in all rows except row $i$. Conversely, $A \mapsto a_{ij}$ is linear in row $i$, and $\phi_{i,j}$ is independent of row $i$, so $A \mapsto a_{ij} \phi_{i,j}(A)$ is also linear in row $i$. Since $\psi_j$ is a linear combination of $n$-linear maps, is it itself $n$-linear.

    Now assume that $A$ has two equal adjacent rows, say $a_k, a_{k+1} \in R^n$. If $i \neq k$ and $i \neq k+1$, then $M(A)_{i,j}$ has two equal rows, so $\phi_{i,j}(A) = 0$. Thus
    %
    \begin{equation*}
        \psi_j(A)
            = (-1)^{k+j} a_{kj} \phi_{k,j}(A)
              + (-1)^{k+1+j} a_{(k+1)j} \phi_{k+1,j}(A).
    \end{equation*}
    %
    Since $a_k = a_{k+1}$ we also have $a_{kj} = a_{(k+1)j}$ and $M(A)_{k,j} = M(A)_{k+1,j}$. Thus $\psi_j(A) = 0$, so \cref{enum:alternating-adjacent-rows} implies that $\psi_j$ is alternating.

    Finally suppose that $\phi$ is a determinant function. Then $M(I_n)_{j,j} = I_{n-1}$ and we have
    %
    \begin{equation*}
        \psi_j(I_n)
            = (-1)^{j+j} \phi_{j,j}(I_n)
            = \phi(I_{n-1})
            = 1,
    \end{equation*}
    %
    so $\psi_j$ is also a determinant function.
\end{proof}


\begin{corollary}[Existence of determinants]
    For every positive integer $n$, there exists a determinant function $\mat{n}{R} \to R$.
\end{corollary}

\begin{proof}
    The identity map on $\mat{1}{R} \cong R$ is a determinant function for $n = 1$, and \cref{thm:determinant-recursive-definition} allows us to recursively construct a determinant for each $n > 1$.
\end{proof}

We finally show that determinants are unique by showing that any determinant function must be given by the Leibniz formula:

\begin{theorem}[Uniqueness of determinants]
    \label{thm:determinant-uniqueness}
    Let $n$ be a positive integer. There is precisely one determinant function on $\mat{n}{R}$, namely the function $\det \colon \mat{n}{R} \to R$ given by
    %
    \begin{equation*}
        \det A
            = \sum_{\sigma \in S_n} (\sign\sigma) a_{1 \sigma(1)} \cdots a_{n \sigma(n)}
    \end{equation*}
    %
    for $A = (a_{ij}) \in \mat{n}{R}$. If $\phi \colon \mat{n}{R} \to R$ is any alternating $n$-linear function, then
    %
    \begin{equation*}
        \phi(A)
            = (\det A) \phi(I_n).
    \end{equation*}
\end{theorem}
%
We use the notation $\det$ for the unique determinant on $\mat{n}{R}$ for all $n$.

\begin{proof}
    Let $e_1, \ldots, e_n$ denote the rows of $I_n$, and denote the rows of a matrix $A = (a_{ij}) \in \mat{n}{R}$ by $a_1, \ldots, a_n$. Then $a_i = \sum_{j=1}^n a_{ij} e_j$, so
    %
    \begin{equation*}
        \phi(A)
            = \sum_{k_1, \ldots, k_n} a_{1k_1} \cdots a_{nk_n} \phi(e_{k_1}, \ldots, e_{k_n}),
    \end{equation*}
    %
    where the sum is taken over all $k_i = 1, \ldots, n$. Since $\phi$ is alternating we have $\phi(e_{k_1}, \ldots, e_{k_n}) = 0$ if two of the indices $k_1, \ldots, k_n$ are equal. Thus it suffices to sum over those sequences $(k_1, \ldots, k_n)$ that are permutations of $(1, \ldots, n)$, and so
    %
    \begin{equation*}
        \phi(A)
            = \sum_{\sigma \in S_n} a_{1 \sigma(1)} \cdots a_{n \sigma(n)} \phi(e_{\sigma(1)}, \ldots, e_{\sigma(n)}).
    \end{equation*}
    %
    Next notice that, since $\phi$ is also skew-symmetric by \cref{enum:alternating-implies-skew-symmetric}, we have $\phi(e_{\sigma(1)}, \ldots, e_{\sigma(n)}) = (-1)^m \phi(e_1, \ldots, e_n)$, where $m$ is the number of transpositions of $(1, \ldots, n)$ it takes to obtain the permutation $(\sigma(1), \ldots, \sigma(n))$. But then $(-1)^m$ is just the sign of $\sigma$, so
    %
    \begin{equation*}
        \phi(A)
            = \sum_{\sigma \in S_n} (\sign\sigma) a_{1 \sigma(1)} \cdots a_{n \sigma(n)} \phi(I_n).
    \end{equation*}
    %
    Finally, if $\phi$ is a determinant function, then $\phi(I_n) = 1$, so we must have $\phi = \det$. The rest of the theorem follows directly from this.
\end{proof}


\section{Properties of determinants}

We begin with what is surely the most important property of determinants, which is also our first application of the uniqueness theorem for determinants:

\begin{theorem}
    \label{thm:determinant-multiplicative}
    Let $A,B \in \mat{n}{R}$. Then
    %
    \begin{equation*}
        \det AB
            = (\det A) (\det B).
    \end{equation*}
    %
    In particular, $\det \colon \matGL{n}{R} \to R^*$ is a group homomorphism.
\end{theorem}

\begin{proof}
    The map $\phi \colon \mat{n}{R} \to R$ given by $\phi(A) = \det AB$ is clearly $n$-linear and alternating. Hence $\phi(A) = (\det A) \phi(I)$, and $\phi(I) = \det B$.

    Furthermore, if $A$ is invertible, then $1 = \det I = (\det A) (\det \inv{A})$. Thus $\det A \in R^*$, so $\det$ is a group homomorphism as claimed.
\end{proof}


\begin{corollary}
    \label{cor:determinant-similar-matrices}
    If $A,B \in \mat{n}{\fieldF}$ are similar matrices, then $\det A = \det B$.
\end{corollary}

\begin{proof}
    Let $P \in \mat{n}{\fieldF}$ be such that $B = \inv{P}AP$. \Cref{thm:determinant-multiplicative} then implies that
    %
    \begin{equation*}
        \det B
            = (\det \inv{P})(\det A)(\det P)
            = (\det A)(\det \inv{P}P)
            = \det A.
    \end{equation*}
\end{proof}

\Cref{cor:determinant-similar-matrices} allows us to define the determinant of a general linear operator $T \colon V \to V$ on a finite-dimensional $\fieldF$-vector space. If $\calV$ and $\calW$ are bases for $V$, then the matrix representations $\mr{\calV}{T}{\calV}$ and $\mr{\calW}{T}{\calW}$ are similar. This allows us to define the determinant $\det T$ of $T$ as the matrix representation $\mr{\calV}{T}{\calV}$ for any basis $\calV$.

Next the fairly obvious result that the determinant of a matrix equals the determinant of its transpose:

\begin{proposition}
    Let $A \in \mat{n}{R}$. Then $\det A = \det \trans{A}$.
\end{proposition}

\begin{proof}
    Writing $A = (a_{ij})$, first notice that
    %
    \begin{equation*}
        \det \trans{A}
            = \sum_{\sigma \in S_n} (\sign \inv{\sigma}) a_{\sigma(1)1} \cdots a_{\sigma(n)n},
    \end{equation*}
    %
    since $\sign \sigma = \sign \inv{\sigma}$. Next notice that, if $j = \sigma(i)$, then $a_{\sigma(i)i} = a_{j \inv{\sigma}(j)}$. Since $R$ is commutative, it follows that
    %
    \begin{equation*}
        \det \trans{A}
            = \sum_{\sigma \in S_n} (\sign \inv{\sigma}) a_{1\inv{\sigma}(1)} \cdots a_{n\inv{\sigma}(n)},
    \end{equation*}
    %
    and since $\sigma \mapsto \inv{\sigma}$ is a bijection on $S_n$, it follows that $\det \trans{A} = \det A$ as desired.
\end{proof}


Let $A \in \mat{n}{R}$. For $1 \leq i,j \leq n$, the \keyword{$(i,j)$-th cofactor} of $A$ is the number $A_{i,j} = (-1)^{i+j} \det M(A)_{i,j}$, where we recall that $M(A)_{i,j}$ is the $(i,j)$-th minor of $A$. The \keyword{cofactor matrix} of $A$ is the matrix $\cof A \in \mat{n}{R}$ whose $(i,j)$-th entry is the cofactor $A_{i,j}$. Note that
%
\begin{equation*}
    (\trans{A})_{i,j}
        = (-1)^{i+j} \det M(\trans{A})_{i,j}
        = (-1)^{j+i} \det M(A)_{j,i}
        = A_{j,i},
\end{equation*}
%
so $\cof \trans{A} = \trans{(\cof A)}$. Of greater importance than the cofactor matrix is the \keyword{adjoint matrix} of $A$, written $\adj A$, which is just the transpose of $\cof A$. That is, the $(i,j)$-th entry of $\adj A$ is the cofactor $A_{j,i}$. Similar to the cofactor matrix we have
%
\begin{equation*}
    \adj \trans{A}
        = \trans{(\cof \trans{A})}
        = \cof A
        = \trans{(\adj A)}.
\end{equation*}
%
We then have the following:

\begin{proposition}
    \label{thm:adjoint-matrix-product}
    Let $A \in \mat{n}{R}$. Then
    %
    \begin{equation*}
        (\adj A) A
            = (\det A) I
            = A (\adj A).
    \end{equation*}
\end{proposition}

\begin{proof}
    Writing $A = (a_{ij})$ and fixing some $j \in \{1, \ldots, n\}$, \cref{thm:determinant-recursive-definition} implies that
    %
    \begin{equation*}
        \det A
            = \sum_{i=1}^n (-1)^{i+j} a_{ij} \det M(A)_{i,j}
            = \sum_{i=1}^n a_{ij} A_{i,j},
    \end{equation*}
    %
    which is just the $(j,j)$-th entry in the product $(\adj A)A$.

    Next we claim that if $k \neq j$, then $\sum_{i=1}^n a_{ik} A_{i,j} = 0$. Let $B = (b_{ij}) \in \mat{n}{R}$ be the matrix obtained from $A$ by replacing the $j$th column of $A$ by its $k$th column. Then $B$ has two equal columns, so $\det B = 0$. Also, $b_{ij} = a_{ik}$ and $M(B)_{i,j} = M(A)_{i,j}$, so it follows that
    %
    \begin{align*}
        0
            &= \det B
             = \sum_{i=1}^n (-1)^{i+j} b_{ij} \det M(B)_{i,j} \\
            &= \sum_{i=1}^n (-1)^{i+j} a_{ik} \det M(A)_{i,j}
             = \sum_{i=1}^n a_{ik} A_{i,j}.
    \end{align*}
    %
    That is, the $(j,k)$-th entry of the product $(\adj A)A$ is zero, so the off-diagonal entries of $(\adj A)A$ are zero. In total we thus have $(\adj A)A = (\det A) I$.

    Finally we prove the equality $A(\adj A) = (\det A) I$, Applying the first equality to $\trans{A}$ yields
    %
    \begin{equation*}
        (\adj \trans{A}) \trans{A}
            = (\det \trans{A})I
            = (\det A)I,
    \end{equation*}
    %
    and transposing we get
    %
    \begin{equation*}
        A (\adj A)
            = A \trans{(\adj \trans{A})}
            = (\det A) I
    \end{equation*}
    %
    as desired.
\end{proof}

\begin{corollary}
    Let $A \in \mat{n}{R}$. The following are equivalent:
    %
    \begin{enumcorollary}
        \item $A$ is a (two-sided) unit in $\mat{n}{R}$.
        \item $A$ is a left- or right-unit in $\mat{n}{R}$.
        \item $\det A$ is a unit in $R$.
    \end{enumcorollary}
\end{corollary}

\begin{proof}
    If $A$ is e.g. a left-unit, then \cref{thm:determinant-multiplicative} implies that
    %
    \begin{equation*}
        1
            = \det I_n
            = (\det A)(\det \inv{A}),
    \end{equation*}
    %
    so $\det A$ is a unit in $R$. Conversely, if $\det A$ is a unit then \cref{thm:adjoint-matrix-product} implies that $\inv{(\det A)}(\adj A)$ is a two-sided inverse of $A$.
\end{proof}

Notice that this gives us a second proof of the fact that a matrix is invertible just when it has either a left- or right-inverse. In fact, we see that this holds for matrices with entries in any commutative ring.


We close this section by proving a result on the determinant of a block matrix:

\begin{proposition}
    \label{prop:block-matrix-determinant}
    Let $A_{11}, \ldots, A_{nn}$ be square matrices with entries in $R$ and consider the block matrix
    %
    \begin{equation*}
        M
            = \begin{pmatrix}
                A_{11} & A_{12} & \cdots & A_{1n} \\
                0      & A_{22} & \cdots & A_{2n} \\
                \vdots & \ddots & \ddots & \vdots \\
                0      & \cdots & 0      & A_{nn}
            \end{pmatrix},
    \end{equation*}
    %
    where the remaining $A_{ij}$ are matrices of appropriate dimensions. Then $\det M = \bigprod_{i=1}^n \det A_{ii}$.
\end{proposition}

\begin{proof}
    By induction it suffices to consider the case where $M$ has the block form
    %
    \begin{equation*}
        M
            = \begin{pmatrix}
                A & C \\
                0 & B
            \end{pmatrix},
    \end{equation*}
    %
    where $A \in \mat{r}{R}$, $B \in \mat{s}{R}$ and $C \in \mat{r,s}{R}$ for appropriate integers $r,s$. Notice that if we define the matrices
    %
    \begin{equation*}
        M_1
            = \begin{pmatrix}
                I_r & 0 \\
                0   & B
            \end{pmatrix}
        \quad \text{and} \quad
        M_2
            = \begin{pmatrix}
                A & C   \\
                0 & I_s
            \end{pmatrix},
    \end{equation*}
    %
    then $M = M_1 M_2$. But using \cref{thm:determinant-recursive-definition} we easily see that $\det M_1 = \det B$ and $\det M_2 = \det A$, so it follows that
    %
    \begin{equation*}
        \det M
            = (\det M_1) (\det M_2)
            = (\det A) (\det B)
    \end{equation*}
    %
    as desired.
\end{proof}


\section{Cross products}

We now study rigorously the well-known

\begin{definition}[Cross products]
    Let $v = (\alpha_1, \alpha_2, \alpha_3)$ and $w = (\beta_1, \beta_2, \beta_3)$ be vectors in $\reals^3$. The \keyword{cross product} of $v$ and $w$ is the vector
    %
    \begin{equation*}
        v \crossp w =
        \begin{pmatrix}
            \alpha_2 \beta_3 - \alpha_3 \beta_2 \\
            \alpha_3 \beta_1 - \alpha_1 \beta_3 \\
            \alpha_1 \beta_2 - \alpha_2 \beta_1
        \end{pmatrix}.
    \end{equation*}
\end{definition}
%
Denote the standard basis on $\reals^3$ by $\calE = (e_1, e_2, e_3)$. We easily see that $e_i \crossp e_j = e_k$ when $(i,j,k)$ is a cyclic permutation of $(1,2,3)$. Perhaps surprisingly, there is an important connection between cross products and determinants which from which will follow most of the elementary properties of cross products:

\begin{lemma}
    \label{lem:cross-product-determinant}
    Let $v,w,u \in \reals^3$. Then
    %
    \begin{equation*}
        \inner{u}{v \crossp w}
            = \det(u,v,w).
    \end{equation*}
\end{lemma}

\begin{proof}
    By multilinearity of the inner product and of determinants, it suffices to prove the lemma when $u$ is a basis vector. But it is clear that
    %
    \begin{equation*}
        \inner{e_i}{v \crossp w}
            = \det(e_i,v,w),
    \end{equation*}
    %
    as desired.
\end{proof}
%
The product $\inner{u}{v \crossp w}$ is called the \keyword{(scalar) triple product} of $u$, $v$ and $w$, and is denoted $[u,v,w]$. We call it the \emph{scalar} triple product to distinguish it from the \emph{vector} triple product $u \crossp (v \crossp w)$, whose properties we will examine in \cref{cor:vector-triple-product}. The scalar triple product has some very nice properties summarised in the following proposition:

\begin{proposition}
    Let $u,v,w \in \reals^3$.
    %
    \begin{enumproposition}
        \item The cross product map $(v,w) \mapsto v \crossp w$ is bilinear.

        \item $v \crossp w = - w \crossp v$.

        \item The triple product $[u,v,w]$ is invariant under cyclic permutations, i.e.
        %
        \begin{equation*}
            [u,v,w]
                = [v,w,u]
                = [w,u,v]
        \end{equation*}
        %
        and invariant under interchange of inner product and cross product, i.e.
        %
        \begin{equation*}
            \inner{u}{v \crossp w}
                = [u,v,w]
                = \inner{u \crossp v}{w}.
        \end{equation*}

        \item $v \crossp w = 0$ if and only if $v$ and $w$ are linearly dependent.

        \item $v \crossp w$ is orthogonal to both $v$ and $w$.
    \end{enumproposition}
\end{proposition}

\begin{proof}
    The first three claims follow from \cref{lem:cross-product-determinant} since the determinant is multilinear and alternating (hence skew-symmetric).
    
    For the fourth claim, if $v$ and $w$ are linearly dependent then $\det(u,v,w) = 0$ for all $u \in \reals^3$, so $v \crossp w = 0$. Conversely, if $v$ and $w$ are linearly independent, then extending to a basis $(u,v,w)$ for $\reals^3$ we have $\det(u,v,w) \neq 0$, implying that $v \crossp w \neq 0$.

    To prove the final claim, notice that
    %
    \begin{equation*}
        \inner{v}{v \crossp w}
            = \det(v,v,w)
            = 0,
    \end{equation*}
    %
    and similarly for $w$.
\end{proof}


\begin{proposition}
    Let $a,b,v,w \in \reals^3$. Then
    %
    \begin{equation*}
        \inner{a \crossp b}{v \crossp w}
            = \det
            \begin{pmatrix}
                \inner{a}{v} & \inner{b}{v} \\
                \inner{a}{w} & \inner{b}{w}
            \end{pmatrix}.
    \end{equation*}
    %
    In particular,
    %
    \begin{equation}
        \label{eq:Lagrange-identity}
        \norm{v \crossp w}^2
            = \det
            \begin{pmatrix}
                \norm{v}^2 & \inner{v}{w} \\
                \inner{v}{w} & \norm{w}^2
            \end{pmatrix}.
    \end{equation}
\end{proposition}

\begin{proof}
    By linearity it suffices to prove the identity when the four vectors are basis vectors. If $a = b$ or $v = w$ then both sides are zero, so we may assume that $a = e_i$, $b = e_j$, $v = e_k$ and $v = e_l$ with $i \neq j$ and $k \neq l$. By potentially swapping $a$ and $b$ and/or $v$ and $w$ we may assume that $e_i \crossp e_j = e_p$ and $e_k \crossp e_l = e_q$ for some $p,q \in \{1,2,3\}$.

    If $p = q$ then $i = k$ and $j = l$, so both sides equal $1$. If instead $p \neq q$, then the two cross products on the left-hand side are orthogonal, so the inner product is zero. Furthermore, either $k$ or $l$ equals $p$, so one of the rows in the right-hand side matrix is zero, and hence the determinant is zero.
\end{proof}
%
The identity \cref{eq:Lagrange-identity} is just Lagrange's identity in three dimensions. If $\theta$ is the angle between $v$ and $w$, then $\inner{v}{w} = \norm{v} \, \norm{w} \cos\theta$, so
%
\begin{equation*}
    \norm{v \crossp w}^2
        = \norm{v}^2 \norm{w}^2 - \inner{v}{w}^2
        = \norm{v}^2 \norm{w}^2 (1 - \cos^2 \theta)
        = \norm{v}^2 \norm{w}^2 \sin^2 \theta.
\end{equation*}
%
Hence $\norm{v \crossp w} = \norm{v} \, \norm{w} \, \abs{\sin \theta}$, which is the area of the parallelogram spanned by $v$ and $w$. If $u \in \reals^3$ is another vector and $\phi$ is the angle between $u$ and the normal of the plane spanned by $v$ and $w$ (e.g. $v \crossp w$), then
%
\begin{equation*}
    \abs[\big]{[u,v,w]}
        = \abs{\inner{u}{v \crossp w}}
        = \norm{u} \, \norm{v \crossp w} \, \abs{\cos\phi}
        = \norm{u} \, \norm{v} \, \norm{w} \, \abs{\sin\theta \cos\phi}.
\end{equation*}
%
But this is the volume of the parallelepiped spanned by $u$, $v$ and $w$. This gives a geometric interpretation (or \enquote{proof}) of the invariance of the scalar triple product.


\begin{corollary}
    \label{cor:vector-triple-product}
    Let $u,v,w \in \reals^3$. Then
    %
    \begin{equation}
        \label{eq:bac-cab}
        u \crossp (v \crossp w)
            = v\inner{u}{w} - w\inner{u}{v}.
    \end{equation}
    %
    In particular, the cross product satisfies the \keyword{Jacobi identity}
    %
    \begin{equation}
        \label{eq:cross-product-Jacobi}
        u \crossp (v \crossp w)
            + v \crossp (w \crossp u)
            + w \crossp (u \crossp v)
            = 0.
    \end{equation}
\end{corollary}
%
The identity \cref{eq:bac-cab} is sometimes called the \enquote{bac-cab rule}, a name that would have been self-explanatory had we used the names $a$, $b$ and $c$ instead of $u$, $v$ and $w$. Note that to conform to this rule we need to write the vectors before the scalars.

\begin{proof}
    For $x \in \reals^3$ we have
    %
    \begin{align*}
        \inner{x}{u \crossp (v \crossp w)}
            &= [x, u, v \crossp w] \\
            &= \inner{x \crossp u}{v \crossp w} \\
            &= \det \begin{pmatrix}
                \inner{x}{v} & \inner{u}{v} \\
                \inner{x}{w} & \inner{u}{w}
            \end{pmatrix} \\
            &= \inner{x}{v} \inner{u}{w} - \inner{u}{v} \inner{x}{w} \\
            &= \inner[\big]{x}{ v\inner{u}{w} - w\inner{u}{v} }.
    \end{align*}
    %
    The claim then follows since $x$ was arbitrary.
\end{proof}


We now study how the cross product transforms under linear transformations. Since $\mat{d}{\reals}$ is a finite-dimensional vector space, it has a unique vector space topology. More concretely, all norms on $\mat{d}{\reals}$ are Lipschitz equivalent, so we may choose whatever norm we wish. We choose the Euclidean norm, identifying $\mat{d}{\reals}$ with $\reals^{d^2}$. First a lemma:

\begin{lemma}
    \label{lem:GL-density}
    $\matGL{d}{\reals}$ is dense in $\mat{d}{\reals}$.
\end{lemma}

\begin{proof}
    Let $A \in \mat{d}{\reals}$, and let $t \in \reals \setminus \{0\}$. Then $A - tI$ is invertible if and only if $\det(A - tI) = 0$, but $\det(A - tI)$ is a polynomial in $t$, so it has finitely many roots. Hence the nonzero roots of $\det(A - tI)$ are bounded away from zero, so since $A - tI \to A$ as $t \to 0$, the claim follows.
\end{proof}


\begin{proposition}[Transformation of cross products]
    Let $u,v,w \in \reals^3$, and let $A \in \mat{3}{\reals}$. Then we have the following:
    %
    \begin{enumproposition}
        \item \label{enum:triple-product-transformation} $[Au, Av, Aw] = (\det A) [u,v,w]$.
        
        \item \label{enum:cross-product-transformation} $Av \crossp Aw = (\cof A)(v \crossp w) = \trans{(\adj A)} (v \crossp w)$.

        \item \label{enum:cross-product-orthogonal-transformation} If $A$ is orthogonal, then $A(v \crossp w) = (\det A)(Av \crossp Aw)$.
    \end{enumproposition}
\end{proposition}

\begin{proof}
\begin{proofsec*}
    \item[\itemref{enum:triple-product-transformation}]
    Simply notice that
    %
    \begin{equation*}
        [Au, Av, Aw]
            = \det(Au, Av, Aw)
            = (\det A) \det(u,v,w)
            = (\det A) \inner{u}{v \crossp w},
    \end{equation*}
    %
    where the second equality follows since $\det(Au, Av, Aw)$ is also the determinant of the matrix
    %
    \begin{equation*}
        \bigl( Au \mid Av \mid Aw \bigr)
            = A \bigl( u \mid v \mid w \bigr),
    \end{equation*}
    %
    and the determinant is multiplicative.

    \item[\itemref{enum:cross-product-transformation}]
    First assume that $A$ is invertible. Then replacing $u$ with $\inv{A} u$ in \itemref{enum:triple-product-transformation} we obtain
    %
    \begin{align*}
        \inner{u}{Av \crossp Aw}
            &= (\det A) \inner{\inv{A} u}{v \crossp w} \\
            &= (\det A) \inner{u}{\trans{(\inv{A})} (v \crossp w)} \\
            &=  \inner{u}{(\cof A)(v \crossp w)},
    \end{align*}
    %
    where the last equality follows from \cref{thm:adjoint-matrix-product}. Hence we obtain the desired identity when $A$ is invertible. Finally notice that both the maps $A \mapsto \cof A$ and $A \mapsto Av \crossp Aw$ are continuous. Hence the claim for general $A$ follows from \cref{lem:GL-density}.

    \item[\itemref{enum:cross-product-orthogonal-transformation}]
    Notice that $\inv{A} = \trans{A}$, so this follows immediately from \itemref{enum:cross-product-transformation}.
\end{proofsec*}
\end{proof}
%
This gives a geometric interpretation of the determinant. If $[u,v,w]$ is the signed volume of the parallelepiped spanned by $u$, $v$ and $w$, and $[Au,Av,Aw]$ is the signed volume of the parallelepiped spanned by $Au$, $Av$ and $Aw$, then $\det A$ is the factor by which this volume increasing when applying $A$ to each of $u$, $v$ and $w$. In particular, this explains why the determinant of $A$ is zero if and only if $A$ is singular: This means that $A$ sends a basis of $\reals^3$ to a linearly dependent set, and the parallelepiped spanned by such a set has zero volume.


If $A$ is a proper rotation, i.e. if $A$ is orthogonal and $\det A = 1$, then \cref{enum:cross-product-orthogonal-transformation} implies that $A(v \crossp w) = Av \crossp Aw$. This allows us to define a cross product on any three-dimensional inner product space, when this is equipped with an orientation.

First, if $\calV$ and $\calW$ are ordered bases for any finite-dimensional real vector space $V$, then we say that $\calV$ and $\calW$ have the \keyword{same orientation} if the change of basis operator $\basischange{\calW}{\calV}$ has positive determinant. It follows that orientation partitions the set of ordered bases for $V$ into two \keyword{orientation classes}, each called an \keyword{orientation} of $V$. If $V$ is equipped with an orientation $\calO$, then we call this class the \keyword{positive orientation} of $V$, and the other class the \keyword{negative orientation} of $V$. An ordered basis for $V$ is called \keyword{positive} if it lies in $\calO$ and \keyword{negative} if it does not.

Returning to the case where $V$ is three-dimensional and equipped with an orientation, let $\calV$ and $\calW$ be positive ordered orthonormal bases for $V$. For vectors $v,w \in V$ we can then consider the cross products of their coordinate vectors, i.e.
%
\begin{equation*}
    \coordvec{v}{\calV} \crossp \coordvec{w}{\calV}
    \quad \text{and} \quad
    \coordvec{v}{\calW} \crossp \coordvec{w}{\calW}.
\end{equation*}
%
Since $\basischangemat{\calW}{\calV}$ is orthogonal with determinant $1$, we have
%
\begin{equation*}
    \basischangemat{\calW}{\calV}(\coordvec{v}{\calV} \crossp \coordvec{w}{\calV})
        = \basischangemat{\calW}{\calV} \cdot \coordvec{v}{\calV} \crossp \basischangemat{\calW}{\calV} \cdot \coordvec{w}{\calV}
        = \coordvec{v}{\calW} \crossp \coordvec{w}{\calW}.
\end{equation*}
%
Hence we have
%
\begin{equation*}
    \inv{\coordmap{\calV}}(\coordvec{v}{\calV} \crossp \coordvec{w}{\calV})
        = \inv{\coordmap{\calW}}(\coordvec{v}{\calW} \crossp \coordvec{w}{\calW}),
\end{equation*}
%
so we may define the cross product of $v$ and $w$ as $v \crossp w = \inv{\coordmap{\calV}}(\coordvec{v}{\calV} \crossp \coordvec{w}{\calV})$ where $\calV$ is any positive ordered orthonormal basis for $V$. Notice that this means that $\coordvec{v \crossp w}{\calV} = \coordvec{v}{\calV} \crossp \coordvec{w}{\calV}$.

This allows us to generalise most of the above results to abstract real vector spaces. For instance, using that the coordinate map $\coordmap{\calV}$ is an isometry, the scalar triple product of $u,v,w \in V$ is given by
%
\begin{equation*}
    [u,v,w]
        = \inner{u}{v \crossp w}
        = \inner{\coordvec{u}{\calV}}{\coordvec{v \crossp w}{\calV}}
        = \inner{\coordvec{u}{\calV}}{\coordvec{v}{\calV} \crossp \coordvec{w}{\calV}}
        = \bigl[ \coordvec{u}{\calV}, \coordvec{v}{\calV}, \coordvec{w}{\calV} \bigr],
\end{equation*}
%
and hence it has all the properties of the scalar triple product on $\reals^3$, such as invariance under cyclic permutations. Notice also that it is indeed a \emph{scalar} quantity, in the sense that it is invariant under a change of basis. Similarly, the \enquote{bac-cab rule} \cref{eq:bac-cab} becomes
%
\begin{align*}
    \coordvec{u \crossp (v \crossp w)}{\calV}
        &= \coordvec{u}{\calV} \crossp \coordvec{v \crossp w}{\calV} \\
        &= \coordvec{u}{\calV} \crossp (\coordvec{v}{\calV} \crossp \coordvec{w}{\calV}) \\
        &= \coordvec{v}{\calV} \inner{\coordvec{u}{\calV}}{\coordvec{w}{\calV}} - \coordvec{w}{\calV} \inner{\coordvec{u}{\calV}}{\coordvec{v}{\calV}} \\
        &= \coordvec{v}{\calV} \inner{u}{w} - \coordvec{w}{\calV} \inner{u}{v} \\
        &= \coordvec{ v \inner{u}{w} - w \inner{u}{v} }{\calV}.
\end{align*}
%
Hence $u \crossp (v \crossp w) = v \inner{u}{w} - w \inner{u}{v}$ since $\coordmap{\calV}$ is an isomorphism. In particular, the cross product on $V$ also satisfies the Jacobi identity \cref{eq:cross-product-Jacobi}, so $V$ becomes a Lie algebra whose Lie bracket is given by the cross product, i.e. $[v,w] = v \crossp w$.