\chapter{Structural properties of vector spaces}\label{testchapter}

\section{Projections I}

Let $V$ be a vector space. A linear operator $P \colon V \to V$ is called a \keyword{projection} if it is idempotent, i.e., if $P^2 = P$.

\begin{proposition}
    \label{prop:projection-characterisation}
    A linear map $P \colon V \to V$ is a projection if and only if there exist subspaces $U$ and $W$ of $V$ such that $V = U \dirsum W$, $P|_U = \iota_U$ and $P|_W = 0$. In this case $U = \im P$ and $W = \ker P$.
\end{proposition}
%
We say that $P$ is the projection onto $U$ along $W$.

\begin{proof}
    Assume that $P$ is a projection, and let $v \in \im P$. Then $v = Pu$ for some $u \in V$, and
    %
    \begin{equation*}
        Pv
            = P^2 u
            = Pu
            = v.
    \end{equation*}
    %
    If also $v \in \ker P$, then $v = 0$. Furthermore, for any $v \in V$ we have $v = Pv + (v - Pv) \in \im P \dirsum \ker P$, so $\im P$ and $\ker P$ are indeed complements in $V$.

    The converse is obvious, and so is the characterisation of $U$ and $W$.
\end{proof}

We will return to projections in {sec:projections-2} TODO.


\section{Quotient spaces and complements}

If $U$ is a subspace of an $\fieldF$-vector space $V$, then its underlying additive group is a subgroup of the underlying additive group of $V$. Since $V$ considered as such is abelian, we may consider the quotient group $V/U$ whose elements are cosets $v + U$ for $v \in V$. It is then trivial to check that the operation $\alpha(v + U) \defeq \alpha v + U$ for $\alpha \in \fieldF$ makes $V/U$ into a vector space. We denote by $\pi_U$ or simply by $\pi$ the quotient map $\pi \colon V \to V/U$ given by $\pi(v) = v + U$.

\begin{theorem}
    Let $U$ be a subspace of $V$. If $T \colon V \to W$ satisfies $U \subseteq \ker T$, then there is a unique linear map $\tilde{T} \colon V/U \to W$ such that the diagram
    %
    \begin{equation*}
        \begin{tikzcd}[row sep=small]
            & W \\
            V
                \ar[ur, "T"]
                \ar[dr, "\pi", swap] \\
            & V/U
                \ar[uu, "\tilde{T}", swap, dashed]
        \end{tikzcd}
    \end{equation*}
    %
    commutes.
\end{theorem}

\begin{proof}
    The corresponding result for groups yields a unique group homomorphism $\tilde{T}$. This is easily seen to also be a linear map. % TODO do I also need for topological vector spaces? If not, put in preface that e.g. $\cong$ is only linear isomorphism, not anything topological.
\end{proof}
%
This has the following immediate consequence:

\begin{corollarynoproof}[Canonical decomposition]
    \label{cor:canonical-decomposition}
    Every linear map $T \colon V \to W$ may be decomposed as follows:
    %
    \begin{equation*}
        \begin{tikzcd}
            V
                \ar[r, "\pi", swap, twoheadrightarrow]
                \ar[rrr, bend left, "T"]
            & V/\ker T
                \ar[r, "\sim", "\tilde{T}"']
            & \im T
                \ar[r, "\iota_{\im T}", swap, hookrightarrow]
            & U
        \end{tikzcd}
    \end{equation*}
    %
    In particular we have the \keyword{first isomorphism theorem}: $V/\ker T \cong \im T$.
\end{corollarynoproof}
% TODO better command for \cong and \sim

If $U$ is a subspace of $V$, then a subspace $W$ of $V$ with the property that $V = U \dirsum W$ is called a \keyword{complement} of $U$. Complements are certainly not unique, but we have the following:

\begin{lemma}
    \label{lem:nested-complements}
    Assume that $V$ has two direct sum compositions
    %
    \begin{equation*}
        U \dirsum W_1
            = V
            = U \dirsum W_2,
    \end{equation*}
    %
    where $W_1 \subseteq W_2$. Then $W_1 = W_2$.
\end{lemma}

\begin{proof}
    Assume that $v \in W_2$. Then there exist unique $u \in U$ and $w \in W_1$ such that $v = u + w$. But then $w$ also lies in $W_2$, and uniqueness implies that $u = 0$ and $w = v$. But then $v \in W_1$ as desired.
\end{proof}
%
Next we note the following characterisation of complements:

\begin{proposition}
    \label{prop:complement-iso-to-quotient}
    Let $U$ be a subspace of $V$, and let $W$ be a complement of $U$. The projection $P$ onto $W$ along $U$ induces an isomorphism $V/U \cong W$.
\end{proposition}

\begin{proof}
    Note that $\ker P = U$ and $\im P = W$ by \cref{prop:projection-characterisation}, so \cref{cor:canonical-decomposition} implies that $W \cong V/U$ as claimed. % TODO if I want to do TVS, when is this a homeomorphism?
\end{proof}


So far in this section we have not made use of the fact that all vector spaces have bases. This fact enters the present discussion through the following result:

\begin{proposition}
    Every subspace $U$ of a vector space $V$ has a complement.
\end{proposition}

\begin{proof}
    Choose a basis $\calU$ for $U$ and extend it to a basis $\calV$ for $V$ using \cref{prop:basis-existence}. Then we clearly have $V = U \dirsum \gen{\calV \setminus \calU}$.
\end{proof}

If $U$ is a subspace of $V$, then the dimension of the quotient space $V/U$ is called the \keyword{codimension} of $U$ in $V$ and is denoted $\codim_V U$ or simply $\codim U$. The results above then implies the following:

\begin{corollarynoproof}
    If $U$ is a subspace of $V$, then
    %
    \begin{equation*}
        \dim V
            = \dim U + \codim U.
    \end{equation*}
\end{corollarynoproof}


\begin{corollarynoproof}[The rank--nullity theorem]
    \label{cor:rank-nullity}
    Let $T \in \lin(V,W)$. Then $\codim \ker T = \dim \im T$, and in particular
    %
    \begin{equation*}
        \dim V
            = \dim \ker T + \dim \im T.
    \end{equation*}
\end{corollarynoproof}

% TODO proofs??


\section{Linear maps}

We begin by surveying the different kinds of ways two linear maps can be \textquote{the same}. The most general way two maps can be the same is the following:
%
\begin{definition}[Equivalence of maps]
    The linear maps $T \colon V \to W$ and $S \colon X \to Y$ are \keyword{equivalent} if there exist linear isomorphisms $P \colon X \to V$ and $Q \colon Y \to W$ such that
    %
    \begin{equation*}
        S = \inv{Q} TP.
    \end{equation*}
    %
    The matrices $A,B \in \mat{m,n}{\fieldF}$ are \keyword{equivalent} if there exist invertible matrices $P \in \mat{n}{\fieldF}$ and $Q \in \mat{m}{\fieldF}$ such that
    %
    \begin{equation*}
        B
            = \inv{Q} AP.
    \end{equation*}
\end{definition}
%
If isomorphic vector spaces are \textquote{the same}, then it makes sense that this notion of sameness should be inherited by linear maps between vector spaces. In {par:matrix-rep} TODO we saw that a map between finite-dimensional spaces is equivalent to its basis representation.

Next we have the following notion:
%
\begin{definition}[Similarity of maps]
    The linear maps $T \colon V \to V$ and $S \colon W \to W$ are \keyword{similar} if there exists a linear isomorphism $P \colon W \to V$ such that
    %
    \begin{equation*}
        S = \inv{P} TP.
    \end{equation*}
    %
    The matrices $A,B \in \mat{n}{\fieldF}$ are \keyword{similar} if there exists an invertible matrix $P \in \mat{n}{\fieldF}$ such that
    %
    \begin{equation*}
        B
            = \inv{P} AP.
    \end{equation*}
\end{definition}
%
Notice that this only makes sense for endomorphisms, but the two maps in question can of course be defined on different spaces. As before, endomorphisms of finite-dimensional spaces are similar to their basis representation. We will also see in {prop:diagonalisability-equivalent-properties} TODO that a map is so-called \keyword{diagonalisable} if and only if it is similar to a multiplication operator.

The third and final sameness notion is easy to state for matrices, but only makes sense for general linear transformations between spaces equipped with sesquilinear forms. We give the general definition here, but it will only make sense after reading {ch:sesquilinear-forms} TODO.
%
\begin{definition}[Congruency of maps]
    If $V$ and $W$ satisfy the assumptions in {par:Hilbert-space-adjoints} TODO, then the linear maps $T \colon V \to V$ and $S \colon W \to W$ are \keyword{congruent} if there exists a linear isomorphism $P \colon W \to V$ such that
    %
    \begin{equation*}
        S = P^* TP.
    \end{equation*}
    %
    The matrices $A,B \in \mat{n}{\fieldF}$ are \keyword{congruent} if there exists an invertible matrix $P \in \mat{n}{\fieldF}$ such that
    %
    \begin{equation*}
        B
            = \trans{P}AP.
    \end{equation*}
\end{definition}
%
This notion will turn up in the matrix representation of sesquilinear forms, cf. \cref{par:sesquilinear-matrix-transformation}. % TODO return to matrix congruence

% Note that all of these notions can be qualified by adverbs such as \textquote{orthogonally} or \textquote{isometrically} if the mediating maps (or matrices) $P$ and $Q$ above have the corresponding properties, here of being orthogonal and isometric. [TODO but what's the difference between orthogonal and isometric??]


If a linear map $T \colon V \to W$ is bijective, then its inverse is easily seen to be linear. But if $T$ is only injective (or surjective), does it have a linear left-inverse (or right-inverse)? The answer is affirmative:

\begin{lemma}
    If $T \colon V \to W$ is injective (surjective), then it has a linear left-inverse (right-inverse).
\end{lemma}

\begin{proof}
    First assume that $T$ is injective and restrict its codomain to obtain an isomorphism $\tilde{T} \colon V \to \im T$. If $U$ is a complement of $\im T$, writing $W = \im T \dirsum U$ and letting $S = \inv{\tilde{T}} \dirsum 0$ we get a linear left-inverse of $T$.

    Next assume that $T$ is surjective. Writing $V = \ker T \dirsum U$, $T|_U \colon U \to W$ is an isomorphism. If $\iota_U \colon U \to V$ is the inclusion map, $S = \iota_U \circ \inv{T|_U}$ is a right-inverse of $T$.
\end{proof}
%
Similarly, we can ask whether monomorphisms (epimorphisms) are necessarily injective (surjective):

\begin{lemma}
    If $T \colon V \to W$ is a monomorphism (epimorphism), then it is injective (surjective).
\end{lemma}

\begin{proof}
    First assume that $T$ is not injective, and assume that $v \neq v'$ satisfy $Tv = Tv'$. Let $U$ be a nontrivial vector space, let $u \in U$ be nonzero, and consider linear maps $S,R \colon U \to V$ with $Su = v$ and $Ru = v'$, and that agree on a complement of $\gen{u}$. Then $TS = TR$ but $S \neq R$, so $T$ is not a monomorphism.

    Similarly, if $T$ is not surjective then let $w \in W \setminus \im T$ and define maps $S,R \colon W \to U$ that agree on a complement of $\gen{w}$, and that satisfy $Sw \neq Rw$. Then $ST = RT$, but $S \neq R$.
\end{proof}
%
These lemmas together imply the following:

\begin{theoremnoproof}
    A linear map is injective (surjective) if and only if it is a monomorphism (epimorphism) if and only if it has a left-inverse (right-inverse).
\end{theoremnoproof}


Finally we note that between \emph{finite-dimensional} spaces, \cref{cor:rank-nullity} has the following fundamental corollary:

\begin{corollarynoproof}
    If $V$ and $W$ are finite-dimensional, then $T \colon V \to W$ is injective if and only if it is surjective.
\end{corollarynoproof}


\section{Duality}

If $V$ is an $\fieldF$-vector space, then a \keyword{linear functional} is a linear map $V \to \fieldF$. Since $\fieldF$ itself is an $\fieldF$-vector space, the set $\lin(V,\fieldF)$ is also vector space. We denote this by $V^*$ and call it the \keyword{algebraic dual space} of $V$.

We note that if $v \in V$ is nonzero, then there exists a $\phi \in V^*$ with $\phi(v) \neq 0$: For extend $v$ to a basis for $V$, let $\phi(v) = 1$ and let $\phi = 0$ on any complement of $\gen{v}$.

The algebraic dual space is of little interest when the vector space in question is an infinite-dimensional topological $\fieldK$-vector space. If $V$ is such a space, we instead often let $V^*$ denote the \keyword{topological dual space}, the subspace of the algebraic dual space consisting of the \emph{continuous} functionals. In the sequel, $V^*$ will denote the algebraic dual space unless otherwise stated.

We study how a basis for $V$ gives rise to a basis for $V^*$. Let $\calV = \set{v_i}{i \in I}$, where $I$ is some index set, be a basis for a vector space $V$. For $i \in I$ we then define $v_i^* \in V^*$ by $v_i^*(v_j) = \delta_{ij}$, and let $\calV^* = \set{v_i^*}{i \in I}$.

\begin{proposition}
    \label{prop:dual-basis}
    If $\calV$ is a basis for $V$, then the set $\calV^*$ is linearly independent, and hence $\dim V \leq \dim V^*$. If $V$ is finite-dimensional and $\calV = (v_1, \ldots, v_n)$, then $\calV^*$ is a basis for $V^*$ called the \keyword{dual basis} of $\calV$, and
    %
    \begin{equation*}
        \phi
            = \sum_{i=1}^n \phi(v_i) v_i^*
    \end{equation*}
    %
    for all $\phi \in V^*$. In particular, $V \cong V^*$.
\end{proposition}

\begin{proof}
    Applying the functional
    %
    \begin{equation*}
        \alpha_{i_1} v_{i_1}^* + \cdots + \alpha_{i_n} v_{i_n}^* = 0
    \end{equation*}
    %
    to the vector $v_{i_k}$ we find that $\alpha_{i_k} = 0$. If $V$ is finite-dimensional with $\dim V = n$ and $\phi \in V^*$, then
    %
    \begin{equation*}
        \phi(v_j)
            = \sum_{i=1}^n \phi(v_i) \delta_{ij}
            = \sum_{i=1}^n \phi(v_i) v_i^*(v_j),
    \end{equation*}
    %
    so $\phi = \sum_{i=1}^n \phi(v_i) v_i^* \in \gen{\calV^*}$.
\end{proof}
%
The above in particular says that if $\phi = \phi_1 v_1^* + \cdots + \phi_n v_n^*$, then $\phi_i = \phi(v_i)$.

So in the finite-dimensional case, $V$ and $V^*$ are isomorphic. In the infinite-dimensional case, one can show (cf. \cite[Theorem~3.12]{romanlinalg}, which says that then $\dim V < \dim V^*$) that the algebraic dual space of $V$ always has a strictly greater dimension than $V$, so these cannot be isomorphic. If instead $V$ is a topological vector space, then we instead consider the continuous dual space $V^*$, and since this is generally smaller than the algebraic one, $V$ again has a chance of being isomorphic to $V^*$ (though note that the dual basis elements are not guaranteed to be continuous). We will return to this point below.


If $V$ is a vector space, we may consider its dual $V^*$. And if $V$ is finite-dimensional, then so is $V^*$, and so we may consider \emph{its} dual, $V^{**}$. On the other hand, if $V$ is a topological vector space, then its topological dual $V^*$ naturally carries the weak$^*$-topology, in which case we may also consider \emph{its} (topological) dual. In either case we call $V^{**}$ the (algebraic or topological) \keyword{double dual space} of $V$. This will again denote the algebraic double dual space unless we state otherwise.

We construct a map from $V$ into $V^{**}$ as follows: For $v \in V$ define $\ev_v \colon V^* \to \fieldF$ by evaluation at $v$, i.e. $\ev_v(\phi) = \phi(v)$. This induces a map $\ev \colon V \to V^{**}$ given by $v \mapsto \ev_v$. If $v \neq w$, then we may hope to find a $\phi \in V^*$ such that $\phi(v) \neq \phi(w)$, which would implies that $\ev_v(\phi) \neq \ev_w(\phi)$, and so $\ev$ would be injective. If $V$ is finite-dimensional, then this is clearly possible. However, if $V$ is an infinite-dimensional topological vector space and $V^*$ instead denotes the topological dual, then we can still performs the constructions above, but then there might not even be any nonzero continuous linear functionals on $V$. This is for instance the case for the Lebesgue space $\calL^p([0,1])$ for $p \in (0,1)$ (cf. \cite[ยง1.47]{rudinfunctional}). On the other hand, the Hahn--Banach theorem implies that we can in fact find such a functional $\phi$ in case $V$ is locally convex.

Whether or not $\ev$ is injective, it may not be surjective, even if $V$ is a Banach space. If $V^*$ denotes the algebraic dual and $\ev$ is an isomorphism, then $V$ is called \keyword{reflexive}. If $V^*$ instead denotes the topological dual, then we also call $V$ reflexive if $\ev$ is an isomorphism, but we also require it to be a homeomorphism. Indeed, as mentioned above the algebraic dual of an infinite-dimensional vector space $V$ is of strictly greater dimension than $V$ itself, $V$ cannot be isomorphic to its algebraic double dual. On the other hand, finite-dimensional vector spaces are always isomorphic to their double dual, so reflexivity is fairly trivial for vector spaces that are not topological. Hence the notion is usually only interesting for topological vector spaces. Whenever we below consider $V^*$ the algebraic (topological) dual, the property of being reflexive will be in relation to the corresponding algebraic (topological) double dual space.

Finally, if $\calV = (v_1, \ldots, v_n)$ is a basis for $V$, then the basis $\calV^*$ itself has a dual basis $\calV^{**}$. Applying an element $v_i^{**}$ to the functional $\phi$ then yields
%
\begin{equation*}
    v_i^{**}(\phi)
        = \phi_i
        = \phi(v_i).
\end{equation*}
%
That is, $v_i^{**} = \ev_{v_i}$.


In the remainder of this section we do not consider topological vector spaces. We now introduce a new concept that is useful in characterising dual spaces:

\begin{definition}
    Let $M \subseteq V$. The \keyword{annihilator} of $M$ is the set
    %
    \begin{equation*}
        M^0
            = \set{\phi \in V^*}{\phi|_M = 0}.
    \end{equation*}
\end{definition}
%
It is easy to see that $M^0$ is a subspace of $V^*$ even when $M$ is not. Furthermore, notice that $\emptyset^0 = V^*$. It is also obvious that if $M \subseteq N$ then $N^0 \subseteq M^0$.

Assume that we have a direct sum decomposition $V = U \dirsum W$. If $\phi \in U^*$ then we may extend $\phi$ to a functional $\overline{\phi}$ on $V$ by letting $\overline{\phi}(w) = 0$ for all $w \in W$. We say that $\overline{\phi}$ is the \keyword{extension by $0$} of $\phi$. The map $\eta \colon U^* \to V^*$ given by $\eta(\phi) = \overline{\phi}$ has a left-inverse, namely the pullback\footnote{In {sec:operator-adjoints} TODO we will meet this pullback again under the name \emph{operator adjoint}.}
%
\begin{align*}
    \iota_U^\dagger \colon V^* &\to U^*, \\
    \psi &\mapsto \psi \circ \iota_U,
\end{align*}
%
where $\iota_U \colon U \to V$ is the inclusion map: For notice that $(\iota_U^\dagger \circ \eta)(\phi) = \overline{\phi} \circ \iota_U = \phi$. In particular, $\eta$ is injective and $\iota_U^\dagger$ is surjective. Now notice that $\im \eta = W^0$, and that $\ker \iota_U^\dagger = U^0$. Hence we have proved:

\begin{propositionnoproof}
    \label{prop:subspace-dual-complement-annihilator}
    If $V = U \dirsum W$, then the map $U^* \to W^0$ given by $\phi \mapsto \overline{\phi}$ is an isomorphism. If $U$ is finite-dimensional, i.e. if $W$ has finite codimension, then in particular $\dim W^0 = \codim W$.
\end{propositionnoproof}

\begin{propositionnoproof}
    If $U$ is a subspace of $V$, then $V^*/U^0 \cong U^*$.
\end{propositionnoproof}

\begin{corollary}
    If $U$ is a subspace of $V$, then $(V/U)^* \cong U^0$.
\end{corollary}

\begin{proof}
    If $W$ is a complement of $U$, then $V/U \cong W$ by \cref{prop:complement-iso-to-quotient}, so $(V/U)^* \cong W^*$. But then \cref{prop:subspace-dual-complement-annihilator} implies the claim.
\end{proof}


Finally, we can also use annihilators to characterise the dual space of a direct sum:

\begin{proposition}
    $(U \dirsum W)^* = U^0 \dirsum W^0$.
\end{proposition}
%
Since $U^0 \cong W^*$ and $W^0 \cong U^*$, this gives an alternative proof of \cref{prop:dual-of-product}. TODO find a place for dual-of-product

\begin{proof}
    The sum of $U^0$ and $W^0$ is clearly direct, and the inclusion \textquote{$\supseteq$} is obvious. Now let $\phi \in V^*$, and let $P_U$ and $P_W$ be the projections onto $U$ and $W$ along $W$ and $U$, respectively. then $P_W + P_U = \id_V$, so
    %
    \begin{equation*}
        \phi
            = \phi \circ (P_W + P_U)
            = \phi \circ P_W + \phi \circ P_U
            \in U^0 \dirsum W^0,
    \end{equation*}
    %
    proving the other inclusion.
\end{proof}


\section{Operator adjoints}\label{sec:operator-adjoints}

Let $\catC$ be a locally small category, and let $f \colon A \to B$ be an arrow in $\catC$. For every object $C$, precomposition with $f$ then induces an arrow
%
\begin{align*}
    \hom[\catC](f,C) \colon \hom[\catC](B,C) &\to \hom[\catC](A,C), \\
        g &\mapsto g \circ f.
\end{align*}
%
This gives rise to a contravariant functor $\hom[\catC](-,C) \colon \catC \to \mathbf{Set}$. Specialising to the case where $\catC$ is the category $\catVect{\fieldF}$ and where $C$ is the field $\fieldF$ (considered as a vector space), we obtain the functor $(-)^*$ sending a vector space $V$ to its (algebraic) dual $V^*$, and a linear map $T$ to its pullback. Since we will use the notation $T^*$ for the Hilbert space adjoint, we instead write $T^\dagger$ for the pullback of $T$, following \textcite{follandrealanalysis}. We also call this the \emph{operator adjoint} of $T$:

\begin{definition}[Operator adjoints]
    Let $V$ and $W$ be $\fieldF$-vector spaces, and let $T \colon V \to W$ be a linear map. The \emph{(operator) adjoint} of $T$ is the pullback
    %
    \begin{align*}
        T^\dagger \colon W^* &\to V^*, \\
        \phi &\mapsto \phi \circ T.
    \end{align*}
\end{definition}
%
This already satisfies $\id_V^\dagger = \id_{V^*}$ and $(ST)^\dagger = T^\dagger S^\dagger$ by functoriality, so that in particular $(\inv{T})^\dagger = \inv{(T^\dagger)}$ when $T$ is invertible. Furthermore, it is easy to show that the map $T \mapsto T^\dagger$ is linear. As before, if $W$ is either finite-dimensional or a locally convex space, if $Tv \neq Sv$ there is a $\phi \in W^*$ with $\phi(Tv) \neq \phi(Sv)$, so in these cases $T \mapsto T^\dagger$ is injective.


\begin{proposition}
    \label{prop:operator-adjoint-kernel-image}
    Let $T \in \lin(V,W)$.
    %
    \begin{enumproposition}
        \item \label{enum:operator-adjoint-kernel} $\ker T^\dagger = (\im T)^0$.
        \item \label{enum:operator-adjoint-image} $\im T^\dagger = (\ker T)^0$.
    \end{enumproposition}
\end{proposition}

\begin{proof}
    For part \itemref{enum:operator-adjoint-kernel}, notice that
    %
    \begin{align*}
        \ker T^\dagger
            &= \set{\psi \in W^*}{T^\dagger \psi = 0} \\
            &= \set{\psi \in W^*}{\psi \circ T = 0} \\
            &= \set{\psi \in W^*}{\psi(\im T) = \{0\}} \\
            &= (\im T)^0.
    \end{align*}
    %
    For part \itemref{enum:operator-adjoint-image}, if $\phi \in \im T^\dagger$ then there is a $\psi \in W^*$ with $\phi = T^\dagger \psi = \psi \circ T$. Hence $\ker T \subseteq \ker \phi$, so $\phi \in (\ker T)^0$. For the opposite inclusion, let $\phi \in (\ker T)^0$. Let $U$ be a complement of $\ker T$ and let $S \colon W \to U$ be a linear left-inverse of $T|_U$. Letting $\psi = \phi \circ S$ we thus get
    %
    \begin{equation*}
        T^\dagger \psi
            = \psi \circ T
            = \phi \circ S \circ T.
    \end{equation*}
    %
    This agrees with $\phi$ on $U$ by definition of $T$, and it agrees with $\phi$ on $\ker T$ since $\phi$ lies in $(\ker T)^0$. Thus $\phi \in \im T^\dagger$.
\end{proof}


Since $T^\dagger$ is itself a linear map, we may of course consider \emph{its} adjoint $T^{\dagger\dagger} \colon V^{**} \to W^{**}$. If $V$ is not reflexive, then as far as I know there is little to say about $T^{\dagger\dagger}$, but if it is then we have the following:

\begin{proposition}
    If $V$ is reflexive and $T \colon V \to W$ is linear, then
    %
    \begin{equation*}
        T^{\dagger\dagger}
            = \ev \circ T \circ \inv{\ev},
    \end{equation*}
    %
    where the leftmost $\ev$ is evaluation on $W$, and the rightmost $\ev$ is evaluation on $V$.
\end{proposition}

\begin{proof}
    If $v \in V$, then notice that $T^{\dagger\dagger} \ev_v = \ev_v \circ T^\dagger$. But if $\phi \in W^*$, then notice that
    %
    \begin{equation*}
        (\ev_v \circ T^\dagger)(\phi)
            = (T^\dagger\phi)v
            = (\phi \circ T)v
            = \ev_{Tv}(\phi).
    \end{equation*}
    %
    Hence $T^{\dagger\dagger} \ev_v = \ev_{Tv}$ for all $v \in V$, so $T^{\dagger\dagger} \circ \ev = \ev \circ T$. The claim follows since $V$ is reflexive.
\end{proof}


We now consider the case where $V$ and $W$ are finite-dimensional in more detail.

\begin{corollary}
    \label{cor:adjoint-rank}
    If $T \in \lin(V,W)$ with $V$ and $W$ finite-dimensional, then $\rank T^\dagger = \rank T$.
\end{corollary}

\begin{proof}
    Note that
    %
    \begin{equation*}
        \dim \im T^\dagger
            \overset{(1)}{=} \dim (\ker T)^0
            \overset{(2)}{=} \codim \ker T
            \overset{(3)}{=} \dim \im T,
    \end{equation*}
    %
    where $(1)$ follows by \cref{enum:operator-adjoint-kernel}, $(2)$ by \cref{prop:subspace-dual-complement-annihilator}, and $(3)$ by \cref{cor:rank-nullity}.
\end{proof}


\begin{proposition}
    \label{prop:adjoint-mr}
    If $T \in \lin(V,W)$ is a linear map between finite-dimensional vector spaces, and $\calV$ and $\calW$ are ordered bases for $V$ and $W$ respectively, then
    %
    \begin{equation*}
        \mr{\calV^*}{T^\dagger}{\calW^*}
            = \trans{\bigl( \mr{\calW}{T}{\calV} \bigr)}.
    \end{equation*}
\end{proposition}

\begin{proof}
    Write $\calV = (v_1, \ldots, v_n)$ and $\calW = (w_1, \ldots, w_m)$. Then
    %
    \begin{equation*}
        \bigl( \mr{\calW}{T}{\calV} \bigr)_{ij}
            = \bigl( \coordvec{Tv_j}{\calW} \bigr)_i
            = w_i^*(Tv_j),
    \end{equation*}
    %
    and
    %
    \begin{equation*}
        \bigl( \mr{\calV^*}{T^\dagger}{\calW^*} \bigr)_{ij}
            = \bigl( \coordvec{T^\dagger w_j^*}{\calV^*} \bigr)_i
            = v_i^{**}(T^\dagger w_j^*)
            = T^\dagger w_j^*(v_i)
            = w_j^*(Tv_i).
    \end{equation*}
    %
    These expressions are the same, but with $i$ and $j$ switched.
\end{proof}

From this we obtain the following result on the row and column rank of a matrix. This is often proved by showing that one can apply elementary row and column operations to the matrix in question, preserving the row and column rank, and obtain a diagonal matrix whose entries are either zero or one. The common row and column rank of the matrix is then simply the number of ones. Going through the abstract theory above we avoid these considerations.

\begin{corollary}
    The row rank and the column rank of a matrix $A \in \mat{m,n}{\fieldF}$ are equal.
\end{corollary}

\begin{proof}
    The matrix representation of the multiplication operator $M_A$ with respect to the standard bases on $\fieldF^n$ and $\fieldF^m$ is just $A$ itself, and \cref{prop:adjoint-mr} then implies that the matrix representation of $(M_A)^\dagger$ with respect to the dual bases is $\trans{A}$. But the rank of an operator equals the rank of any matrix representation of that operator, so \cref{cor:adjoint-rank} implies that $A$ and $\trans{A}$ have the same (column) rank. Finally, the column rank of $\trans{A}$ is the row rank of $A$, proving the claim.
\end{proof}


% If $V$ and $W$ are instead \emph{topological} vector spaces, of arbitrary dimension, and $V^*$ and $W^*$ denote their respective \emph{continuous} dual spaces, then we may also consider the adjoint $T^\dagger$ of a \emph{continuous} linear map $T \colon V \to W$. It then turns out that $T^\dagger$ is also continuous when $V^*$ and $W^*$ are equipped with the appropriate topologies.

% \begin{proposition}
%     Let $T \colon V \to W$ be a continuous linear map, and let $T^\dagger \colon W^* \to V^*$ be its adjoint.
%     %
%     \begin{enumprop}
%         \item $T^\dagger$ is continuous with respect to the weak$^*$-topologies on $W^*$ and $V^*$.

%         \item \label{enum:operator-adjoint-continuous-normed} If $V$ and $W$ are normed vector spaces, then $T^\dagger$ is continuous with respect to the operator norms on $W^*$ and $V^*$, and $\norm{T^\dagger} = \norm{T}$.
%     \end{enumprop}
% \end{proposition}

% \begin{proof}
%     Let $(\phi_i)_{i \in I}$ be a net in $W^*$ that converges to some $\phi \in W^*$. That is, $\phi_i(w) \to \phi(w)$ for all $w \in W$, so in particular $\phi_i(Tv) \to \phi(Tv)$ for all $v \in V$. But then $T^\dagger \phi_i = \phi_i \circ T$ converges to $T^\dagger \phi = \phi \circ T$, so $T^\dagger$ is continuous as claimed.

%     If $V$ and $W$ are normed, then
%     %
%     \begin{equation*}
%         \norm{T^\dagger \phi}
%             = \norm{\phi \circ T}
%             \leq \norm{\phi} \, \norm{T}
%     \end{equation*}
%     %
%     for all $\phi \in W^*$, implying that $T^\dagger$ is bounded with $\norm{T^\dagger} \leq \norm{T}$. If $T \neq 0$, then let $v \in V$ with $\norm{v} = 1$ such that $Tv \neq 0$. The Hahn--Banach theorem then furnishes a $\phi \in W^*$ with $\norm{\phi} = 1$ and $\phi(Tv) = \norm{Tv}$ (cf. \cite[Theorem~5.8(b)]{follandrealanalysis}). It follows that
%     %
%     \begin{equation*}
%         \norm{T^\dagger}
%             \geq \norm{T^\dagger \phi}
%             \geq \abs{(T^\dagger \phi)v}
%             = \abs{\phi(Tv)}
%             = \norm{Tv}.
%     \end{equation*}
%     %
%     This inequality then holds for all $v \in V$ with $\norm{v} = 1$, implying that $\norm{T^\dagger} \geq \norm{T}$.
% \end{proof}

% % TODO: $T \mapsto T^\dagger$ not necessarily injective now?


\section{Resolutions of the identity}

Let $V$ be a vector space. Two projections $P,Q \in \lin(V)$ are said to be \keyword{orthogonal} if $PQ = QP = 0$, in which case we write $P \perp Q$. A \keyword{resolution of the identity} on $V$ is a decomposition
%
\begin{equation*}
    \id_V
        = P_1 + \cdots + P_k.
\end{equation*}
%
of the identity map on $V$, where $P_1, \ldots, P_k$ are pairwise orthogonal projections on $V$. If $V$ is an inner product space and the $P_i$ are themselves orthogonal projections, then we also say that the resolution of the identity is \keyword{orthogonal}. [TODO move orthogonal to somewhere in chapter on sesquilinear]

\begin{proposition}
    \label{prop:resolution-of-the-identity-characterisation}
    If $\id_V = P_1 + \cdots + P_k$ is a resolution of the identity, then
    %
    \begin{equation*}
        V
            = \im P_1 \dirsum \cdots \dirsum \im P_k,
        \quad \text{and} \quad
        \ker P_i
            = \bigdirsum_{j \neq i} \im P_j
    \end{equation*}
    %
    for all $i = 1, \ldots, k$. Conversely, if
    %
    \begin{equation*}
        V
            = U_1 \dirsum \cdots \dirsum U_k
    \end{equation*}
    %
    and $P_i$ is the projection onto $U_i$ along $\bigdirsum_{j \neq i} U_j$, then $\id_V = P_1 + \cdots + P_k$ is a resolution of the identity.
\end{proposition}

\begin{proof}
    Clearly $V$ is a (not necessarily direct) sum of the above images. To see that the sum is direct, if for $v_1, \ldots, v_k \in V$ we have
    %
    \begin{equation*}
        P_1 v_1 + \cdots P_k v_k = 0,
    \end{equation*}
    %
    then applying $P_i$ we get $P_i v_i = 0$. Furthermore, we clearly have $\bigdirsum_{j \neq i} \im P_j \subseteq \ker P_i$ by orthogonality. For the opposite inclusion, notice that
    %
    \begin{equation*}
        \im P_i \dirsum \ker P_i
            = V
            = \im P_i \dirsum \biggl( \bigdirsum_{j \neq i} \im P_j \biggr).
    \end{equation*}
    %
    And since $\bigdirsum_{j \neq i} \im P_j \subseteq \im P_i$ by orthogonality, the opposite inclusion follows from \cref{lem:nested-complements}.

    For the converse, if $i \neq j$ then $\im P_i = U_i \subseteq \ker P_j$, so $P_i \perp P_j$. Furthermore, if $v = u_1 + \cdots + u_k$ with $u_i \in U_i$, then $P_i v = u_i$, so
    %
    \begin{equation*}
        v
            = u_1 + \cdots + u_k
            = P_1 v + \cdots + P_k v
            = (P_1 + \cdots + P_k) v,
    \end{equation*}
    %
    as desired.
\end{proof}